---
title:  "wikië°ì´í„°ë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ìº¡ì…”ë‹"

categories:
  - pj
tags:
  - [attention, RNN]

toc: true
toc_sticky: true

date: 2022-03-28
last_modified_at: 2022-03-28
---


### mac gpu ì‚¬ìš©ê°€ëŠ¥í™•ì¸ (1ì´ë©´ ì‚¬ìš©ê°€ëŠ¥)


```python
import tensorflow as tf
```


```python
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
```

    Num GPUs Available:  0



```python
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive


# import


```python
# tensorflow.compat.v2 ë¥¼ ì‚¬ìš©
import tensorflow.compat.v2 as tf
import pandas as pd
from matplotlib import pyplot as plt
import re
import cv2
import time
```

# dataset
> ê° train ë°ì´í„° ( train_1 ~ train_10) ëŠ” small_data.ipynb ì—ì„œ ë”°ë¡œ ì²˜ë¦¬í•˜ì—¬ ë§Œë“¤ì–´ì§.  
> ì˜ì–´ë§Œ ì¶”ì¶œ, image_url ê³¼ caption_feature ë§Œ ì¶”ì¶œ.

ì¶œì²˜ : https://www.kaggle.com/c/wikipedia-image-caption/code?competitionId=29705&searchQuery=tensor


```python
# caption_featureë¥¼ ë³´ë©´
# 1. [SEP] ìœ¼ë¡œ ë‚˜ë‰˜ì–´ì ¸ ìˆëŠ”ë°, ê·¸ ë’¤ì— ìˆëŠ” ë‚´ìš©ì— ëŒ€í•´ í•  ê²ƒ. (BERT)ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìŠ¤í˜ì…œ í† í°ì¸ë° íŒŒì¸íŠœë‹ì€ ì•ˆí•  ê²ƒì´ë¯€ë¡œ..
# 2. ìˆ«ìì •ë³´ëŠ” ì œì™¸í•˜ì ëŒ€ë¶€ë¶„ íŠ¹ìˆ˜í•œ ê²½ìš°ì— ì“°ì´ê±°ë‚˜ ì£¼ì†Œ ë“±ì´ë‹¤.

df = pd.read_csv('/content/drive/MyDrive/dataset/wiki/train_1.tsv', delimiter = '\t')
p = re.compile('\[SEP\].+') # \ë¡œ ê°ì‹¸ì§„ ê³³ì€
df['caption_title_and_reference_description'] = df['caption_title_and_reference_description'].apply(
                                lambda x: '<start> ' +
                                        re.sub('\d+', '', p.search(x).group().replace('[SEP] ', '')).lower()
                                        +' <end>'
                                        if p.search(x).group() not in['[SEP] ', ''] else None)
df=df.dropna(axis=0)
print(df.shape)
df.head()
```

    (301484, 2)






  <div id="df-e23dee60-b000-4f2a-9687-a1c1871c9bd4">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_url</th>
      <th>caption_title_and_reference_description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://upload.wikimedia.org/wikipedia/commons...</td>
      <td>&lt;start&gt; downtown deer park &lt;end&gt;</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://upload.wikimedia.org/wikipedia/commons...</td>
      <td>&lt;start&gt; jÃ¼rgen ovens's justitia, -, museumsber...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://upload.wikimedia.org/wikipedia/commons...</td>
      <td>&lt;start&gt;  mv agusta  raid &lt;end&gt;</td>
    </tr>
    <tr>
      <th>4</th>
      <td>https://upload.wikimedia.org/wikipedia/commons...</td>
      <td>&lt;start&gt; seth macfarlane's logo &lt;end&gt;</td>
    </tr>
    <tr>
      <th>6</th>
      <td>https://upload.wikimedia.org/wikipedia/commons...</td>
      <td>&lt;start&gt; erskine river at lorne &lt;end&gt;</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-e23dee60-b000-4f2a-9687-a1c1871c9bd4')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-e23dee60-b000-4f2a-9687-a1c1871c9bd4 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-e23dee60-b000-4f2a-9687-a1c1871c9bd4');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
import cv2
import numpy as np
from urllib import request

def url_to_image(url):
    '''
    url ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì—¬, (512,512,3) ì˜ rgb ndarrayë¡œ ë¦¬í„´
    '''
    resp = request.urlopen(url)
    image = np.asarray(bytearray(resp.read()), dtype='uint8')
    image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0
    return image
    #return cv2.resize(image, (512,512))
```


```python
# ê°€ì¥ ê¸´ ìº¡ì…˜ê³¼ ê·¸ì˜ ì´ë¯¸ì§€ ì¶œë ¥í•´ë³´ê¸° (ëª¨ë¸ìƒì—ì„œëŠ” ì‹¤í–‰ì•ˆí•´ë„ë¨.)
from google.colab.patches import cv2_imshow
long_caption = max(df['caption_title_and_reference_description'], key = lambda x: len(x))
a = df[df['caption_title_and_reference_description'] == long_caption ]['image_url']
#a = df[df['caption_title_and_reference_description'] == 'Downtown Deer Park']['image_url']
x = url_to_image(str(a.iloc[0]))
print(long_caption)
cv2_imshow(x)
print(x.shape)
print(len(long_caption))
```

    <start> figure : genomic context scheme of smrc and its closest homologues in other organisms. the Î±r rna genes are represented by red arrows and the flanking orfs by arrows on different colors depending on their product function (legend). numbers indicate the Î±r rna gene's and flanking orfs coordinates in each organism genome database. the gene strand is represented with the file direction. on the left of the figure identification names are used which correspond to a certain organism: Î±r_smrc = sinorhizobium meliloti  (nc_), Î±r_smedrc = sinorhizobium medicae wsm chromosome (nc_), Î±r_sfrc = sinorhizobium fredii ngr chromosome (nc_), Î±r_atrc = agrobacterium tumefaciens str. c chromosome linear (nc_), Î±r_reciatrc = rhizobium etli ciat  (nc_), Î±r_arrc = agrobacterium radiobacter k chromosome  (nc_), Î±r_rltrc = rhizobium leguminosarum bv. trifolii wsm (nc_), Î±r_avrc = agrobacterium vitis s chromosome  (nc_), Î±r_rlvrc = rhizobium leguminosarum bv. viciae  (nc_), Î±r_rltrc = rhizobium leguminosarum bv. trifolii wsm (nc_), Î±r_recfnrc = rhizobium etli cfn  (nc_), Î±r_mlrc = mesorhizobium loti maff chromosome (nc_), Î±r_mcrc = mesorhizobium ciceri biovar biserrulae wsm chromosome (nc_), Î±r_bcrcii = brucella canis atcc  chromosome ii (nc_), Î±r_bsrcii = brucella suis atcc  chromosome ii (nc_), Î±r_bmmrcii = brucella melitensis bv.  str. m chromosome ii (nc_), Î±r_basrcii = brucella abortus s chromosome  (nc_), Î±r_bmrcii = brucella melitensis atcc  chromosome ii (nc_), Î±r_bsrcii = brucella suis  chromosome ii (nc_), Î±r_barcii = brucella abortus bv.  str. - chromosome ii (nc_), Î±r_bmarcii = brucella melitensis biovar abortus  chromosome ii (nc_), Î±r_borcii = brucella ovis atcc  chromosome ii (nc_), Î±r_bmircii = brucella microti ccm  chromosome  (nc_), Î±r_oarc = ochrobactrum anthropi atcc  chromosome  (nc_), Î±r_msbncrc = mesorhizobium sp. bnc (nc_), Î±r_bahrc = bartonella henselae str. houston- (nc_), Î±r_bacrc = bartonella clarridgeiae  (nc_), Î±r_batrc = bartonella tribocorum cip  (nc_), Î±r_baqrc = bartonella quintana str. toulouse (nc_), Î±r_babrc = bartonella bacilliformis kc (nc_), Î±r_bagrc = bartonella grahamii asaup (nc_), Î±r_acrc = azorhizobium caulinodans ors  (nc_), Î±r_stnrc = starkeya novella dsm  chromosome (nc_), Î±r_xarc = xanthobacter autotrophicus py chromosome (nc_), Î±r_mesrc = methylocella silvestris bl chromosome (nc_), Î±r_beirc = beijerinckia indica subsp. indica atcc  chromosome (nc_), Î±r_rhprc = rhodopseudomonas palustris bisa chromosome (nc_). <end>



<img src = '/assets/images/project/source_2.png' width = '30%' height = '30%'>




    (7992, 1080, 3)
    2492


# ì „ì²˜ë¦¬
> ìœ„ì—ì„œ í™•ì¸í–ˆë“¯, caption ì´ ì œì¼ ê¸´ í–‰ì— ëŒ€í•´  
> 1. ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ë‹¤(7992,1080). >  512,512 ë¡œ ì••ì¶•í•˜ê²Œ ë˜ë©´ ì‹¬ê°í•˜ê²Œ ì°Œê·¸ëŸ¬ ì§ˆê²ƒ,
> 2. captionì´ ë„ˆë¬´ ê¸¸ë‹¤. 3011ì.

> í•´ê²°ë°©ë²•
> 1. captionì´ ë„ˆë¬´ ê¸´ í–‰(100ì ì´ìƒ)ì€ ì‚­ì œ.  
> 2. image íŒŒì¼ì´ ë„ˆë¬´ í°(ê°€ë¡œ ì„¸ë¡œ ë¹„ìœ¨ì´ 2:1 í˜¹ì€ 1:2 ë¥¼ ì´ˆê³¼) ê²½ìš°ëŠ” ì‚­ì œ


```python
# ìœ„ url_to_image ë‹¤ì‹œ ì •ì˜
def url_to_image(url):
    '''
    url ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ì—¬, (512,512,3) ì˜ rgb ndarrayë¡œ ë¦¬í„´
    '''
    resp = request.urlopen(url)
    image = np.asarray(bytearray(resp.read()), dtype='uint8')
    image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0
    if type(image) == type(None):
        return np.array([None])
    image = image/255.0
    if ( image.shape[0]/image.shape[1] > 2 ) or ( image.shape[0]/image.shape[1] < 1/2 ):  # ê°€ë¡œì„¸ë¡œ ë¹„ìœ¨ì´ 1:2, 2:1ì„ ë²—ì–´ë‚œë‹¤ë©´
        return np.array([None])
    else:
        return cv2.resize(image, (299,299))
```


```python
X_pre_train=[]
y_train=[]
for i,j in enumerate((df.iloc[1500:2000]['image_url'])):
    try:
        temp = url_to_image(j)
    except:
        pass
    if None in temp:
        pass
    else:
        try:
          X_pre_train.append(temp)
          y_train.append( df.iloc[i]['caption_title_and_reference_description'] )
        except:
          pass
print(len(X_pre_train))
print(X_pre_train[2].shape)
print(len(y_train))
print(y_train[2])

# png íŒŒì¼ì¼ ê²½ìš° libpngê²½ê³ ê°€ ë‚˜ì˜¤ëŠ”ë° ë¬´ì‹œí•´ë„ ì¢‹ì„ë“¯í•˜ë‹¤.
# srv íŒŒì¼ì˜ ê²½ìš°
```

    388
    (299, 299, 3)
    388
    <start>  mv agusta  raid <end>



```python
image_model = tf.keras.applications.InceptionV3(include_top=False,
                                                weights='imagenet')
new_input = image_model.input
hidden_layer = image_model.layers[-1].output

image_features_extract_model = tf.keras.Model(new_input, hidden_layer)
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5
    87916544/87910968 [==============================] - 1s 0us/step
    87924736/87910968 [==============================] - 1s 0us/step



```python
# imagenet ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„±ì¶”ì¶œ
BATCH_SIZE = 64
image_dataset = tf.data.Dataset.from_tensor_slices(X_pre_train)
image_dataset = image_dataset.batch(BATCH_SIZE)
np_batch_features = np.array([np.zeros((64,2048))])   # ì—¬ê¸°ì„œ 64ëŠ” batch_sizeê°€ ì•„ë‹ˆë¼ InceptionV3ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì•„ì›ƒí’‹ ëª¨ì–‘
for img in image_dataset:
    batch_features = image_features_extract_model(img)
    batch_features = tf.reshape(batch_features,
                              (batch_features.shape[0], -1, batch_features.shape[3]))
    for temp_img in batch_features:
      np_batch_features = np.append(np_batch_features, np.array(temp_img).reshape(1,64,2048), axis=0)

np_batch_features = np_batch_features[1:]
```


```python
np_batch_features.shape
```




    (388, 64, 2048)




```python
# y_train ì€ ìº¡ì…˜ ë¬¸ì¥ì¸ë°, í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ ë¬¸ì¥ë“¤ì„ ë‹¨ì–´ë³„ ë²¡í„°í™” í•´ì¤€ë‹¤. (cap_vector)

top_k = 5000
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,
                                                  oov_token="<unk>",
                                                  filters='!"#$%&*+.-;?@[]^`{}~ ')
tokenizer.fit_on_texts(y_train)
train_seqs = tokenizer.texts_to_sequences(y_train)
cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')
print(y_train[3])
print(len(cap_vector))
cap_vector[3]
```

    <start> erskine river at lorne <end>
    388





    array([  2, 304,  31,   9, 305,   3,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
             0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)




```python
#size_test = int( 0.01 * len(cap_vector))
size_test = 1       # test ëŠ” ë”± ë§ˆì§€ë§‰ì— í™•ì¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ì.
X_test = np.array(np_batch_features)[0:size_test]
X_train = np.array(np_batch_features)[size_test:]
y_test = cap_vector[0:size_test]
y_train = cap_vector[size_test:]
len(X_test), len(y_test), len(X_train), len(y_train)
```




    (1, 1, 387, 387)




```python
X_test.shape
```




    (1, 64, 2048)



# ì „ì—­ë³€ìˆ˜


```python
BUFFER_SIZE = 1000
embedding_dim = 512
units = 512
vocab_size = top_k + 1
num_steps = len(X_train) // BATCH_SIZE
features_shape = 2048
attention_features_shape = 64
```

## ë°ì´í„°ì…‹ ì§€ì •


```python
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
```

# optimizer, loss func..


```python
optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits = True, reduction='none')
def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)

    mask = tf.cast(mask, dtype = loss_.dtype)
    loss_ *= mask

    return tf.reduce_mean(loss_)
```

# model ì •ì˜


```python
class BahdanauAttention(tf.keras.Model):
  def __init__(self, units):
    '''
    W1, W2, V ëŠ” í•™ìŠµê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë²¡í„°
    '''
    super(BahdanauAttention, self).__init__()   # ë¶€ëª¨í´ë˜ìŠ¤ (tf.keras.Model.init()ì‚¬ìš©)
    self.W1 = tf.keras.layers.Dense(units)      # units(ì „ì—­) ìˆ˜ ì˜ nodeë¥¼ ê°–ëŠ” W1 ê°€ì¤‘ì¹˜
    self.W2 = tf.keras.layers.Dense(units)      # ìœ„ì™€ ë™ w2 ê°€ì¤‘ì¹˜
    self.V = tf.keras.layers.Dense(1)           # Vê°€ì¤‘ì¹˜ëŠ” í•˜ë‚˜ë¡œ.

  def call(self, features, hidden):
    '''
    feautures : ì´ë¯¸ì§€ì˜ í”¼ì³ë§µ
    hidden : hidden state
    '''
    hidden_with_time_axis = tf.expand_dims(hidden, 1)
    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +
                                         self.W2(hidden_with_time_axis)))
    score = self.V(attention_hidden_layer)
    attention_weights = tf.nn.softmax(score, axis=1)
    context_vector = attention_weights * features
    context_vector = tf.reduce_sum(context_vector, axis=1)
    return context_vector, attention_weights
```


```python
class CNN_Encoder(tf.keras.Model):
    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        self.fc = tf.keras.layers.Dense(embedding_dim)
    def call(self, x):
        x = self.fc(x)
        x = tf.nn.relu(x)
        return x
```


```python
class RNN_Decoder(tf.keras.Model):
  def __init__(self, embedding_dim, units, vocab_size):
    super(RNN_Decoder, self).__init__()
    self.units = units

    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc1 = tf.keras.layers.Dense(self.units)
    self.fc2 = tf.keras.layers.Dense(vocab_size)

    self.attention = BahdanauAttention(self.units)

  def call(self, x, features, hidden):
    context_vector, attention_weights = self.attention(features, hidden)
    x = self.embedding(x)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
    output, state = self.gru(x)
    x = self.fc1(output)
    x = tf.reshape(x, (-1, x.shape[2]))
    x = self.fc2(x)
    return x, state, attention_weights

  def reset_state(self, batch_size):
    return tf.zeros((batch_size, self.units))
```

# trian step ì •ì˜


```python
encoder = CNN_Encoder(embedding_dim)
decoder = RNN_Decoder(embedding_dim, units, vocab_size)
```


```python
checkpoint_path = "./drive/MyDrive/dataset/wiki/checkpoints/train"
ckpt = tf.train.Checkpoint(encoder=encoder,
                           decoder=decoder,
                           optimizer=optimizer)
ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)
```


```python
start_epoch = 0
if ckpt_manager.latest_checkpoint:
  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])
  # restoring the latest checkpoint in checkpoint_path
  ckpt.restore(ckpt_manager.latest_checkpoint)
```


```python
# adding this in a separate cell because if you run the training cell
# many times, the loss_plot array will be reset
loss_plot = []
```


```python
@tf.function
def train_step(img_tensor, target):
  loss = 0

  # initializing the hidden state for each batch
  # because the captions are not related from image to image
  hidden = decoder.reset_state(batch_size=target.shape[0])

  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)

  with tf.GradientTape() as tape:
      features = encoder(img_tensor)

      for i in range(1, target.shape[1]):
          # passing the features through the decoder
          predictions, hidden, _ = decoder(dec_input, features, hidden)

          loss += loss_function(target[:, i], predictions)

          # using teacher forcing
          dec_input = tf.expand_dims(target[:, i], 1)

  total_loss = (loss / int(target.shape[1]))

  trainable_variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, trainable_variables)

  optimizer.apply_gradients(zip(gradients, trainable_variables))

  return loss, total_loss
```


```python
EPOCHS = 20

for epoch in range(start_epoch, EPOCHS):
    start = time.time()
    total_loss = 0

    for (batch, (img_tensor, target)) in enumerate(dataset):
        batch_loss, t_loss = train_step(img_tensor, target)
        total_loss += t_loss

        if batch % 100 == 0:
            average_batch_loss = batch_loss.numpy()/int(target.shape[1])
            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')
    # storing the epoch end loss value to plot later
    loss_plot.append(total_loss / num_steps)

    if epoch % 5 == 0:
      ckpt_manager.save()

    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')
    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\n')
```

    Epoch 5 Batch 0 Loss 1.7554
    Epoch 5 Loss 1.809353
    Time taken for 1 epoch 157.57 sec

    Epoch 6 Batch 0 Loss 1.1847
    Epoch 6 Loss 1.441257
    Time taken for 1 epoch 46.38 sec

    Epoch 7 Batch 0 Loss 1.0604
    Epoch 7 Loss 1.354600
    Time taken for 1 epoch 45.60 sec

    Epoch 8 Batch 0 Loss 1.2198
    Epoch 8 Loss 1.334983
    Time taken for 1 epoch 45.94 sec

    Epoch 9 Batch 0 Loss 1.0771
    Epoch 9 Loss 1.477467
    Time taken for 1 epoch 46.14 sec

    Epoch 10 Batch 0 Loss 1.2247
    Epoch 10 Loss 1.347626
    Time taken for 1 epoch 45.70 sec

    Epoch 11 Batch 0 Loss 1.0726
    Epoch 11 Loss 1.260275
    Time taken for 1 epoch 46.71 sec

    Epoch 12 Batch 0 Loss 1.2278
    Epoch 12 Loss 1.200646
    Time taken for 1 epoch 46.94 sec

    Epoch 13 Batch 0 Loss 1.2482
    Epoch 13 Loss 1.216185
    Time taken for 1 epoch 46.13 sec

    Epoch 14 Batch 0 Loss 1.0949
    Epoch 14 Loss 1.133379
    Time taken for 1 epoch 45.98 sec

    Epoch 15 Batch 0 Loss 1.1599
    Epoch 15 Loss 1.292254
    Time taken for 1 epoch 46.25 sec

    Epoch 16 Batch 0 Loss 0.9927
    Epoch 16 Loss 1.213316
    Time taken for 1 epoch 46.76 sec

    Epoch 17 Batch 0 Loss 1.0226
    Epoch 17 Loss 1.102528
    Time taken for 1 epoch 46.03 sec

    Epoch 18 Batch 0 Loss 0.8826
    Epoch 18 Loss 1.238099
    Time taken for 1 epoch 46.58 sec

    Epoch 19 Batch 0 Loss 0.8167
    Epoch 19 Loss 1.090817
    Time taken for 1 epoch 47.73 sec

    Epoch 20 Batch 0 Loss 0.8715
    Epoch 20 Loss 1.050230
    Time taken for 1 epoch 48.49 sec



# ì˜ˆì¸¡ ëª¨ë¸


```python
def calc_max_length(tensor):
    return max(len(t) for t in tensor)

max_length = max_length = calc_max_length(y_train)

def evaluate(image):
    attention_plot = np.zeros((max_length, attention_features_shape))

    hidden = decoder.reset_state(batch_size=1)

    temp_input = tf.expand_dims(url_to_image(image), 0)
    img_tensor_val = image_features_extract_model(temp_input)
    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],
                                                 -1,
                                                 img_tensor_val.shape[3]))

    features = encoder(img_tensor_val)

    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
    result = []

    for i in range(max_length):
        predictions, hidden, attention_weights = decoder(dec_input,
                                                         features,
                                                         hidden)

        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()

        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()
        result.append(tokenizer.index_word[predicted_id])

        if tokenizer.index_word[predicted_id] == '<end>':
            return result, attention_plot

        dec_input = tf.expand_dims([predicted_id], 0)

    attention_plot = attention_plot[:len(result), :]
    return result, attention_plot
```


```python
# captions on the validation set
image_url = df['image_url'][20]
real_caption = df['caption_title_and_reference_description'][20]
result, attention_plot = evaluate(image_url)

cv2_imshow(url_to_image(image_url)*255)
print('Real Caption:', real_caption)
print('Prediction Caption:', ' '.join(result))
```

<img src = '/assets/images/project/source_3.png' width='50%' height='50%'>



    Real Caption: <start> entering a mining settlement from jamieson <end>
    Prediction Caption: a np on either may river the wyndham's <end>

# ê²°ê³¼ í•´ì„
1. ê´‘ì‚° ì…êµ¬ ê¸¸ì— ëŒ€í•œ ì´ë¯¸ì§€ë¥¼ 'wyndhamì˜ river' ì •ë„ì˜ ìº¡ì…˜ì„ ìƒì„±í–ˆë‹¤.    
2. ê¸¸ì˜ ìƒ‰ê¹”ë§Œ ë‹¤ë¥´ë‹¤ë©´ ì¶©ë¶„íˆ ê°•ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì¼ ê²ƒ. ë¬¼ë¡  ì„±ê³µì ì¸ ê²°ê³¼ë¼ê³  ë³´ê¸´ í˜ë“¤ë‹¤.  

# ê°œì„ í•  ì 
1. kaggleì˜ wikií•™ìŠµë°ì´í„°ê°€ ë„ˆë¬´ ë°©ëŒ€í•˜ë‹¤ ë³´ë‹ˆ, ì¼ë¶€ë¥¼ ìª¼ê°œì–´ ì‚¬ìš©í•˜ê¸°ë„ í–ˆê³ , ìº¡ì…˜ì˜ ê²½ìš° ë„ˆë¬´ ë””í…Œì¼(ì§€ëª…ì´ ë“¤ì–´ê°„ë‹¤ë˜ê°€ ë“±)í•œ ë¶€ë¶„ì„ ì—†ì• ëŠ” ê³¼ì •ì´ í•„ìš”í–ˆë‹¤ê³  ìƒê°í•œë‹¤. (í†µê³„ì  ì–¸ì–´ëª¨ë¸)  
2. í•™ìŠµë°ì´í„°ì˜ ì¦ê°•ì´ ë” í•„ìš”í–ˆë‹¤ê³  ìƒê°í•œë‹¤. (ìƒ‰ì±„ ë³€ê²½, íšŒì „ ë“±)  
3. ì „ì²´ êµ¬ì¡°ë¥¼ ì¢€ ë” ìª¼ê°œì„œ ì‚¬ìš©í•  ê±¸ ì‹¶ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ training.py ìœ¼ë¡œ ê°€ì¤‘ì¹˜íŒŒì¼ì„ ìƒì„±í•œë‹¤ê±°ë‚˜, main.py result.py ë“± ìœ¼ë¡œ í•˜ì—¬ ê°€ì‹œì„±, ì ‘ê·¼ì„±ì„ ë†’ì´ëŠ” ê²Œ í•„ìš”í•´ë³´ì¸ë‹¤. (ì´ í”„ë¡œì íŠ¸ëŠ” ëª‡ë‹¬ ì „ì— ì‹œë„í–ˆë˜ ê²ƒì´ê¸° ë–„ë¬¸ì— ë¶€ì¡±í•œì ì´ ë§ì´ ë³´ì´ëŠ” ë“¯ í•˜ë‹¤.)  

<br>

    ğŸŒœ ê°œì¸ ê³µë¶€ ê¸°ë¡ìš© ë¸”ë¡œê·¸ì…ë‹ˆë‹¤. ì˜¤ë¥˜ë‚˜ í‹€ë¦° ë¶€ë¶„ì´ ìˆì„ ê²½ìš°
    ì–¸ì œë“ ì§€ ëŒ“ê¸€ í˜¹ì€ ë©”ì¼ë¡œ ì§€ì í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤! ğŸ˜„

[ë§¨ ìœ„ë¡œ ì´ë™í•˜ê¸°](#){: .btn .btn--primary }{: .align-right}
