---
title:  "정규화, 표준화 그리고 PCA"

categories:
  - ds
tags:
  - []

toc: true
toc_sticky: true

date: 2022-02-20
last_modified_at: 2022-02-21
---

## 정규화와 표준화
> 정규화 (normalization)

정규화는 모든 값들을 0과 1사이의 값으로 단순하게 축소한다.  
예를 들어 0~100까지의 값중 35 는 0.35가 될 뿐이다.   
식 : x = (원래값 - 최댓값) / (최댓값 - 최솟값)  

~~~
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

df_scaled = scaler.fit_transform(df)
~~~

> 표준화 (standardization)

표준화는 데이터를 0을 중심으로 양쪽으로 데이터를 분포시킨다.  
정확히는 0의 평균을 갖고, 1의 표준편차를 갖도록 변환하는 것.  

식 : x = (원래값 - 평균) / 표준편차

~~~
from sklearn.preprocessing import StandardScaler
sclaer = StandardScaler()

df_scaled = scaler.fit_transform(df)
~~~

> fit 과 fit transform 의 차이

fit은 해당 데이터에 맞춤으로 모델(객체)을 설정해주는 것.  
fit_transform 은 fit함과 동시에 해당 데이터를 모델을 사용해서 변형해주어 return함.

## 분산과 공분산

> 분산 (variance)

하나의 feature가 갖는 '평균으로 부터 퍼져있는 정도' 이다.  
식 : Var(x) = E [(X-X평균)^2]  

분산의 양의 제곱근이 표준편차(Standard Deviation)  

> 공분산 (Covariance)

두 feature가 갖는 공동 변화량이다. 직관적으로는 이해하기 힘들더라.. 그 의미를 파악하자면  
**0보다크면 X가 증가할때 Y도 증가한다.(양의 상관관계)**  
**0보 작으면 서로 음의 상관관계**  
식 : Cov(X,Y) = E[(X-X평균)(Y-Y평균)]

> 상관계수 (Correlation coefficient)

공분산과 자연스럽게 이어지는데, 상관계수는 **얼만큼** 상관관계를 갖는지도 알려준다.  

식 : Corr(X,Y) = Cov(X,Y) / (sd(X)*sd(Y))  (sd는 표준편차)

## PCA
**표준화or정규화 필수!!!!**  
PCA는 고 차원(feature 종류가 많을 때)의 데이터셋을 차원축소 하고자 할 때 사용한다.  
여기서 중요한건 PCA는 특정한 feature를 선택(selection)하는 것이 아니라  
모든 feature의 특징을 담아내는 feature로 추출(Extraction) 한다는 것이다.  

어떻게 Extraction할 것이냐 !!
바로 **축을 고르는 것** 이다.  

어떠한 축에 모든 feature들을 projection시켰을 때, '가장 그 정보들을 많이 담는다'면 그 축은 모든 feature의 특징을  
잘 담고 있는 축이 될 것이다.  
'정보를 많이 담는다'는 것은 공분산이 가장 큰 것이다라고 이해했으며,  
그 축에 projection시킨 값들의 집합하나가 하나의 차원이 될 것이다.  
그리고 그 다음 축은 첫번째 축과 직교인 축으로 고르게 될 것이다.  
<img src="/assets/images/source_10.png" width="60%" height="60%" title="제목" alt="아무거나"/>

~~~
features = df.loc[:,'bill_length_mm':'body_mass_g']
species = df['species']
~~~
↪️ df에서 'bill_length_mm'부터 'body_mass_g' 까지의 feature의 데이터만 가져오고, 'species'만 가져온다.  
~~~
import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features = pd.DataFrame(scaler.fit_transform(features), columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])
~~~
↪️ 표준화를 진행한다. (**필수!!!!!**)
~~~
import numpy as np
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
extracted_df = pd.DataFrame(pca.fit_transform(features), columns=['PC1', 'PC2'])
~~~
↪️ pca를 실행한 후 'PC1', 'PC2'로 저장된(자동으로 이름 이렇게 지음) 두 차원만 불러온다.  
두 차원으로 차원축소 성공 !
<br>

    🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
    언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}
