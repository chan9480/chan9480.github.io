var store = [{
        "title": "test게시물",
        "excerpt":"test_제목  test 게시물입니다.   test_제목 2  test2 목차 테스트 입니다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Example_sub"],
        "tags": ["Test","Test_2"],
        "url": "http://localhost:4000/example_sub/test/",
        "teaser": null
      },{
        "title": "파이썬, 자료구조",
        "excerpt":"서론     옛날에 RTOS관련 대학원에 진학해보려 했다가 받은 질문중에 하나가 스택과 큐의 차이점과 예시를 설명하라는 내용이었다.  스택은 쌓아놓은 책을 위에서 부터 다시꺼낸다면 큐는 만화책 반납통같이 위에서 넣으면 밑에서 꺼내는 개념이라고 말씀드렸다. 물론 틀린건 아니지만 지금 생각해보면 그냥 얘는 개념만 알고 실사용은 해보지 않은 친구라고 생각하셨을 것 같다. 오늘 각각을 구현해보면서 이해해보는 시간을 가졌다.    큐(queue) 를 연결리스트로 구현     연결리스트   큐        class Node():     def __init__(self, data):         self._data = data         self._next = None     node 들을 선언했다.    class Queue():     def __init__(self):         self._front = None         self._rear = None      ‘맨앞’ (꺼내는곳) 와 ‘맨뒤’ (넣는곳) 자리를 마련했다. 큐를 들여다볼수 있는 창문 두개라고 비유하여 이해했다.        def enqueue(self,item):         new_node = Node(item)         if self._front == None: # 빈 큐 라면 '맨앞' '맨뒤'에 모두 같은 노드를 넣어준당.             self._front = new_node             self._rear = self._front         else: # 뭔가 있는 큐라면 뒤의 노드에 대해서만 새로운 노드를 연결해주면 된다.             self._rear._next = new_node           #기존 '맨뒤노드'의 next에 새로운 노드를 넣어준당. (연결!)             self._rear = self._rear._next         #큐의 '맨뒤' 에 새로운 노드를 위치시킨당. (맨뒤 업데이트!)      값 item을 큐에 추가하는 함수를 정의했다. (내부에서는 item을 value로 갖는 node를 연결해준것)        def dequeue(self):         if self._front == None: # 빈큐라면 None을 리턴             return None         else : #뭔가 있는 큐라면 맨앞에서 뽑은 값 리턴 + '맨앞' 업데이트             temp = self._front._data       # '맨앞'노드의 데이터를 잠깐 빼두자.             self._front = self._front._next  #'맨앞' 노드의 next 에 있는 노드를 큐의 '맨앞'으로 둔다.         if self._front ==None:             # 이렇게 되는 경우는 1개짜리의 큐에서 dequeue를 진행한 후가 될거다.             self._rear = None           #그러면 '맨뒤'도 비워주자 (이거 안하면 '맨뒤'에 꺼낸 노드가 아직 남아있음)         return temp      빈큐라면 None을 리턴, 빈큐가 아니라면 제일 먼저 넣은 하나를 큐에서 제거하는 함수를 정의했다.        def return_queue(self):         result = []         temp = self._front      # 초기값은 '맨앞' node!         while temp!= None:      # temp에 None이 들어가버리면 다 끝난거다!             result.append(temp._data)             temp = temp._next   # temp에는 그 다음 node 넣장. (None이 들어갈수도 있음 그러면 loop끝)         return result      현재 큐를 list로 리턴하는 함수를 정의했다.    스택(stack)을 list로 구현     스택의 경우 넣는곳과 빼는곳이 같은 자료구조이다.  그런데 파이썬에서는 list의경우 list.append(x) 와 list.pop() 으로 구현이 가능하다.  (물론 큐도 가능하다. 그러나 pop(0)를 사용하여야 하는데, 그렇게 되면 리스트 끝에서부터 index 0 의 방향으로 탐색을 하기때문에, 비효율적이다.)        class Stack():     def __init__(self):         self._data = []      def push(self, item):         self._data.append(item)       def pop(self):         return self._data.pop() if self._data else None       def return_stack(self):         return self._data     간단해서 코드 리뷰는 생략했다.    데크 구현    동작은 하는데, pop에서 비효율적으로 탐색하는 부분이 있다.(top에써 꺼낼라해도 bottom부터 탐색을 해야하는 부분) 양방향 연결리스트로 수정을 해서 업데이트를 해야겠다  https://user-images.githubusercontent.com/84547813/143423521-f8877649-559d-4a49-aa82-c6f69335bd04.png    class Node:     def __init__(self, value, next=None):         self.value = value         self.next = next  class Deque:     def __init__(self):         self.top = None         self.bottom = None       def append(self, item):         if self.top == None:        # self.top이 비어있다면             self.top = Node(item)   # self.top 에도 bottom에도 뉴 노드를 넣어준다.             self.bottom = self.top         else :             node = Node(item)             self.top.next = node    # 현재 top의 next에 뉴 노드를 넣어주고             self.top = node         # 이제 top은 뉴 노드로 한다.       def appendleft(self, item):         node = Node(item)         if self.bottom == None :        # bottom이 비었다면 top, bottom에 둘다 넣어줘             self.top = node             self.bottom = node         else :             node.next = self.bottom     # 뉴 노드의 next 를 bottom으로 설정 후, 앞으로 bottom은 뉴노드다!             self.bottom = node      top 방향과 bottom방향에 값을 추가할 수 있는 함수 두개를 정의했다. ````     def pop(self):         # top 추출         if self.top == None:        # top이 None 이면 return None             return None         elif self.top == self.bottom:   # top과 bottom이 같다면(None은 아님)             result = self.top.value     # value를 리턴하고, top bottom을 비우자 (None)             self.top =None             self.bottom = None             return result         else :                          # 이제 정상적으로 길이 1이상의 데크라면             node = self.bottom          # 초기값 bottom부터 next가 top인 곳까지 node를 찾아서             result = self.top.value     # (리턴값은 어쨋든 top value)             while node !=None:          # top을 업데이트해주자.                 if node.next == self.top:                     self.top = node                 node = node.next             return result    def popleft(self):     # bottom 추출     if self.bottom == None:         return None     elif self.top == self.bottom:         self.top = None         result = self.bottom.value         self.bottom = None         return result     else :         result = self.bottom.value         self.bottom = self.bottom.next         return result ```` &gt; top에서 추출하는 pop과 bottom에서 추출하는 popleft를 정의했다. ````  def ord_desc(self):     node = self.bottom      # 초기값은 bottom 부터     result = []     while node != None :    # 노드에 None이 들어있다면 종료         result.append(node.value)         node = node.next     return result ```` &gt; 현재 데크를 리스트로 리턴하는 함수를 정의했다.  ","categories": ["Something_else"],
        "tags": ["algorithm","data_structure"],
        "url": "http://localhost:4000/something_else/4th/",
        "teaser": null
      },{
        "title": "게임 데이터 분석 기초",
        "excerpt":"   목차  1. 샘플데이터 불러오기  2. 데이터 전처리  3. 필수포함 주제  4. 다음 분기에 어떤 게임을 설계해야 할까   1. 샘플 데이터 불러오기  링크 : https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv   import pandas as pd df=pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv') print(df.columns) df.head() # 의미가 있을만한 feature : 년도, 플랫폼. 장르. 제작한 회사??, 출고량     Index(['Unnamed: 0', 'Name', 'Platform', 'Year', 'Genre', 'Publisher',        'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'],       dtype='object')                           Unnamed: 0       Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales                       0       1       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       0.04       0       0       0                 1       2       The Munchables       Wii       2009.0       Action       Namco Bandai Games       0.17       0       0       0.01                 2       3       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       0.02       0                 3       4       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       0.04       0       0       0                 4       5       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       0.12       0.09       0       0.04            2. 데이터 전처리, EDA   import numpy as np  # 첫번째 열 삭제 try:   df=df.drop(['Unnamed: 0'], axis=1) except:   pass  # 출고량 numerical value 화 (전부 (M이 있을 수 있음) 1000을 곱해주되, K가 들어가면 안곱해줌, ) def to_int(x):   x=str(x)   if 'K' in x:     x=x.replace('K','')     x=int(x)   else:     x=x.replace('M','')     x=int(float(x)*1000)   return x try:   df['NA_Sales']=df['NA_Sales'].apply(to_int)   df['EU_Sales']=df['EU_Sales'].apply(to_int)   df['JP_Sales']=df['JP_Sales'].apply(to_int)   df['Other_Sales']=df['Other_Sales'].apply(to_int) except:   pass # 각 판매량을 각게임의 전체판매량의 비중으로 계산하여 feature 추가( 예를 들어 30, 30, 20 , 20 이면 0.3, 0.3, 0.2, 0.2) # 이유 : 특정 게임의 흥행을 고려하지 않는 feature 필요. df['Game_sales']=df['NA_Sales']+df['EU_Sales']+df['JP_Sales']+df['Other_Sales'] df['NA_Sales(rate)']=df['NA_Sales']/df['Game_sales'] df['EU_Sales(rate)']=df['EU_Sales']/df['Game_sales'] df['JP_Sales(rate)']=df['JP_Sales']/df['Game_sales'] df['Other_Sales(rate)']=df['Other_Sales']/df['Game_sales']  # 년도가 이상하게 표기되어있는 경우. 수정 (0 이면 2000년, 98이면 1998년 등) def re_year(x):   if x&gt;1900 and x&lt;2100:     return x   elif x&lt;=21:     return 2000+x   elif x&gt;21:     return 1900+x   else:     return np.nan df['Year']=df['Year'].apply(re_year) df=df.dropna() df.head()                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 1       The Munchables       Wii       2009.0       Action       Namco Bandai Games       170       0       0       10       180       0.944444       0.00       0.0       0.055556                 2       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       20       0       20       0.000000       0.00       1.0       0.000000                 3       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 4       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       120       90       0       40       250       0.480000       0.36       0.0       0.160000            3-1 지역에 따라서 선호하는 게임 장르가 다를까      가정1 : 지역에 따른 ‘전체 게임 출고량’은 오직 인구수에만 종속된다. (선호도 이외의 영향은 없다)   가정2 : 게임의 전지역 출고량은 지역 장르선호도와 독립적이다. (특정 게임의 흥행은 선호도에 가중치를 주지않는다.)   from scipy.stats import chi2_contingency  # 년도에 상관없이 지역별로 각 장르 게임들의 출고량 rate 합을 그룹화 grouped_df1 grouped_df1=df.groupby('Genre').sum()[['NA_Sales(rate)','EU_Sales(rate)','JP_Sales(rate)', 'Other_Sales(rate)']] grouped_df1_T=grouped_df1.transpose() grouped_df1_T['Total_sales']=grouped_df1_T.sum(axis=1) grouped_df1_T.head()                    Genre       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Total_sales                       NA_Sales(rate)       1525.619292       342.188702       348.445844       807.461452       501.594820       319.931602       675.419334       432.557585       692.748044       409.577643       1210.555555       195.972163       7462.072036                 EU_Sales(rate)       823.705975       208.330453       143.982251       327.132529       211.535672       120.129892       375.877842       232.452620       379.688596       196.063888       538.379128       207.948389       3765.227234                 JP_Sales(rate)       658.357007       669.495053       294.008705       436.876869       106.652981       99.585575       72.600294       725.787645       108.868986       185.525061       388.643571       224.475923       3970.877670                 Other_Sales(rate)       232.317726       47.985792       48.563200       109.529150       53.216527       26.352931       94.102530       74.202151       98.694374       54.833408       161.421746       40.603524       1041.823060            # 2 sample 카이제곱검정을 통해 두변수가 독립적이라는 귀무가설을 검정. chi2 = chi2_contingency(grouped_df1_T[grouped_df1_T.columns.difference(['Total_sales'])]) chi2   (1834.559710433277,  0.0,  33,  array([[1488.73850969,  582.62976243,  383.67180728,  772.3979737 ,           401.13232067,  260.069752  ,  559.65540272,  673.14873972,           588.14360876,  388.72616642, 1056.3610598 ,  307.39693302],         [ 751.19065507,  293.98449093,  193.59388796,  389.73811456,           202.40414873,  131.22651567,  282.39204255,  339.65873755,           296.76667854,  196.1442266 ,  533.02077654,  155.10695933],         [ 792.21943666,  310.04143385,  204.16766346,  411.02496081,           213.45912599,  138.39388924,  297.81582526,  358.2103317 ,           312.97557992,  206.85729735,  562.13348299,  163.57864294],         [ 207.85139859,   81.34431278,   53.5666413 ,  107.83895093,            56.00440462,   36.30984309,   78.13672947,   93.98219103,            82.11413278,   54.27230963,  147.48468066,   42.91746471]]))   p값이 0 이므로 지역에 따른 장르는 독립적이지 않다.  즉, 지역에 따라 선호하는 게임이 유의미하게 다르다.   3-2 연도별 게임의 트렌드가 있을까     트렌드? 게임의 흥망을 고려해야 하는가?  -&gt; 어떤 게임이 대박이 났다면 대박난 게임은 트렌드를 잘탔기때문인가? 아니면 그저 잘 만들어졌기 때문인가?  예시   2000년도에 A라는 게임이 대박을 쳤고 action 장르라고 해보자.  데이터 분석을 할 때, 이 A 게임으로인한 2000년 action 장르 트렌드영향도를  더 크게 ?  -&gt; 대답 : 영향을 더 크게 고려해야함. (즉 rate 가 아닌 출고량으로 분석하겠다)           가정1 : 각 년도에 나오는 각 장르당 게임수는 트렌드와 독립적이다.       ( 매년 무조건 1만가지의 게임만 출시할수 있다는 제한이 있는 경우와 없는경우를 생각해보면 편할 것)       # 년도와 장르별로 게임 판매량을 그룹화함 grouped_df2=df.groupby(['Year', 'Genre']).sum()[['Game_sales']]  # 년도별 출시게임의 갯수를 저장 count=df.groupby(['Year']).count() count=count.reset_index()['Game_sales']  # 년도index, 장르columns 대하여 정리 grouped_df2=grouped_df2.pivot_table(values='Game_sales', index='Year', columns='Genre') grouped_df2=grouped_df2.reset_index() grouped_df2=grouped_df2.fillna(0) grouped_df2['Count']=count grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       0       1980.0       340.0       0.0       770.0       2700.0       0.0       0.0       0.0       0.0       7070.0       0.0       0.0       0.0       8                 1       1981.0       14790.0       0.0       0.0       0.0       6920.0       2250.0       480.0       0.0       10020.0       440.0       780.0       0.0       46                 2       1982.0       6500.0       0.0       0.0       870.0       5030.0       10040.0       1570.0       0.0       3810.0       0.0       1060.0       0.0       36                 3       1983.0       2860.0       400.0       0.0       2140.0       6930.0       780.0       0.0       0.0       490.0       0.0       3200.0       0.0       17                 4       1984.0       1850.0       0.0       0.0       1450.0       690.0       3140.0       5950.0       0.0       31100.0       0.0       6170.0       0.0       14            # 특정 년도는 게임수가 적어 묶어주겠음 (A개 이상으로) A=1200 temp=0 for i in grouped_df2.T.columns:   grouped_df2.T[i]=grouped_df2.T[i]+temp   if int(grouped_df2['Count'][i]) &lt; A :     temp=grouped_df2.T[i]     grouped_df2=grouped_df2.T.drop([i],axis=1).T   else:     temp=0 #마지막에도 80개이하면 버려지게됨. 끝에 붙여줌 if temp[temp.index=='Count'][0] &lt; A:   grouped_df2.iloc[-1]=grouped_df2.iloc[-1]+temp  #년도도 더해져버림... 적절히 평균으로 만들어줌. def qq(x):   i=1   while x/i&gt;2021 :     i+=1   return x/i grouped_df2['Year']=grouped_df2['Year'].apply(qq) grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       123220.0       32930.0       84759.0       39350.0       280500.0       94000.0       96939.0       122690.0       106629.0       33980.0       106290.0       25550.0       1255.0                 21       1999.5       157599.0       28570.0       84349.0       64460.0       106010.0       19460.0       139800.0       126390.0       53669.0       41480.0       164670.0       48020.0       1541.0                 23       2002.5       154630.0       13190.0       48710.0       39420.0       88770.0       7040.0       82330.0       75380.0       74740.0       32029.0       121270.0       13460.0       1600.0                 25       2004.5       161670.0       16980.0       36440.0       85790.0       70150.0       28670.0       102009.0       82460.0       88490.0       60170.0       120639.0       12480.0       1674.0                 27       2006.5       172480.0       35920.0       40160.0       158300.0       85230.0       34570.0       73010.0       95650.0       109230.0       70580.0       234359.0       13610.0       2200.0            # 각 출시년도에 대하여 Count로 나누어줌. (가정1) for i in grouped_df2.columns.difference(['Year','Count']):   grouped_df2[i]=grouped_df2[i]/grouped_df2['Count'] grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       98.183267       26.239044       67.537052       31.354582       223.505976       74.900398       77.242231       97.760956       84.963347       27.075697       84.693227       20.358566       1255.0                 21       1999.5       102.270604       18.539909       54.736535       41.829981       68.792992       12.628164       90.720311       82.018170       34.827385       26.917586       106.859182       31.161583       1541.0                 23       2002.5       96.643750       8.243750       30.443750       24.637500       55.481250       4.400000       51.456250       47.112500       46.712500       20.018125       75.793750       8.412500       1600.0                 25       2004.5       96.577061       10.143369       21.768220       51.248507       41.905615       17.126643       60.937276       49.259259       52.861410       35.943847       72.066308       7.455197       1674.0                 27       2006.5       78.400000       16.327273       18.254545       71.954545       38.740909       15.713636       33.186364       43.477273       49.650000       32.081818       106.526818       6.186364       2200.0            chi2_1=chi2_contingency(grouped_df2[grouped_df2.columns.difference(['Count'])]) chi2_1   (1279.7439040830805,  1.724383653993331e-199,  108,  array([[ 118.47019129,   16.78986204,   32.5745123 ,   54.55699306,            62.97423775,   18.65798994,   52.46450991,   64.28669272,            70.50976375,   27.45568949,   91.15531716,   12.84964063,          2279.56894258],         [ 109.02005572,   15.45056757,   29.97610713,   50.20508837,            57.95090591,   17.16967856,   48.27951852,   59.15866891,            64.88533773,   25.26560281,   83.8840357 ,   11.82464991,          2097.73218419],         [ 100.89920463,   14.29966228,   27.74320144,   46.46533568,            53.63417102,   15.89071753,   44.68320059,   54.75196835,            60.05206038,   23.38358031,   77.63555455,   10.94383748,          1941.47313078],         [ 102.93759729,   14.58854787,   28.303677  ,   47.40404079,            54.71770286,   16.21174604,   45.58590253,   55.85808222,            61.26524813,   23.85598164,   79.20396875,   11.1649278 ,          1980.69528914],         [ 102.74194399,   14.56081945,   28.24988025,   47.31394002,            54.61370102,   16.18093241,   45.4992575 ,   55.75191287,            61.14880139,   23.81063862,   79.05342591,   11.14370664,          1976.93058537],         [ 101.38163783,   14.3680338 ,   27.87585106,   46.68750216,            53.89061412,   15.96669642,   44.89684607,   55.01375601,            60.33918958,   23.49538511,   78.00675638,   10.99616366,          1950.75597005],         [ 101.03873223,   14.31943644,   27.78156589,   46.52958987,            53.70833858,   15.91269187,   44.74499037,   54.82768164,            60.13510286,   23.41591609,   77.74291222,   10.95897107,          1944.15788038],         [ 101.30462451,   14.3571193 ,   27.85467552,   46.65203657,            53.84967678,   15.95456751,   44.86274074,   54.97196549,            60.29335365,   23.47753713,   77.94749951,   10.98781056,          1949.27410224],         [ 102.16087759,   14.47846941,   28.09011049,   47.04635174,            54.30482828,   16.08941967,   45.24193231,   55.43660286,            60.80296879,   23.67597539,   78.60633208,   11.0806824 ,          1965.74987477],         [ 102.35553281,   14.50605638,   28.1436328 ,   47.13599288,            54.40829957,   16.12007612,   45.32813535,   55.54223062,            60.9188215 ,   23.7210871 ,   78.75610695,   11.1017953 ,          1969.49537382]]))   p값 거의 0으로 년도와 장르는 독립적이지않다. 즉, 유의미한 연관이 있으며 연도별 트랜드는 존재한다.   심지어 게임 갯수를 2000개이상씩 묶어도 결과는 같다   3-3 출고량이 높은 게임에 대한 분석 및 시각화 프로세스   출고량 10000이상의 것으로 하겠다.   sorted_df=df.sort_values(by=['Game_sales'], ascending=False) sorted_df=sorted_df.reset_index(drop=True) i=0 while sorted_df['Game_sales'].iloc[i]&gt;10000:   i+=1 sorted_df=sorted_df.iloc[0:i] sorted_df #61개임.                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Wii Sports       Wii       2006.0       Sports       Nintendo       41490       29020       3770       8460       82740       0.501450       0.350737       0.045564       0.102248                 1       Super Mario Bros.       NES       1985.0       Platform       Nintendo       29080       3580       6810       770       40240       0.722664       0.088966       0.169235       0.019135                 2       Mario Kart Wii       Wii       2008.0       Racing       Nintendo       15850       12880       3790       3310       35830       0.442367       0.359475       0.105777       0.092381                 3       Wii Sports Resort       Wii       2009.0       Sports       Nintendo       15750       11010       3280       2960       33000       0.477273       0.333636       0.099394       0.089697                 4       Pokemon Red/Pokemon Blue       GB       1996.0       Role-Playing       Nintendo       11270       8890       10220       1000       31380       0.359146       0.283301       0.325685       0.031867                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...                 56       Super Mario All-Stars       SNES       1993.0       Platform       Nintendo       5990       2150       2120       290       10550       0.567773       0.203791       0.200948       0.027488                 57       Pokemon FireRed/Pokemon LeafGreen       GBA       2004.0       Role-Playing       Nintendo       4340       2650       3150       350       10490       0.413727       0.252622       0.300286       0.033365                 58       Super Mario 64       DS       2004.0       Platform       Nintendo       5080       3110       1250       980       10420       0.487524       0.298464       0.119962       0.094050                 59       Just Dance 3       Wii       2011.0       Misc       Ubisoft       6050       3150       0       1070       10270       0.589094       0.306719       0.000000       0.104187                 60       Call of Duty: Ghosts       X360       2013.0       Shooter       Activision       6720       2630       40       820       10210       0.658178       0.257591       0.003918       0.080313          61 rows × 14 columns    import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(40, 6)) plt.subplot(131) plt.pie(sorted_df['Publisher'].value_counts(), labels=sorted_df['Publisher'].value_counts().index, autopct='%.1f%%'); plt.title('Publisher');  plt.subplot(132) plt.pie(sorted_df['Platform'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Platform');  plt.subplot(133)  #plt.bar(sorted_df['Genre'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Genre'); grouped_df3=sorted_df.groupby(['Genre']).count() grouped_df3 plt.bar(grouped_df3.index, grouped_df3['Name']);      4. 다음 분기에 어떤 게임을 설계해야 할까   4-1. 장르는 트렌드와 시장규모 측면에서 유리한 것으로 선정  4-2. 퍼블리셔는 그 장르의 큰 규모 게임 사례가 있으면 좋겠다. (운영 노하우가 있을거라 가정)  4-3. 플랫폼은 퍼블리셔가 가장 많이 사용하는 플랫폼을 우선으로 (+ 장르에 상관없이 큰 규모의 게임 사례가 있으면 좋겠다.)      4-1. 장르의 트렌드    트렌드와 시장의 크기를 고려하여 Action 장르로 선정. (그외 후보 sports, shooting)    print('y축은 출고량') plt.figure(figsize=(30,10)) ax1 = plt.subplot2grid((3,7), (0,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Action']) plt.title('Action') ax1 = plt.subplot2grid((3,7), (0,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Adventure']) plt.title('Adventure') ax1 = plt.subplot2grid((3,7), (0,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Fighting']) plt.title('Fighting') ax1 = plt.subplot2grid((3,7), (0,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Misc']) plt.title('Misc') ax1 = plt.subplot2grid((3,7), (1,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Platform']) plt.title('Platform') ax1 = plt.subplot2grid((3,7), (1,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Puzzle']) plt.title('Puzzle') ax1 = plt.subplot2grid((3,7), (1,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Racing']) plt.title('Racing') ax1 = plt.subplot2grid((3,7), (1,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Role-Playing']) plt.title('Role') ax1 = plt.subplot2grid((3,7), (2,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Shooter']) plt.title('Shooter') ax1 = plt.subplot2grid((3,7), (2,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Simulation']) plt.title('Simulation') ax1 = plt.subplot2grid((3,7), (2,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Sports']) plt.title('Sports') ax1 = plt.subplot2grid((3,7), (2,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Strategy']) plt.title('Strategy');  ax1 = plt.subplot2grid((3,7), (0,4), colspan=3) plt.bar(grouped_df1.index, grouped_df1.sum(axis=1)) plt.title('Genre market size');   y축은 출고량         4-2. 퍼블리셔 별 Action 게임 데이터    action 게임중에서 총 출고량 2000 이상인 것들 중 각 퍼블리셔의 빈도수을 봄.  Ubisoft, Nintendo, Take-Two Interactive 세개의 퍼블리셔가 후보였는데  출고량이 가장 많은 게임을 운영했던 Take-Two Interactive 를 채택. (게임명 : Grand Theft Auto V)   action_df=df[df['Genre']=='Action'].sort_values(by=['Game_sales'], ascending=False) i=0 while action_df['Game_sales'].iloc[i]&gt;2000:   i+=1 action_df=action_df.iloc[0:i] print(action_df.groupby('Publisher').count()['Name'].sort_values(ascending=False)[0:3])  plt.figure(figsize=(30,5)) plt.subplot(131) plt.scatter(action_df[action_df['Publisher'] == 'Ubisoft']['Year'], action_df[action_df['Publisher'] == 'Ubisoft']['Game_sales']) plt.subplot(132) plt.scatter(action_df[action_df['Publisher'] == 'Nintendo']['Year'], action_df[action_df['Publisher'] == 'Nintendo']['Game_sales']) plt.subplot(133) plt.scatter(action_df[action_df['Publisher'] == 'Take-Two Interactive']['Year'], action_df[action_df['Publisher'] == 'Take-Two Interactive']['Game_sales'])  print(action_df[action_df['Publisher'] == 'Take-Two Interactive'][['Year','Game_sales']][0:5])   Publisher Ubisoft                 22 Nintendo                21 Take-Two Interactive    20 Name: Name, dtype: int64          Year  Game_sales 3483   2013.0       21390 14669  2004.0       20810 10913  2013.0       16380 5340   2002.0       16150 9786   2001.0       13100         4-3. 플랫폼 : 퍼블리셔와 밀접한 플랫폼    ‘Take-Two Interactive’ 가 사용했던 플랫폼 中   print(pd.DataFrame(df[df['Publisher']=='Take-Two Interactive'].groupby(['Platform']).count()).sort_values(by='Name', ascending=False)['Name'][0:3])  print(df[df['Publisher']=='Take-Two Interactive'][df['Platform']=='PS3'].sort_values(by='Year').iloc[-1]['Year'])   Platform X360    70 PS2     60 PS3     53 Name: Name, dtype: int64 2016.0   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.   This is separate from the ipykernel package so we can avoid doing imports until   결론 : Action 게임을,Take-Two Interactive 퍼블리셔와 함께, X360 혹은 PS3 플랫폼에서 설계할것이다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["pandas","시각화","가설검정"],
        "url": "http://localhost:4000/pj/section_1/",
        "teaser": null
      },{
        "title": "대출 승인 시스템의 승인 기준 머신러닝 예측",
        "excerpt":"AI SECTION2 PROJECT   목차  1) 데이터 선정 이유 및 문제 정의   2) 데이터를 이용한 가설 및 평가지표, 베이스라인 선택   3) EDA와 데이터 전처리   4) 머신러닝 방식 적용 및 교차검증   5) 머신러닝 모델 해석   # 설치 (처음1회) '''!pip install pandas_profiling==2.11 !pip install category_encoders !pip install shap'''   '!pip install pandas_profiling==2.11\\n!pip install category_encoders\\n!pip install shap'   1. 데이터 선정 이유 및 문제 정의  대출승인시스템을 선정하였다.  (kaggle url : https://www.kaggle.com/caiocampiao/loan-approval-systemlas)  이유     일단 기본적으로는 명확한 가이드라인을 따라 승인/거절이 나누어질 것이라 생각하였고, 그 외 분명 승인허가에 영향을 끼치는 적당한 외부 교란요인(특별한 경우)도 어느정도 있을 것이라 추측했다.    문제    매번 데이터를 확인해가며 수작업으로 승인/거절을 내리는 것(문제를 위한 가정) 보다 정확도 높은 모델을 사용하여 자동으로 결정되도록 해보자.    import pandas as pd  df=pd.read_csv(\"https://ai-bootcamp-chanwoo.s3.ap-northeast-2.amazonaws.com/HI/clientes.csv\") print(df.shape) df.head()   (614, 13)                           cod_cliente       sexo       estado_civil       dependentes       educacao       empregado       renda       renda_conjuge       emprestimo       prestacao_mensal       historico_credito       imovel       aprovacao_emprestimo                       0       LP001002       Male       No       0       Graduate       No       5849       0       NaN       360.0       1.0       Urban       Y                 1       LP001003       Male       Yes       1       Graduate       No       4583       1508       128.0       360.0       1.0       Rural       N                 2       LP001005       Male       Yes       0       Graduate       Yes       3000       0       66.0       360.0       1.0       Urban       Y                 3       LP001006       Male       Yes       0       Not Graduate       No       2583       2358       120.0       360.0       1.0       Urban       Y                 4       LP001008       Male       No       0       Graduate       No       6000       0       141.0       360.0       1.0       Urban       Y            2. 데이터를 이용한 가설 및 평가지표 선택  target     대출 승인 승인/거절 (aprovacao_emprestimo) (YorN)    Baseline model          Boolean target 이기 때문에 하나의 값이 70% 이상의 불균형이 아닌이상 둘중(Y,N) 큰 비율과              DummyClassifier를 사용한 결과의 평가지표를 비교하여 큰 것을 베이스라인 모델로 사용해보겠다.      target = ['aprovacao_emprestimo']   3. EDA와 데이터 전처리   EDA   # profiling # 결측치 확인 # 데이터 형 확인 # 데이터 분포 확인 from pandas_profiling import ProfileReport  ProfileReport(df, minimal=True, title = 'Section2 Project LAS', dark_mode=True)   Summarize dataset:   0%|          | 0/21 [00:00&lt;?, ?it/s]    Generate report structure:   0%|          | 0/1 [00:00&lt;?, ?it/s]    Render HTML:   0%|          | 0/1 [00:00&lt;?, ?it/s]     데이터 전처리   import numpy as np # 결측치 있는 feature # sexo                성별            # estado_civil        결혼유무        # dependentes         자녀수           # empregado           현재 직장유무 # emprestimo          대출금 # prestacao_mensal    한달분납금 # historico_credito   과거 신용  # 형변환 필요 # dependentes         자녀수        숫자로 변환필요 # renda_conjuge       배우자 수입   숫자로 변환 필요 # historico_credito   과거 신용     숫자인데 boolean 이라 인코딩필요(문자로 변환하자)  # high cardinality categorical 변수는 없음  #cod_cliente 고객 코드. -&gt; 마지막 숫자만 남겨놓자 #df['cod_cliente']=df['cod_cliente'].apply(lambda x : int(x[4:7])) df=df.drop('cod_cliente', axis=1)  # 성별 새롭게 'Dont_know'으로 처리 df['sexo']=df['sexo'].fillna('Dont_know')  #결혼 유무 새롭게 'Dont_know'으로 처리 df['estado_civil']=df['estado_civil'].fillna('Dont_know')  #현재 직장유무  새롭게 'Dont_know'으로 처리 df['empregado']=df['empregado'].fillna('Dont_know')  # 자녀수 숫자로 변환 # 자녀수 3이상은 3으로처리 def to_int(x):   try :     return int(x)   except :     return 3 df['dependentes']=df['dependentes'].apply(to_int)  #대출금 평균 df['emprestimo']=df['emprestimo'].fillna(df['emprestimo'].mean())  #한달분납금 평균 df['prestacao_mensal']=df['prestacao_mensal'].fillna(df['prestacao_mensal'].mean())  #과거 신용 새롭게 'Dont_know' 으로 처리 df['historico_credito']=df['historico_credito'].fillna(2) #과거 신용 문자로 변환 df['historico_credito']=df['historico_credito'].apply(str)  #배우자 수입 숫자로 변환 #변환 안되면 평균값 def to_float(x):   try:     return float(x)   except:     return np.nan df['renda_conjuge']=df['renda_conjuge'].apply(to_float) df['renda_conjuge']=df['renda_conjuge'].fillna(df['renda_conjuge'].mean())  # target을 True과 False 으로 df['aprovacao_emprestimo']=df['aprovacao_emprestimo']=='Y'  # 결측치 확인 print(\"결측치 갯수\", df.isnull().sum().sum())  # 데이터 형확인 df.info()   결측치 갯수 0 &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 614 entries, 0 to 613 Data columns (total 12 columns):  #   Column                Non-Null Count  Dtype   ---  ------                --------------  -----    0   sexo                  614 non-null    object  1   estado_civil          614 non-null    object  2   dependentes           614 non-null    int64    3   educacao              614 non-null    object  4   empregado             614 non-null    object  5   renda                 614 non-null    int64    6   renda_conjuge         614 non-null    float64  7   emprestimo            614 non-null    float64  8   prestacao_mensal      614 non-null    float64  9   historico_credito     614 non-null    object  10  imovel                614 non-null    object  11  aprovacao_emprestimo  614 non-null    bool    dtypes: bool(1), float64(3), int64(2), object(6) memory usage: 53.5+ KB   feature enginnering   # sexo                  성별                bool # estado_civil          결혼유무            bool # dependentes           자녀수              int # educacao              (대학?) 졸업유무    bool # empregado             현재 직장유무       bool # renda                 수입                int # renda_conjuge         배우자 수입         int # emprestimo            대출금              int # prestacao_mensal      한달 분할금         int # historico_credito     과거 신용           bool # imovel                거주지역            object 3가지 # aprovacao_emprestimo  대출허가            타겟.Bool  # total_renda = 본인수입 + 배우자 수입 df['total_renda']=df['renda']+df['renda_conjuge']  # ratio_emprestimo = 대출금 / total_renda *100 df['ratio_emprestimo']=df['emprestimo']/df['total_renda']*100  # ratio2_emprestimo = 대출금 / renda *100 df['ratio2_emprestimo']=df['emprestimo']/df['renda']*100  # ratio_prestacao_mensal = 한달분납금 / total_renda *100 df['ratio_prestacao_mensal']=df['prestacao_mensal']/df['total_renda'] *100  # ratio2_prestacao_mensal = 한달분납금 / renda *100 df['ratio2_prestacao_mensal']=df['prestacao_mensal']/df['renda'] *100  # family = 전체 가족수 (본인+배우자+자녀) df['family']=df['estado_civil'].apply(lambda x: 1 if x=='Yes' else 0) + df['dependentes']+1  # per_man_renda = total_renda / 가족수 df['per_man_renda']=df['total_renda']/df['family']  # per_man_emprestimo = 대출금 / 가족수 df['per_man_emprestimo'] = df['emprestimo']/df['family']  # per_man_prestacao_mensal = 한달분납금/가족수 df['per_man_prestacao_mensal']=df['prestacao_mensal']/df['family']   Data Leakage 는 없는 것으로 보임. (일단 profileReport 상 high correlation은 없음)  모델의 한계     데이터가 적어서 (614개) 결측치 (크게는 한feature에 50개) 에 대한 처리가 영향력이 클것으로 보임.    4. 머신러닝 방식 적용 및 교차검증      모델 성능 개선을 위해 어떤 방법을 적용했나요? 그 방법을 선택한 이유는 무엇인가요?       최종 모델에 관해 설명하세요.    from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=0.2, random_state=2)   Baseline model      test 데이터셋의 다수비율은 0.683       DummyClassifier model 은 정확도 0.601, 정밀도 0.701, 재현율 0.726, roc_auc : 0.530    y_test.value_counts(normalize=True).max()   0.6829268292682927   # Decision Tree model from sklearn.dummy import DummyClassifier from sklearn.pipeline import make_pipeline from category_encoders import OrdinalEncoder from sklearn.impute import SimpleImputer from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score baseline_pipe = make_pipeline(     OrdinalEncoder(),     SimpleImputer(),     DummyClassifier(random_state=12) )  baseline_pipe.fit(X_train, y_train)  y_pred=baseline_pipe.predict(X_test) # 정확도 print('정확도 : ', accuracy_score(y_test, y_pred)) # 정밀도 print('정밀도 : ', precision_score(y_test, y_pred)) # 재현율 print('재현율 : ', recall_score(y_test, y_pred)) #ROC print('roc_auc : ', roc_auc_score(y_test, y_pred))   정확도 :  0.6016260162601627 정밀도 :  0.7011494252873564 재현율 :  0.7261904761904762 roc :  0.5297619047619048   The default value of strategy will change from stratified to prior in 0.24.   #오차행렬 from sklearn.metrics import plot_confusion_matrix import matplotlib.pyplot as plt  fig, ax = plt.subplots() matrix = plot_confusion_matrix(baseline_pipe, X_test, y_test,                             cmap = plt.cm.Blues,                             ax = ax); plt.title(f'Baseline_model_Confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      main model      OrdinalEncoder, SimpleImputer로 processing_pipe를 구성       model= Xgboost Classifie       {base_score, min_child_weigh,t max_depth, gamma} 4개 파라미터에 대해 GridSearchCV (cv=3) 파라미터 튜닝을 하였음.       {각각 0.6, 3 , 5 , 8}       정확도 : 0.764, 정밀도 : 0.784, 재현율 : 0.905, ROC 0.683    from xgboost import XGBClassifier processor = make_pipeline(     OrdinalEncoder( ),     SimpleImputer(strategy='median') )  X_train_processed=processor.fit_transform(X_train) X_test_processed=processor.transform(X_test)  model=XGBClassifier(n_estimators=99, seed=2, n_jobs=2, random_state=22 )   from scipy.stats import randint, uniform from sklearn.model_selection import GridSearchCV import multiprocessing dists = {   'xgbclassifier__base_score' :[0.4,0.5, 0.6], #   'xgbclassifier__min_child_weight' : [8,9],    #   'xgbclassifier__max_depth' : [1,5,10],   'xgbclassifier__gamma' : [2,3,4,5], }  clf = GridSearchCV(     model,     param_grid=dists,     cv=3,     scoring= 'roc_auc',     verbose=1,     n_jobs=6 )  clf.fit(X_train_processed, y_train);   Fitting 3 folds for each of 72 candidates, totalling 216 fits   [Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers. [Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   26.7s [Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.7min [Parallel(n_jobs=6)]: Done 216 out of 216 | elapsed:  2.0min finished /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True)   model=clf.best_estimator_  y_pred=model.predict(X_test_processed) # 정확도 print(accuracy_score(y_test, y_pred)) # 정밀도 print(precision_score(y_test, y_pred)) # 재현율 print(recall_score(y_test, y_pred)) #ROC_auc print('roc_auc : ', roc_auc_score(y_test, y_pred)) clf.best_params_   0.7642276422764228 0.7835051546391752 0.9047619047619048 roc_auc :  0.6831501831501832      {'xgbclassifier__base_score': 0.4,  'xgbclassifier__gamma': 2,  'xgbclassifier__max_depth': 1,  'xgbclassifier__min_child_weight': 8}   fig, ax = plt.subplots() matrix = plot_confusion_matrix(model, X_test_processed, y_test, cmap = plt.cm.Blues,ax = ax); plt.title(f'main_model_confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      5) 머신러닝 모델 해석      모델이 관측치를 예측하기 위해서 어떤 특성을 활용했나요?       어떤 특성이 있다면 모델의 예측에 도움이 될까요? 해당 특성은 어떻게 구할 수 있을까요?    feature importances   import shap  explainer = shap.TreeExplainer(model) shap_values = explainer.shap_values(X_test_processed[:100]) shap.summary_plot(shap_values, X_test_processed[:100], feature_names=X_test.columns)      예측결과는 True 예측이 성공한 경우   def explain(row_number):     positive_class = True     positive_class_index = 1      # row 값을 변환합니다     row = X_test.iloc[[row_number]]     row_processed = processor.transform(row)      # 예측하고 예측확률을 얻습니다     pred = model.predict(row_processed)[0]     pred_proba = model.predict_proba(row_processed)[0, positive_class_index]     pred_proba *= 100     if pred != positive_class:         pred_proba = 100 - pred_proba      # 예측결과와 확률값을 얻습니다     print(f'이 대출에 대한 예측결과는 {pred} 으로, 확률은 {pred_proba:.0f}% 입니다.')      # SHAP를 추가합니다     shap_values = explainer.shap_values(row_processed)      # Fully Paid에 대한 top 3 pros, cons를 얻습니다     feature_names = row.columns     feature_values = row.values[0]     shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))     pros = shaps.sort_values(ascending=False)[:3].index     cons = shaps.sort_values(ascending=True)[:3].index      # 예측에 가장 영향을 준 top3     print('\\n')     print('Positive 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = pros if pred == positive_class else cons     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # 예측에 가장 반대적인 영향을 준 요인 top1     print('\\n')     print('Negative 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = cons if pred == positive_class else pros     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # SHAP     shap.initjs()     return shap.force_plot(         base_value=explainer.expected_value,         shap_values=shap_values,         features=row,         link='logit'     )   df2=pd.DataFrame() df2['y_test']=y_test['aprovacao_emprestimo'] df2['y_pred']=y_pred df2['Right / wrong'] = df2['y_test']==df2['y_pred'] df2=df2.reset_index()   df2[df2['Right / wrong']==True][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       2       265       True       True       True                 3       84       True       True       True                 5       436       True       True       True                 6       542       True       True       True                 7       526       True       True       True            explain(6)   이 대출에 대한 예측결과는 True 으로, 확률은 85% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. imovel : Semiurban 3. ratio_prestacao_mensal : 9.857612267250822   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. renda : 3652 2. total_renda : 3652.0 3. ratio2_emprestimo : 2.6013143483023002          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 성공한 경우   df2[df2['Right / wrong']==True][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       0       414       False       False       True                 1       569       False       False       True                 15       108       False       False       True                 23       225       False       False       True                 24       369       False       False       True            explain(23)   이 대출에 대한 예측결과는 False 으로, 확률은 53% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_emprestimo : 5.230769230769231 2. per_man_emprestimo : 85.0 3. imovel : Rural   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. total_renda : 3250.0 3. ratio2_prestacao_mensal : 11.076923076923077          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 True 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       11       510       False       True       False                 13       1       False       True       False                 16       469       False       True       False                 21       199       False       True       False                 48       18       False       True       False            explain(48)   이 대출에 대한 예측결과는 True 으로, 확률은 91% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. ratio2_prestacao_mensal : 7.366482504604052 3. per_man_emprestimo : 66.5   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. imovel : Rural 2. renda : 4887 3. educacao : Not Graduate          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       4       109       True       False       False                 30       155       True       False       False                 33       130       True       False       False                 38       604       True       False       False                 42       407       True       False       False            explain(30)   이 대출에 대한 예측결과는 False 으로, 확률은 94% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 0.0 2. per_man_renda : 7999.8 3. per_man_prestacao_mensal : 36.0   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_prestacao_mensal : 0.450011250281257 2. ratio2_prestacao_mensal : 0.450011250281257 3. ratio_emprestimo : 1.5000375009375235          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["ML","XGboost","GridSearch"],
        "url": "http://localhost:4000/pj/section_2/",
        "teaser": null
      },{
        "title": "진짜 시작 그리고 m1맥북 세팅",
        "excerpt":"반성  지난 6월 코드스테이츠를 시작하면서 블로그를 개설했는데 꾸준히 쓰겠다고 했는데,  꾸준히는 개뿔 이게 테스트용 첫번째 글 이후 이게 5개월만에 두번째 글을 올리게 되었다.   왜 필요성을 느꼈는가  사실 대학교를 다닐때도 기록보단 필기를 하였고, 시험공부를 위한 필기였기에 그 필요성을 느끼지 못하다가  이번에 section4 딥러닝 파트를 마무리하면서 그 필요성을 느끼게 되었다.  그 전환점으로 지난 2년간 쓴 아이폰에 용기를 얻어 맥북을 구매하면서, 개발세팅을 새로하게되었는데  m1 칩셋을 사용하다보니 세팅하는게 하나하나가 쉽지 않았다.  그와 동시에 이 과정들을 기록해두지 않으면 다시 찾아봐야한다는 공포가 엄습했고  이를 계기로 맥북을 사용하여 지금까지 해온 코드스테이츠  전체적인 복습 과  완성하지 못한 section 4 프로젝트를 마무리하는 과정을 불로깅   하는게 1차 목표다.   터미널 창 띄우기     cmd + space를 눌러 spotlight에서 ‘터미널’을 검색하면 된다.    맥북 m1 세팅  homebrew 설치     macOS용 패키지 관리 어플리케이션! https://brew.sh 위 공식 홈페이지를 참고한다면 어렵지 않게 설치가능하다.  단, m1의 경우 ㅜㅜ finder에서 터미널 어플을 찾아 우클릭하여 ‘정보가져오기’에서 ‘rossetta로 사용하여 열기’를 꼭! 체크해야한다.  로제타는 기존의 intel 프로세서에서 돌아가는 친구들을 m1 칩셋, 즉 apple silicon에서 돌아가게 변환해주는 에뮬이라고 보면 된다.    miniforge 설치     알아보니 아나콘다를 설치안하고 miniforge만 설치해도 되는거였다.. m1에서 conda 를 좀더 에러없게 실행하는 인스톨러이며, anaconda와 동급의 카테고리다. (conda의 인스톨러 종류 : 아나콘다, miniforge, miniconda) 이렇게  https://developer.apple.com/metal/tensorflow-plugin/  위에서 apple silicon 이라되어있는곳을 따라 진행하면 된다 ( 설치파일 받고 아래 코드 3줄)     (brew install miniforge 로 설치해주면 된다고도 하는데 tensorfolw-deps (의존성)설치에서 에러가나더라)    아나콘다 가상환경 생성, 삭제, 패키지 설치     생성        conda create --name 이름 python=3.8      삭제        conda env remove --namve 이름      git 설치     git에 기능이 많겠지만 본인은, github 레포와 연동하여 프로젝트들을 관리하기 위한 용도이다.        brew install git      만으로 설치가 가능하다.        git --version      으로 확인까지!    가상환경 세팅  m1 tensorflow-gpu사용하기     m1은 gpu까지 하나로 싸잡아서 만든 칩셋이라 설정방법도 다르다.. 가상환경에서        conda install -c apple tensorflow-deps     pip install tensorflow-macos     pip install tensorflow-metal      로 설치해준다. 파이썬 내에서        import tensorflow as tf     print(len(tf.config.experimental.list_physical_devices('GPU')))      위 결과가 1이 나오면 사용할 수 있다. 단, 확인을 했으면 아래와같이 tensorflow 2.0을 사용하자.(혹시모르니)        import tensorflow.compat.v2 as tf      jupyter notebook과 pandas 설치        conda install -c conda-forge -y pandas jupyter      로 설치해준다. pandas 필요없다면 pandas나 jupyter만 지우면 된다.    그 외 패키지 설치        conda install 패키지이름      단,conda 명령어로 설치가 되는지 안되는지는 잘 검색해보고 쓰는게 좋다. (pip으로만 설치가 되는 패키지도 있음)        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": ["setting","M1"],
        "url": "http://localhost:4000/something_else/setting/",
        "teaser": null
      },{
        "title": "openCV 이미지 읽기",
        "excerpt":"OpenCV vs pillow(PIL) vs scikit-image     https://github.com/ethereon/lycon  위 링크에서는 세 라이브러리의 속도비교를 해 놓았는데,  속도측이나 기능측이나 빠르 openCV를 사용한 이미지 읽기를 정리해보려한다.  만약 다른 라이블리르 이용할 일이 생길때마다 업데이트를 해야겠다.    openCV 설치  conda install -c anaconda opencv   local 경로로 이미지열기  import cv2 cv2.imread('파일경로', flags) # ndarray형식으로 리턴.     flags 는 컬러 (3차원)가 default, 숫자 0 을 넣으면 흑백(1차원)    링크이미지(.jpeg) 열기     크롤링의 개념을 가져와서 사용한다. ```` import cv2 from google.colab.patches import cv2_imshow import numpy as np import urllib.request    def url_to_image(url):   ‘’’   jpg, png 이미지링크에서 numpy ndarray로 return   ‘’’   resp = urllib.request.urlopen(url)   image = np.asarray(bytearray(resp.read()), dtype=’uint8’)   image = cv2.imdecode(image, cv2.IMREAD_COLOR)   return image   &gt; requests.urlopen : url 에서 request(응답) 객체르 리턴   &gt; -&gt; .read()로써 호출할 수 있음.   &gt; numpy.asarray : 배열로써 이미지르 읽음 &gt; image를 cv2로 다시 읽어 리턴한다.(3차원)    ## svg 파일 열기 &gt; cairosvg 라이브러리 내 함수, svg2png, svg2pdf, svg2svg, svg2ps 등 지원     &gt; m1 mac에서는 어떻게 설치하는 업데이트 할 예정 아직공부중 ## 이미지 확인하기    img = 이미지의 ndarray형식 cv2.imshow(‘name’, img)   # name은 이미지르 띄운 window 이름 ````     위는 주피터노트북에서 쓰면 되느 방식이고 .py르 터미널에서 실행할때는  cv2.waitKey(0) 을 이용해서 키보드입력이 있을때까지 띄워놓아야 한다!  cv2.destroyAllWindows() 또한 띄워놓으 윈도우르 전부 파괴        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": ["Image","OpenCV"],
        "url": "http://localhost:4000/ds/image/",
        "teaser": null
      },{
        "title": "정렬",
        "excerpt":"선택정렬          제일 작은걸 차례대로 찾아서, 맨앞부터 계속 교체해주는 정렬.  n개에 대해서 교환을 하며, 그 과정에서 최솟값을 찾기위해 n번 비교  (비교대상이 1씩 줄긴하지만 n이 최고차항이긴함.)를 하므로 O(n^2)        def selection_sort(li):     n = len(li)     for i in range(n):      # i번쨰가 주체         for j in range(i,n):# i번째 뒤로 쭉             if li[i] &gt; li[j]:                 li[i], li[j] = li[j], li[i]     # 교환             else :                 pass     return li          삽입정렬      순서대로 자리를 찾아서 끼워 넣어주는 정렬  n개에 대해서 하며, 최악의 경우 탐색을 계속 n개에 대해 길게하게 되기때문에 O(n^2)  아래 사진은 31의 자리를 찾아 넣어주고있다.        def insertion_sort(li):     n=len(li)     if n ==0:         return None     for i in range(1,n): # 두번째부터 끝까지 수행하면 댐.         temp = li[i]         for j in range(i-1, -1 , -1): # i-1부터 0까지 비교를 해야댐.             if li[j] &gt; temp:                 li[j+1] = li[j]       # j에서 temp보다 크다면 옆으로복사                 if j==0:              # 그와중에 0에 도착햇다면 그냥 0(맨앞)에 temp대입                     li[0] = temp             else:                 li[j+1] = temp        # j보다 temp가 크다면 그 오른쪽에 temp넣고 break                 break     return li          버블정렬      계속해서 옆과 비교,교환을 하는 방식.  (오름차순에서) 만약 맨앞에 제일큰숫자가 있다면, 끊임없이 교환을 거듭하여 끝까지 갈 것이다.(n번)  그 과정을 (1번부터 n까지), (1번부터 n-1까지) … (1과 2) 반복하면 (n번) O(n^2)이다.         def bubble_sort(li):     n = len(li)     for i in range(n):      # 0부터 n-1까지         for j in range(0,n-i):# j는 항상 0부터이되, n-i까지만반복             if j==(n-1):      # indexError 방지.                 pass             elif li[j] &gt; li[j+1]:                 li[j], li[j+1] = li[j+1], li[j]     # 교환             else :                 pass     return li     퀵정렬          특정값을 잡아 그보다 큰값과 작은값으로 계속해서 반으로 쪼갠다 1개짜리 리스트가 될때까지.  이상적으로 쪼개어진다면 아래그림에서 한층에 대해 모든수를 비교하므로 n , 그 층은 2^x의 해 이므로 log n 이므로 O(nlogn)     그러나 아래처럼 계속 최악의 경우로 된다면 층은 n층이 되기때문에 최악의 경우 O(n^2)        def quick_sort(li):     if len(li)&lt;=1:         return li     else:         L1, L2 = [], []         for x in li[1:]:             if x&lt;=li[0]:                 L1.append(x)             else :                 L2.append(x)         return quick_sort( L1 ) + quick_sort([ li[0] ]) + quick_sort( L2 )  병합정렬  ","categories": ["Something_else"],
        "tags": ["선택정렬","삽입정렬","버블정렬","퀵정렬","병합정렬"],
        "url": "http://localhost:4000/something_else/5th/",
        "teaser": null
      },{
        "title": "선형회귀",
        "excerpt":"선형회귀     엑셀에서 점들의 추세선을 그어본적이 있다. 거기서 R값을 표시할 수도 있었는데, 이게 단순 선형회귀의 예시가 아닌가 싶다.  이 또한, 머신러닝이다. 특정 평가지표가 최소가 되도록 모델에 학습데이터를 fit을 시켜주는 데, 평가지표의 종류는 아래와 같다.  y는 실제값(관측값, 측정값)     y^는 예측값 즉 ax+b, y평균은 y전체의 평균.      # 라이브러리 import from sklearn.linear_model import LinearRegression  # 데이터 정의 df = '타겟을 포함한 데이터 프레임' df_test = '타겟 미포함 \"테스트\" 데이터 프레임'  # 모델 클래스 정의 model = LinearRegression()  # feature, target 정의 feature = ['feature_name'] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 모델 학습 model.fit(X_train, y_train)  # 예측값. X_test = [[x] for x in df_test['feature_name']] y_pred = model.predict(X_test)  # 계수확인 print('절편', model.intercept_) print('계수(여러개일 수 있기때문에 array)', model.coef_)  # 시각화. import matplotlib.pyplot as plt ## train 데이터에 대한 그래프를 검정색 점으로. plt.scatter(X_train, y_train, color='black', linewidth=1)  ## test 데이터에 대한 예측을 파란색 점으로. plt.scatter(X_test, y_pred, color='blue', linewidth=1);   다중선형회귀     x에 대해서 y의 추세선을 그으면 단순선형회귀, x와 y에 대해서 z의 추세선을 그으면 다중선형회귀가 될 것이다.(혹은 x1, x2 에 대하 y일 수도 있겠다.)  그렇다면 4개이상 n개의 특성(feature)에 대해서 target의 추세선을 그릴 수 있을까?  위 평가지표(MSE, MAE, R square)에서 예측값 y^ 는 ax+b일수도 있지만 ax+bw+c의 형태로도 될 수있고 일반화하면 아래와 같다.      # 라이브러리 import from sklearn.linear_model import LinearRegression  # 데이터 정의 df = '타겟을 포함한 데이터 프레임' df_test = '타겟 미포함 \"테스트\" 데이터 프레임'  # 모델 클래스 정의 model = LinearRegression()  # feature, target 정의 feature = ['feature_name_1', 'feature_name_2 ] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 모델 학습 model.fit(X_train, y_train)  # 예측값. X_test = df_test[feature] y_pred = model.predict(X_test)  # 계수확인 print('절편', model.intercept_) print('계수(여러개일 수 있기때문에 array)', model.coef_)   평가지표  ","categories": ["Something_else"],
        "tags": ["단순선형회귀","다중선형회귀","릿지회귀","퀵정렬","병합정렬"],
        "url": "http://localhost:4000/something_else/6th/",
        "teaser": null
      },{
        "title": "데이터 다루기",
        "excerpt":"pandas document   pandas 의 모든것이 들어있는 공식문서  https://pandas.pydata.org/docs/reference/index.html   DataFrame ?   데이터프레임을 먼저 알아야 하는데, 열(특성)과 행(단일데이터) 들로 이루어진 2차원 표라고 생각하면 된다.   데이터 불러오기     csv 불러오기    정확히는 csv 파일을 dataFrame의 객체타입으로 변환하는 것  import pandas as pd path = #csv의 경로 df = pd.read_csv(path) #tsv 파일이라면 comma(',')로 구분되는 것이 아닌 tab('\\t')으로 구분되기때문에 sep을 설정해줘야함. df = pd.read_csv(path, sep='\\t')  #csv 파일의 첫번째 줄은 feature의 이름인 경우가 많다. 그땐 index_col=0 으로 하면 0번째 행을 설정하는것 df = pd.read_csv(path, index_col=0)  # 내가 딱 필요한 feature를 정해서 불러온다. 아주 애용한다. df = pd.read_csv(path, usecols=['A', 'B'])    # skiprows를 이용하면 1,2번째 행은 제외할 수 있다. df = pd.read_csv(path, skiprows = [1, 2])  # nrows를 5로 하면 위에서 5개 데이터만 불러온다. df = pd.read_csv(path, nrows = 5)  #na_values를 이용해서 결측값을 제외하고 부를 수 있지만 dataframe의 내장함수를 사용하는 걸 개인적으로 선호한다. df = pd.read_csv(path, na_values = [0, '?', 'N/A', 'NA', 'nan', 'NaN', 'null'])   데이터 프레임 객체의 기본 내장함수  import pandas as pd df = pd.dataFrame(data)  # 데이터프레임 모양확인 df.shape  # 데이터 타입 확인 df.dtypes  # 인덱스 객체 반환 (인덱스는 각 행들의 이름) df.index  # features 의 인덱스 객체 반환 df.columns  # 전치형태의 데이터프레임객체 반환 (전치는 행열 반전) df.T  # 결측치 처리 df.fillna(0, inplace=True)  # 이 예시에서 결측치는 0으로 처리 및, inplace (default=false) 가 True이면 df원본을 손실하고 바로 변경된다. (false이면 결측치 처리된 df 수정본을 return할 뿐.)  # 컬럼 별 결측치 확인 df.isnull().sum()  # 요약통계량 확인 # count : 데이터의 수, mean/std : 평균/표준편차,  min/max : 최소/최대, 25%등 : 백분위수 df.describe()  데이터 프레임의 데이터 슬라이싱   # 열접근 df.NAME df['NAME'] # 둘은 같은 결과를 도출한다.  # loc 사용 # feature의 '이름' 으로 추출 df.loc[:,['NAME','GENDER']]  # iloc 사용 # 행과 열의 번호(순서)로 추출 df.iloc[2:4, 0:3]     # 이 예시에서 2이상 4미만 행, 0이상 3미만의 열을 추출  # 조건부 슬라이싱 df[df.GENDER == 'M']  # 이 예시에서 'GENDER' 라는 features 가 'M'인 행들을 추출   feature engineering  데이터들을 변형시키거나 특성들간 함수처리를 통해 내가 원하는 특성을 만들어내보자   # apply (중요!) df.apply(함수)              # 여기서 함수는 파이썬 내장함수일 수도, 사용자 정의함수일 수도 있다. lambda도 사용 가능 df.apply(lambda x: x+1)                     #lambda사용 예시 모든 데이터에 1을 더하겠다. df['AGE'] = df['AGE'].apply(lambda x:x+1)   # 모든 AGE에 1을더한걸 원본df에도 적용하겠다.  # 특성간 계산 df['GENDER_AGE'] = df['GENDER'] + df['AGE'].apply(str)  # GENDER가 'M'이고 나이가 23 이면 GENDER_AGE라는 feature는 'M23'을 가짐.      🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": ["Pandas"],
        "url": "http://localhost:4000/ds/Data/",
        "teaser": null
      },{
        "title": "가설 검정",
        "excerpt":"가설 검정?  ‘서울시는 사람이 많이 산다.’ 는 사실확인이 가능할까? 안된다. ‘많이’는 너무 주관적이기 때문이다.  그렇지만 ‘사람의 머리카락은 평균 11만 가닥이다.’ 는 사실확인을 할 수 있을까?  데이터 직군에서는 이를 통계학을 기반으로하여 p value 값을 통해 ‘통계적으로 유의하다.’ or ‘통계적으로 유의하지않다’ 를 결정한다. (고 약속했다.)   귀무 가설  귀무 가설은 하나의 가설 검정이 하나만 갖는다. 이 가설이 기각되는지, 기각되지 않는지에 따라 통계적으로 어떤 의미를 갖는지 명확해지므로  처음 검정을 할 때, 귀무가설이 무엇인지 정확히 짚고 실행해야 할 것이다.   T-Test     one_sample_t_test       two_sample_t_test        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": ["가설 검정","Test_2"],
        "url": "http://localhost:4000/something_else/hypothesis/",
        "teaser": null
      }]
