var store = [{
        "title": "test게시물",
        "excerpt":"test_제목  test 게시물입니다.   test_제목 2  test2 목차 테스트 입니다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Example_sub"],
        "tags": ["Test","Test_2"],
        "url": "http://localhost:4000/example_sub/test/",
        "teaser": null
      },{
        "title": "파이썬, 자료구조",
        "excerpt":"서론     옛날에 RTOS관련 대학원에 진학해보려 했다가 받은 질문중에 하나가 스택과 큐의 차이점과 예시를 설명하라는 내용이었다.  스택은 쌓아놓은 책을 위에서 부터 다시꺼낸다면 큐는 만화책 반납통같이 위에서 넣으면 밑에서 꺼내는 개념이라고 말씀드렸다. 물론 틀린건 아니지만 지금 생각해보면 그냥 얘는 개념만 알고 실사용은 해보지 않은 친구라고 생각하셨을 것 같다. 오늘 각각을 구현해보면서 이해해보는 시간을 가졌다.    큐(queue) 를 연결리스트로 구현     연결리스트         큐        class Node():     def __init__(self, data):         self._data = data         self._next = None     node 들을 선언했다.    class Queue():     def __init__(self):         self._front = None         self._rear = None      ‘맨앞’ (꺼내는곳) 와 ‘맨뒤’ (넣는곳) 자리를 마련했다. 큐를 들여다볼수 있는 창문 두개라고 비유하여 이해했다.        def enqueue(self,item):         new_node = Node(item)         if self._front == None: # 빈 큐 라면 '맨앞' '맨뒤'에 모두 같은 노드를 넣어준당.             self._front = new_node             self._rear = self._front         else: # 뭔가 있는 큐라면 뒤의 노드에 대해서만 새로운 노드를 연결해주면 된다.             self._rear._next = new_node           #기존 '맨뒤노드'의 next에 새로운 노드를 넣어준당. (연결!)             self._rear = self._rear._next         #큐의 '맨뒤' 에 새로운 노드를 위치시킨당. (맨뒤 업데이트!)      값 item을 큐에 추가하는 함수를 정의했다. (내부에서는 item을 value로 갖는 node를 연결해준것)        def dequeue(self):         if self._front == None: # 빈큐라면 None을 리턴             return None         else : #뭔가 있는 큐라면 맨앞에서 뽑은 값 리턴 + '맨앞' 업데이트             temp = self._front._data       # '맨앞'노드의 데이터를 잠깐 빼두자.             self._front = self._front._next  #'맨앞' 노드의 next 에 있는 노드를 큐의 '맨앞'으로 둔다.         if self._front ==None:             # 이렇게 되는 경우는 1개짜리의 큐에서 dequeue를 진행한 후가 될거다.             self._rear = None           #그러면 '맨뒤'도 비워주자 (이거 안하면 '맨뒤'에 꺼낸 노드가 아직 남아있음)         return temp      빈큐라면 None을 리턴, 빈큐가 아니라면 제일 먼저 넣은 하나를 큐에서 제거하는 함수를 정의했다.        def return_queue(self):         result = []         temp = self._front      # 초기값은 '맨앞' node!         while temp!= None:      # temp에 None이 들어가버리면 다 끝난거다!             result.append(temp._data)             temp = temp._next   # temp에는 그 다음 node 넣장. (None이 들어갈수도 있음 그러면 loop끝)         return result      현재 큐를 list로 리턴하는 함수를 정의했다.    스택(stack)을 list로 구현     스택의 경우 넣는곳과 빼는곳이 같은 자료구조이다.  그런데 파이썬에서는 list의경우 list.append(x) 와 list.pop() 으로 구현이 가능하다.  (물론 큐도 가능하다. 그러나 pop(0)를 사용하여야 하는데, 그렇게 되면 리스트 끝에서부터 index 0 의 방향으로 탐색을 하기때문에, 비효율적이다.)        class Stack():     def __init__(self):         self._data = []      def push(self, item):         self._data.append(item)       def pop(self):         return self._data.pop() if self._data else None       def return_stack(self):         return self._data     간단해서 코드 리뷰는 생략했다.    데크 구현    동작은 하는데, pop에서 비효율적으로 탐색하는 부분이 있다.(top에써 꺼낼라해도 bottom부터 탐색을 해야하는 부분) 양방향 연결리스트로 수정을 해서 업데이트를 해야겠다  https://user-images.githubusercontent.com/84547813/143423521-f8877649-559d-4a49-aa82-c6f69335bd04.png    class Node:     def __init__(self, value, next=None):         self.value = value         self.next = next  class Deque:     def __init__(self):         self.top = None         self.bottom = None       def append(self, item):         if self.top == None:        # self.top이 비어있다면             self.top = Node(item)   # self.top 에도 bottom에도 뉴 노드를 넣어준다.             self.bottom = self.top         else :             node = Node(item)             self.top.next = node    # 현재 top의 next에 뉴 노드를 넣어주고             self.top = node         # 이제 top은 뉴 노드로 한다.       def appendleft(self, item):         node = Node(item)         if self.bottom == None :        # bottom이 비었다면 top, bottom에 둘다 넣어줘             self.top = node             self.bottom = node         else :             node.next = self.bottom     # 뉴 노드의 next 를 bottom으로 설정 후, 앞으로 bottom은 뉴노드다!             self.bottom = node      top 방향과 bottom방향에 값을 추가할 수 있는 함수 두개를 정의했다. ````     def pop(self):         # top 추출         if self.top == None:        # top이 None 이면 return None             return None         elif self.top == self.bottom:   # top과 bottom이 같다면(None은 아님)             result = self.top.value     # value를 리턴하고, top bottom을 비우자 (None)             self.top =None             self.bottom = None             return result         else :                          # 이제 정상적으로 길이 1이상의 데크라면             node = self.bottom          # 초기값 bottom부터 next가 top인 곳까지 node를 찾아서             result = self.top.value     # (리턴값은 어쨋든 top value)             while node !=None:          # top을 업데이트해주자.                 if node.next == self.top:                     self.top = node                 node = node.next             return result    def popleft(self):     # bottom 추출     if self.bottom == None:         return None     elif self.top == self.bottom:         self.top = None         result = self.bottom.value         self.bottom = None         return result     else :         result = self.bottom.value         self.bottom = self.bottom.next         return result ```` &gt; top에서 추출하는 pop과 bottom에서 추출하는 popleft를 정의했다. ````  def ord_desc(self):     node = self.bottom      # 초기값은 bottom 부터     result = []     while node != None :    # 노드에 None이 들어있다면 종료         result.append(node.value)         node = node.next     return result ```` &gt; 현재 데크를 리스트로 리턴하는 함수를 정의했다.  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/4th/",
        "teaser": null
      },{
        "title": "게임 데이터 분석 기초",
        "excerpt":"   목차  1. 샘플데이터 불러오기  2. 데이터 전처리  3. 필수포함 주제  4. 다음 분기에 어떤 게임을 설계해야 할까   1. 샘플 데이터 불러오기  링크 : https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv   import pandas as pd df=pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv') print(df.columns) df.head() # 의미가 있을만한 feature : 년도, 플랫폼. 장르. 제작한 회사??, 출고량     Index(['Unnamed: 0', 'Name', 'Platform', 'Year', 'Genre', 'Publisher',        'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'],       dtype='object')                           Unnamed: 0       Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales                       0       1       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       0.04       0       0       0                 1       2       The Munchables       Wii       2009.0       Action       Namco Bandai Games       0.17       0       0       0.01                 2       3       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       0.02       0                 3       4       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       0.04       0       0       0                 4       5       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       0.12       0.09       0       0.04            2. 데이터 전처리, EDA   import numpy as np  # 첫번째 열 삭제 try:   df=df.drop(['Unnamed: 0'], axis=1) except:   pass  # 출고량 numerical value 화 (전부 (M이 있을 수 있음) 1000을 곱해주되, K가 들어가면 안곱해줌, ) def to_int(x):   x=str(x)   if 'K' in x:     x=x.replace('K','')     x=int(x)   else:     x=x.replace('M','')     x=int(float(x)*1000)   return x try:   df['NA_Sales']=df['NA_Sales'].apply(to_int)   df['EU_Sales']=df['EU_Sales'].apply(to_int)   df['JP_Sales']=df['JP_Sales'].apply(to_int)   df['Other_Sales']=df['Other_Sales'].apply(to_int) except:   pass # 각 판매량을 각게임의 전체판매량의 비중으로 계산하여 feature 추가( 예를 들어 30, 30, 20 , 20 이면 0.3, 0.3, 0.2, 0.2) # 이유 : 특정 게임의 흥행을 고려하지 않는 feature 필요. df['Game_sales']=df['NA_Sales']+df['EU_Sales']+df['JP_Sales']+df['Other_Sales'] df['NA_Sales(rate)']=df['NA_Sales']/df['Game_sales'] df['EU_Sales(rate)']=df['EU_Sales']/df['Game_sales'] df['JP_Sales(rate)']=df['JP_Sales']/df['Game_sales'] df['Other_Sales(rate)']=df['Other_Sales']/df['Game_sales']  # 년도가 이상하게 표기되어있는 경우. 수정 (0 이면 2000년, 98이면 1998년 등) def re_year(x):   if x&gt;1900 and x&lt;2100:     return x   elif x&lt;=21:     return 2000+x   elif x&gt;21:     return 1900+x   else:     return np.nan df['Year']=df['Year'].apply(re_year) df=df.dropna() df.head()                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 1       The Munchables       Wii       2009.0       Action       Namco Bandai Games       170       0       0       10       180       0.944444       0.00       0.0       0.055556                 2       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       20       0       20       0.000000       0.00       1.0       0.000000                 3       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 4       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       120       90       0       40       250       0.480000       0.36       0.0       0.160000            3-1 지역에 따라서 선호하는 게임 장르가 다를까      가정1 : 지역에 따른 ‘전체 게임 출고량’은 오직 인구수에만 종속된다. (선호도 이외의 영향은 없다)   가정2 : 게임의 전지역 출고량은 지역 장르선호도와 독립적이다. (특정 게임의 흥행은 선호도에 가중치를 주지않는다.)   from scipy.stats import chi2_contingency  # 년도에 상관없이 지역별로 각 장르 게임들의 출고량 rate 합을 그룹화 grouped_df1 grouped_df1=df.groupby('Genre').sum()[['NA_Sales(rate)','EU_Sales(rate)','JP_Sales(rate)', 'Other_Sales(rate)']] grouped_df1_T=grouped_df1.transpose() grouped_df1_T['Total_sales']=grouped_df1_T.sum(axis=1) grouped_df1_T.head()                    Genre       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Total_sales                       NA_Sales(rate)       1525.619292       342.188702       348.445844       807.461452       501.594820       319.931602       675.419334       432.557585       692.748044       409.577643       1210.555555       195.972163       7462.072036                 EU_Sales(rate)       823.705975       208.330453       143.982251       327.132529       211.535672       120.129892       375.877842       232.452620       379.688596       196.063888       538.379128       207.948389       3765.227234                 JP_Sales(rate)       658.357007       669.495053       294.008705       436.876869       106.652981       99.585575       72.600294       725.787645       108.868986       185.525061       388.643571       224.475923       3970.877670                 Other_Sales(rate)       232.317726       47.985792       48.563200       109.529150       53.216527       26.352931       94.102530       74.202151       98.694374       54.833408       161.421746       40.603524       1041.823060            # 2 sample 카이제곱검정을 통해 두변수가 독립적이라는 귀무가설을 검정. chi2 = chi2_contingency(grouped_df1_T[grouped_df1_T.columns.difference(['Total_sales'])]) chi2   (1834.559710433277,  0.0,  33,  array([[1488.73850969,  582.62976243,  383.67180728,  772.3979737 ,           401.13232067,  260.069752  ,  559.65540272,  673.14873972,           588.14360876,  388.72616642, 1056.3610598 ,  307.39693302],         [ 751.19065507,  293.98449093,  193.59388796,  389.73811456,           202.40414873,  131.22651567,  282.39204255,  339.65873755,           296.76667854,  196.1442266 ,  533.02077654,  155.10695933],         [ 792.21943666,  310.04143385,  204.16766346,  411.02496081,           213.45912599,  138.39388924,  297.81582526,  358.2103317 ,           312.97557992,  206.85729735,  562.13348299,  163.57864294],         [ 207.85139859,   81.34431278,   53.5666413 ,  107.83895093,            56.00440462,   36.30984309,   78.13672947,   93.98219103,            82.11413278,   54.27230963,  147.48468066,   42.91746471]]))   p값이 0 이므로 지역에 따른 장르는 독립적이지 않다.  즉, 지역에 따라 선호하는 게임이 유의미하게 다르다.   3-2 연도별 게임의 트렌드가 있을까     트렌드? 게임의 흥망을 고려해야 하는가?  -&gt; 어떤 게임이 대박이 났다면 대박난 게임은 트렌드를 잘탔기때문인가? 아니면 그저 잘 만들어졌기 때문인가?  예시   2000년도에 A라는 게임이 대박을 쳤고 action 장르라고 해보자.  데이터 분석을 할 때, 이 A 게임으로인한 2000년 action 장르 트렌드영향도를  더 크게 ?  -&gt; 대답 : 영향을 더 크게 고려해야함. (즉 rate 가 아닌 출고량으로 분석하겠다)           가정1 : 각 년도에 나오는 각 장르당 게임수는 트렌드와 독립적이다.       ( 매년 무조건 1만가지의 게임만 출시할수 있다는 제한이 있는 경우와 없는경우를 생각해보면 편할 것)       # 년도와 장르별로 게임 판매량을 그룹화함 grouped_df2=df.groupby(['Year', 'Genre']).sum()[['Game_sales']]  # 년도별 출시게임의 갯수를 저장 count=df.groupby(['Year']).count() count=count.reset_index()['Game_sales']  # 년도index, 장르columns 대하여 정리 grouped_df2=grouped_df2.pivot_table(values='Game_sales', index='Year', columns='Genre') grouped_df2=grouped_df2.reset_index() grouped_df2=grouped_df2.fillna(0) grouped_df2['Count']=count grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       0       1980.0       340.0       0.0       770.0       2700.0       0.0       0.0       0.0       0.0       7070.0       0.0       0.0       0.0       8                 1       1981.0       14790.0       0.0       0.0       0.0       6920.0       2250.0       480.0       0.0       10020.0       440.0       780.0       0.0       46                 2       1982.0       6500.0       0.0       0.0       870.0       5030.0       10040.0       1570.0       0.0       3810.0       0.0       1060.0       0.0       36                 3       1983.0       2860.0       400.0       0.0       2140.0       6930.0       780.0       0.0       0.0       490.0       0.0       3200.0       0.0       17                 4       1984.0       1850.0       0.0       0.0       1450.0       690.0       3140.0       5950.0       0.0       31100.0       0.0       6170.0       0.0       14            # 특정 년도는 게임수가 적어 묶어주겠음 (A개 이상으로) A=1200 temp=0 for i in grouped_df2.T.columns:   grouped_df2.T[i]=grouped_df2.T[i]+temp   if int(grouped_df2['Count'][i]) &lt; A :     temp=grouped_df2.T[i]     grouped_df2=grouped_df2.T.drop([i],axis=1).T   else:     temp=0 #마지막에도 80개이하면 버려지게됨. 끝에 붙여줌 if temp[temp.index=='Count'][0] &lt; A:   grouped_df2.iloc[-1]=grouped_df2.iloc[-1]+temp  #년도도 더해져버림... 적절히 평균으로 만들어줌. def qq(x):   i=1   while x/i&gt;2021 :     i+=1   return x/i grouped_df2['Year']=grouped_df2['Year'].apply(qq) grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       123220.0       32930.0       84759.0       39350.0       280500.0       94000.0       96939.0       122690.0       106629.0       33980.0       106290.0       25550.0       1255.0                 21       1999.5       157599.0       28570.0       84349.0       64460.0       106010.0       19460.0       139800.0       126390.0       53669.0       41480.0       164670.0       48020.0       1541.0                 23       2002.5       154630.0       13190.0       48710.0       39420.0       88770.0       7040.0       82330.0       75380.0       74740.0       32029.0       121270.0       13460.0       1600.0                 25       2004.5       161670.0       16980.0       36440.0       85790.0       70150.0       28670.0       102009.0       82460.0       88490.0       60170.0       120639.0       12480.0       1674.0                 27       2006.5       172480.0       35920.0       40160.0       158300.0       85230.0       34570.0       73010.0       95650.0       109230.0       70580.0       234359.0       13610.0       2200.0            # 각 출시년도에 대하여 Count로 나누어줌. (가정1) for i in grouped_df2.columns.difference(['Year','Count']):   grouped_df2[i]=grouped_df2[i]/grouped_df2['Count'] grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       98.183267       26.239044       67.537052       31.354582       223.505976       74.900398       77.242231       97.760956       84.963347       27.075697       84.693227       20.358566       1255.0                 21       1999.5       102.270604       18.539909       54.736535       41.829981       68.792992       12.628164       90.720311       82.018170       34.827385       26.917586       106.859182       31.161583       1541.0                 23       2002.5       96.643750       8.243750       30.443750       24.637500       55.481250       4.400000       51.456250       47.112500       46.712500       20.018125       75.793750       8.412500       1600.0                 25       2004.5       96.577061       10.143369       21.768220       51.248507       41.905615       17.126643       60.937276       49.259259       52.861410       35.943847       72.066308       7.455197       1674.0                 27       2006.5       78.400000       16.327273       18.254545       71.954545       38.740909       15.713636       33.186364       43.477273       49.650000       32.081818       106.526818       6.186364       2200.0            chi2_1=chi2_contingency(grouped_df2[grouped_df2.columns.difference(['Count'])]) chi2_1   (1279.7439040830805,  1.724383653993331e-199,  108,  array([[ 118.47019129,   16.78986204,   32.5745123 ,   54.55699306,            62.97423775,   18.65798994,   52.46450991,   64.28669272,            70.50976375,   27.45568949,   91.15531716,   12.84964063,          2279.56894258],         [ 109.02005572,   15.45056757,   29.97610713,   50.20508837,            57.95090591,   17.16967856,   48.27951852,   59.15866891,            64.88533773,   25.26560281,   83.8840357 ,   11.82464991,          2097.73218419],         [ 100.89920463,   14.29966228,   27.74320144,   46.46533568,            53.63417102,   15.89071753,   44.68320059,   54.75196835,            60.05206038,   23.38358031,   77.63555455,   10.94383748,          1941.47313078],         [ 102.93759729,   14.58854787,   28.303677  ,   47.40404079,            54.71770286,   16.21174604,   45.58590253,   55.85808222,            61.26524813,   23.85598164,   79.20396875,   11.1649278 ,          1980.69528914],         [ 102.74194399,   14.56081945,   28.24988025,   47.31394002,            54.61370102,   16.18093241,   45.4992575 ,   55.75191287,            61.14880139,   23.81063862,   79.05342591,   11.14370664,          1976.93058537],         [ 101.38163783,   14.3680338 ,   27.87585106,   46.68750216,            53.89061412,   15.96669642,   44.89684607,   55.01375601,            60.33918958,   23.49538511,   78.00675638,   10.99616366,          1950.75597005],         [ 101.03873223,   14.31943644,   27.78156589,   46.52958987,            53.70833858,   15.91269187,   44.74499037,   54.82768164,            60.13510286,   23.41591609,   77.74291222,   10.95897107,          1944.15788038],         [ 101.30462451,   14.3571193 ,   27.85467552,   46.65203657,            53.84967678,   15.95456751,   44.86274074,   54.97196549,            60.29335365,   23.47753713,   77.94749951,   10.98781056,          1949.27410224],         [ 102.16087759,   14.47846941,   28.09011049,   47.04635174,            54.30482828,   16.08941967,   45.24193231,   55.43660286,            60.80296879,   23.67597539,   78.60633208,   11.0806824 ,          1965.74987477],         [ 102.35553281,   14.50605638,   28.1436328 ,   47.13599288,            54.40829957,   16.12007612,   45.32813535,   55.54223062,            60.9188215 ,   23.7210871 ,   78.75610695,   11.1017953 ,          1969.49537382]]))   p값 거의 0으로 년도와 장르는 독립적이지않다. 즉, 유의미한 연관이 있으며 연도별 트랜드는 존재한다.   심지어 게임 갯수를 2000개이상씩 묶어도 결과는 같다   3-3 출고량이 높은 게임에 대한 분석 및 시각화 프로세스   출고량 10000이상의 것으로 하겠다.   sorted_df=df.sort_values(by=['Game_sales'], ascending=False) sorted_df=sorted_df.reset_index(drop=True) i=0 while sorted_df['Game_sales'].iloc[i]&gt;10000:   i+=1 sorted_df=sorted_df.iloc[0:i] sorted_df #61개임.                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Wii Sports       Wii       2006.0       Sports       Nintendo       41490       29020       3770       8460       82740       0.501450       0.350737       0.045564       0.102248                 1       Super Mario Bros.       NES       1985.0       Platform       Nintendo       29080       3580       6810       770       40240       0.722664       0.088966       0.169235       0.019135                 2       Mario Kart Wii       Wii       2008.0       Racing       Nintendo       15850       12880       3790       3310       35830       0.442367       0.359475       0.105777       0.092381                 3       Wii Sports Resort       Wii       2009.0       Sports       Nintendo       15750       11010       3280       2960       33000       0.477273       0.333636       0.099394       0.089697                 4       Pokemon Red/Pokemon Blue       GB       1996.0       Role-Playing       Nintendo       11270       8890       10220       1000       31380       0.359146       0.283301       0.325685       0.031867                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...                 56       Super Mario All-Stars       SNES       1993.0       Platform       Nintendo       5990       2150       2120       290       10550       0.567773       0.203791       0.200948       0.027488                 57       Pokemon FireRed/Pokemon LeafGreen       GBA       2004.0       Role-Playing       Nintendo       4340       2650       3150       350       10490       0.413727       0.252622       0.300286       0.033365                 58       Super Mario 64       DS       2004.0       Platform       Nintendo       5080       3110       1250       980       10420       0.487524       0.298464       0.119962       0.094050                 59       Just Dance 3       Wii       2011.0       Misc       Ubisoft       6050       3150       0       1070       10270       0.589094       0.306719       0.000000       0.104187                 60       Call of Duty: Ghosts       X360       2013.0       Shooter       Activision       6720       2630       40       820       10210       0.658178       0.257591       0.003918       0.080313          61 rows × 14 columns    import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(40, 6)) plt.subplot(131) plt.pie(sorted_df['Publisher'].value_counts(), labels=sorted_df['Publisher'].value_counts().index, autopct='%.1f%%'); plt.title('Publisher');  plt.subplot(132) plt.pie(sorted_df['Platform'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Platform');  plt.subplot(133)  #plt.bar(sorted_df['Genre'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Genre'); grouped_df3=sorted_df.groupby(['Genre']).count() grouped_df3 plt.bar(grouped_df3.index, grouped_df3['Name']);      4. 다음 분기에 어떤 게임을 설계해야 할까   4-1. 장르는 트렌드와 시장규모 측면에서 유리한 것으로 선정  4-2. 퍼블리셔는 그 장르의 큰 규모 게임 사례가 있으면 좋겠다. (운영 노하우가 있을거라 가정)  4-3. 플랫폼은 퍼블리셔가 가장 많이 사용하는 플랫폼을 우선으로 (+ 장르에 상관없이 큰 규모의 게임 사례가 있으면 좋겠다.)      4-1. 장르의 트렌드    트렌드와 시장의 크기를 고려하여 Action 장르로 선정. (그외 후보 sports, shooting)    print('y축은 출고량') plt.figure(figsize=(30,10)) ax1 = plt.subplot2grid((3,7), (0,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Action']) plt.title('Action') ax1 = plt.subplot2grid((3,7), (0,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Adventure']) plt.title('Adventure') ax1 = plt.subplot2grid((3,7), (0,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Fighting']) plt.title('Fighting') ax1 = plt.subplot2grid((3,7), (0,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Misc']) plt.title('Misc') ax1 = plt.subplot2grid((3,7), (1,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Platform']) plt.title('Platform') ax1 = plt.subplot2grid((3,7), (1,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Puzzle']) plt.title('Puzzle') ax1 = plt.subplot2grid((3,7), (1,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Racing']) plt.title('Racing') ax1 = plt.subplot2grid((3,7), (1,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Role-Playing']) plt.title('Role') ax1 = plt.subplot2grid((3,7), (2,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Shooter']) plt.title('Shooter') ax1 = plt.subplot2grid((3,7), (2,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Simulation']) plt.title('Simulation') ax1 = plt.subplot2grid((3,7), (2,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Sports']) plt.title('Sports') ax1 = plt.subplot2grid((3,7), (2,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Strategy']) plt.title('Strategy');  ax1 = plt.subplot2grid((3,7), (0,4), colspan=3) plt.bar(grouped_df1.index, grouped_df1.sum(axis=1)) plt.title('Genre market size');   y축은 출고량         4-2. 퍼블리셔 별 Action 게임 데이터    action 게임중에서 총 출고량 2000 이상인 것들 중 각 퍼블리셔의 빈도수을 봄.  Ubisoft, Nintendo, Take-Two Interactive 세개의 퍼블리셔가 후보였는데  출고량이 가장 많은 게임을 운영했던 Take-Two Interactive 를 채택. (게임명 : Grand Theft Auto V)   action_df=df[df['Genre']=='Action'].sort_values(by=['Game_sales'], ascending=False) i=0 while action_df['Game_sales'].iloc[i]&gt;2000:   i+=1 action_df=action_df.iloc[0:i] print(action_df.groupby('Publisher').count()['Name'].sort_values(ascending=False)[0:3])  plt.figure(figsize=(30,5)) plt.subplot(131) plt.scatter(action_df[action_df['Publisher'] == 'Ubisoft']['Year'], action_df[action_df['Publisher'] == 'Ubisoft']['Game_sales']) plt.subplot(132) plt.scatter(action_df[action_df['Publisher'] == 'Nintendo']['Year'], action_df[action_df['Publisher'] == 'Nintendo']['Game_sales']) plt.subplot(133) plt.scatter(action_df[action_df['Publisher'] == 'Take-Two Interactive']['Year'], action_df[action_df['Publisher'] == 'Take-Two Interactive']['Game_sales'])  print(action_df[action_df['Publisher'] == 'Take-Two Interactive'][['Year','Game_sales']][0:5])   Publisher Ubisoft                 22 Nintendo                21 Take-Two Interactive    20 Name: Name, dtype: int64          Year  Game_sales 3483   2013.0       21390 14669  2004.0       20810 10913  2013.0       16380 5340   2002.0       16150 9786   2001.0       13100         4-3. 플랫폼 : 퍼블리셔와 밀접한 플랫폼    ‘Take-Two Interactive’ 가 사용했던 플랫폼 中   print(pd.DataFrame(df[df['Publisher']=='Take-Two Interactive'].groupby(['Platform']).count()).sort_values(by='Name', ascending=False)['Name'][0:3])  print(df[df['Publisher']=='Take-Two Interactive'][df['Platform']=='PS3'].sort_values(by='Year').iloc[-1]['Year'])   Platform X360    70 PS2     60 PS3     53 Name: Name, dtype: int64 2016.0   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.   This is separate from the ipykernel package so we can avoid doing imports until   결론 : Action 게임을,Take-Two Interactive 퍼블리셔와 함께, X360 혹은 PS3 플랫폼에서 설계할것이다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["pandas","시각화","가설검정"],
        "url": "http://localhost:4000/pj/section_1/",
        "teaser": null
      },{
        "title": "대출 승인 시스템의 승인 기준 머신러닝 예측",
        "excerpt":"AI SECTION2 PROJECT   목차  1) 데이터 선정 이유 및 문제 정의   2) 데이터를 이용한 가설 및 평가지표, 베이스라인 선택   3) EDA와 데이터 전처리   4) 머신러닝 방식 적용 및 교차검증   5) 머신러닝 모델 해석   # 설치 (처음1회) '''!pip install pandas_profiling==2.11 !pip install category_encoders !pip install shap'''   '!pip install pandas_profiling==2.11\\n!pip install category_encoders\\n!pip install shap'   1. 데이터 선정 이유 및 문제 정의  대출승인시스템을 선정하였다.  (kaggle url : https://www.kaggle.com/caiocampiao/loan-approval-systemlas)  이유     일단 기본적으로는 명확한 가이드라인을 따라 승인/거절이 나누어질 것이라 생각하였고, 그 외 분명 승인허가에 영향을 끼치는 적당한 외부 교란요인(특별한 경우)도 어느정도 있을 것이라 추측했다.    문제    매번 데이터를 확인해가며 수작업으로 승인/거절을 내리는 것(문제를 위한 가정) 보다 정확도 높은 모델을 사용하여 자동으로 결정되도록 해보자.    import pandas as pd  df=pd.read_csv(\"https://ai-bootcamp-chanwoo.s3.ap-northeast-2.amazonaws.com/HI/clientes.csv\") print(df.shape) df.head()   (614, 13)                           cod_cliente       sexo       estado_civil       dependentes       educacao       empregado       renda       renda_conjuge       emprestimo       prestacao_mensal       historico_credito       imovel       aprovacao_emprestimo                       0       LP001002       Male       No       0       Graduate       No       5849       0       NaN       360.0       1.0       Urban       Y                 1       LP001003       Male       Yes       1       Graduate       No       4583       1508       128.0       360.0       1.0       Rural       N                 2       LP001005       Male       Yes       0       Graduate       Yes       3000       0       66.0       360.0       1.0       Urban       Y                 3       LP001006       Male       Yes       0       Not Graduate       No       2583       2358       120.0       360.0       1.0       Urban       Y                 4       LP001008       Male       No       0       Graduate       No       6000       0       141.0       360.0       1.0       Urban       Y            2. 데이터를 이용한 가설 및 평가지표 선택  target     대출 승인 승인/거절 (aprovacao_emprestimo) (YorN)    Baseline model          Boolean target 이기 때문에 하나의 값이 70% 이상의 불균형이 아닌이상 둘중(Y,N) 큰 비율과              DummyClassifier를 사용한 결과의 평가지표를 비교하여 큰 것을 베이스라인 모델로 사용해보겠다.      target = ['aprovacao_emprestimo']   3. EDA와 데이터 전처리   EDA   # profiling # 결측치 확인 # 데이터 형 확인 # 데이터 분포 확인 from pandas_profiling import ProfileReport  ProfileReport(df, minimal=True, title = 'Section2 Project LAS', dark_mode=True)   Summarize dataset:   0%|          | 0/21 [00:00&lt;?, ?it/s]    Generate report structure:   0%|          | 0/1 [00:00&lt;?, ?it/s]    Render HTML:   0%|          | 0/1 [00:00&lt;?, ?it/s]     데이터 전처리   import numpy as np # 결측치 있는 feature # sexo                성별            # estado_civil        결혼유무        # dependentes         자녀수           # empregado           현재 직장유무 # emprestimo          대출금 # prestacao_mensal    한달분납금 # historico_credito   과거 신용  # 형변환 필요 # dependentes         자녀수        숫자로 변환필요 # renda_conjuge       배우자 수입   숫자로 변환 필요 # historico_credito   과거 신용     숫자인데 boolean 이라 인코딩필요(문자로 변환하자)  # high cardinality categorical 변수는 없음  #cod_cliente 고객 코드. -&gt; 마지막 숫자만 남겨놓자 #df['cod_cliente']=df['cod_cliente'].apply(lambda x : int(x[4:7])) df=df.drop('cod_cliente', axis=1)  # 성별 새롭게 'Dont_know'으로 처리 df['sexo']=df['sexo'].fillna('Dont_know')  #결혼 유무 새롭게 'Dont_know'으로 처리 df['estado_civil']=df['estado_civil'].fillna('Dont_know')  #현재 직장유무  새롭게 'Dont_know'으로 처리 df['empregado']=df['empregado'].fillna('Dont_know')  # 자녀수 숫자로 변환 # 자녀수 3이상은 3으로처리 def to_int(x):   try :     return int(x)   except :     return 3 df['dependentes']=df['dependentes'].apply(to_int)  #대출금 평균 df['emprestimo']=df['emprestimo'].fillna(df['emprestimo'].mean())  #한달분납금 평균 df['prestacao_mensal']=df['prestacao_mensal'].fillna(df['prestacao_mensal'].mean())  #과거 신용 새롭게 'Dont_know' 으로 처리 df['historico_credito']=df['historico_credito'].fillna(2) #과거 신용 문자로 변환 df['historico_credito']=df['historico_credito'].apply(str)  #배우자 수입 숫자로 변환 #변환 안되면 평균값 def to_float(x):   try:     return float(x)   except:     return np.nan df['renda_conjuge']=df['renda_conjuge'].apply(to_float) df['renda_conjuge']=df['renda_conjuge'].fillna(df['renda_conjuge'].mean())  # target을 True과 False 으로 df['aprovacao_emprestimo']=df['aprovacao_emprestimo']=='Y'  # 결측치 확인 print(\"결측치 갯수\", df.isnull().sum().sum())  # 데이터 형확인 df.info()   결측치 갯수 0 &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 614 entries, 0 to 613 Data columns (total 12 columns):  #   Column                Non-Null Count  Dtype   ---  ------                --------------  -----    0   sexo                  614 non-null    object  1   estado_civil          614 non-null    object  2   dependentes           614 non-null    int64    3   educacao              614 non-null    object  4   empregado             614 non-null    object  5   renda                 614 non-null    int64    6   renda_conjuge         614 non-null    float64  7   emprestimo            614 non-null    float64  8   prestacao_mensal      614 non-null    float64  9   historico_credito     614 non-null    object  10  imovel                614 non-null    object  11  aprovacao_emprestimo  614 non-null    bool    dtypes: bool(1), float64(3), int64(2), object(6) memory usage: 53.5+ KB   feature enginnering   # sexo                  성별                bool # estado_civil          결혼유무            bool # dependentes           자녀수              int # educacao              (대학?) 졸업유무    bool # empregado             현재 직장유무       bool # renda                 수입                int # renda_conjuge         배우자 수입         int # emprestimo            대출금              int # prestacao_mensal      한달 분할금         int # historico_credito     과거 신용           bool # imovel                거주지역            object 3가지 # aprovacao_emprestimo  대출허가            타겟.Bool  # total_renda = 본인수입 + 배우자 수입 df['total_renda']=df['renda']+df['renda_conjuge']  # ratio_emprestimo = 대출금 / total_renda *100 df['ratio_emprestimo']=df['emprestimo']/df['total_renda']*100  # ratio2_emprestimo = 대출금 / renda *100 df['ratio2_emprestimo']=df['emprestimo']/df['renda']*100  # ratio_prestacao_mensal = 한달분납금 / total_renda *100 df['ratio_prestacao_mensal']=df['prestacao_mensal']/df['total_renda'] *100  # ratio2_prestacao_mensal = 한달분납금 / renda *100 df['ratio2_prestacao_mensal']=df['prestacao_mensal']/df['renda'] *100  # family = 전체 가족수 (본인+배우자+자녀) df['family']=df['estado_civil'].apply(lambda x: 1 if x=='Yes' else 0) + df['dependentes']+1  # per_man_renda = total_renda / 가족수 df['per_man_renda']=df['total_renda']/df['family']  # per_man_emprestimo = 대출금 / 가족수 df['per_man_emprestimo'] = df['emprestimo']/df['family']  # per_man_prestacao_mensal = 한달분납금/가족수 df['per_man_prestacao_mensal']=df['prestacao_mensal']/df['family']   Data Leakage 는 없는 것으로 보임. (일단 profileReport 상 high correlation은 없음)  모델의 한계     데이터가 적어서 (614개) 결측치 (크게는 한feature에 50개) 에 대한 처리가 영향력이 클것으로 보임.    4. 머신러닝 방식 적용 및 교차검증      모델 성능 개선을 위해 어떤 방법을 적용했나요? 그 방법을 선택한 이유는 무엇인가요?       최종 모델에 관해 설명하세요.    from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=0.2, random_state=2)   Baseline model      test 데이터셋의 다수비율은 0.683       DummyClassifier model 은 정확도 0.601, 정밀도 0.701, 재현율 0.726, roc_auc : 0.530    y_test.value_counts(normalize=True).max()   0.6829268292682927   # Decision Tree model from sklearn.dummy import DummyClassifier from sklearn.pipeline import make_pipeline from category_encoders import OrdinalEncoder from sklearn.impute import SimpleImputer from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score baseline_pipe = make_pipeline(     OrdinalEncoder(),     SimpleImputer(),     DummyClassifier(random_state=12) )  baseline_pipe.fit(X_train, y_train)  y_pred=baseline_pipe.predict(X_test) # 정확도 print('정확도 : ', accuracy_score(y_test, y_pred)) # 정밀도 print('정밀도 : ', precision_score(y_test, y_pred)) # 재현율 print('재현율 : ', recall_score(y_test, y_pred)) #ROC print('roc_auc : ', roc_auc_score(y_test, y_pred))   정확도 :  0.6016260162601627 정밀도 :  0.7011494252873564 재현율 :  0.7261904761904762 roc :  0.5297619047619048   The default value of strategy will change from stratified to prior in 0.24.   #오차행렬 from sklearn.metrics import plot_confusion_matrix import matplotlib.pyplot as plt  fig, ax = plt.subplots() matrix = plot_confusion_matrix(baseline_pipe, X_test, y_test,                             cmap = plt.cm.Blues,                             ax = ax); plt.title(f'Baseline_model_Confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      main model      OrdinalEncoder, SimpleImputer로 processing_pipe를 구성       model= Xgboost Classifie       {base_score, min_child_weigh,t max_depth, gamma} 4개 파라미터에 대해 GridSearchCV (cv=3) 파라미터 튜닝을 하였음.       {각각 0.6, 3 , 5 , 8}       정확도 : 0.764, 정밀도 : 0.784, 재현율 : 0.905, ROC 0.683    from xgboost import XGBClassifier processor = make_pipeline(     OrdinalEncoder( ),     SimpleImputer(strategy='median') )  X_train_processed=processor.fit_transform(X_train) X_test_processed=processor.transform(X_test)  model=XGBClassifier(n_estimators=99, seed=2, n_jobs=2, random_state=22 )   from scipy.stats import randint, uniform from sklearn.model_selection import GridSearchCV import multiprocessing dists = {   'xgbclassifier__base_score' :[0.4,0.5, 0.6], #   'xgbclassifier__min_child_weight' : [8,9],    #   'xgbclassifier__max_depth' : [1,5,10],   'xgbclassifier__gamma' : [2,3,4,5], }  clf = GridSearchCV(     model,     param_grid=dists,     cv=3,     scoring= 'roc_auc',     verbose=1,     n_jobs=6 )  clf.fit(X_train_processed, y_train);   Fitting 3 folds for each of 72 candidates, totalling 216 fits   [Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers. [Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   26.7s [Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.7min [Parallel(n_jobs=6)]: Done 216 out of 216 | elapsed:  2.0min finished /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True)   model=clf.best_estimator_  y_pred=model.predict(X_test_processed) # 정확도 print(accuracy_score(y_test, y_pred)) # 정밀도 print(precision_score(y_test, y_pred)) # 재현율 print(recall_score(y_test, y_pred)) #ROC_auc print('roc_auc : ', roc_auc_score(y_test, y_pred)) clf.best_params_   0.7642276422764228 0.7835051546391752 0.9047619047619048 roc_auc :  0.6831501831501832      {'xgbclassifier__base_score': 0.4,  'xgbclassifier__gamma': 2,  'xgbclassifier__max_depth': 1,  'xgbclassifier__min_child_weight': 8}   fig, ax = plt.subplots() matrix = plot_confusion_matrix(model, X_test_processed, y_test, cmap = plt.cm.Blues,ax = ax); plt.title(f'main_model_confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      5) 머신러닝 모델 해석      모델이 관측치를 예측하기 위해서 어떤 특성을 활용했나요?       어떤 특성이 있다면 모델의 예측에 도움이 될까요? 해당 특성은 어떻게 구할 수 있을까요?    feature importances   import shap  explainer = shap.TreeExplainer(model) shap_values = explainer.shap_values(X_test_processed[:100]) shap.summary_plot(shap_values, X_test_processed[:100], feature_names=X_test.columns)      예측결과는 True 예측이 성공한 경우   def explain(row_number):     positive_class = True     positive_class_index = 1      # row 값을 변환합니다     row = X_test.iloc[[row_number]]     row_processed = processor.transform(row)      # 예측하고 예측확률을 얻습니다     pred = model.predict(row_processed)[0]     pred_proba = model.predict_proba(row_processed)[0, positive_class_index]     pred_proba *= 100     if pred != positive_class:         pred_proba = 100 - pred_proba      # 예측결과와 확률값을 얻습니다     print(f'이 대출에 대한 예측결과는 {pred} 으로, 확률은 {pred_proba:.0f}% 입니다.')      # SHAP를 추가합니다     shap_values = explainer.shap_values(row_processed)      # Fully Paid에 대한 top 3 pros, cons를 얻습니다     feature_names = row.columns     feature_values = row.values[0]     shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))     pros = shaps.sort_values(ascending=False)[:3].index     cons = shaps.sort_values(ascending=True)[:3].index      # 예측에 가장 영향을 준 top3     print('\\n')     print('Positive 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = pros if pred == positive_class else cons     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # 예측에 가장 반대적인 영향을 준 요인 top1     print('\\n')     print('Negative 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = cons if pred == positive_class else pros     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # SHAP     shap.initjs()     return shap.force_plot(         base_value=explainer.expected_value,         shap_values=shap_values,         features=row,         link='logit'     )   df2=pd.DataFrame() df2['y_test']=y_test['aprovacao_emprestimo'] df2['y_pred']=y_pred df2['Right / wrong'] = df2['y_test']==df2['y_pred'] df2=df2.reset_index()   df2[df2['Right / wrong']==True][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       2       265       True       True       True                 3       84       True       True       True                 5       436       True       True       True                 6       542       True       True       True                 7       526       True       True       True            explain(6)   이 대출에 대한 예측결과는 True 으로, 확률은 85% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. imovel : Semiurban 3. ratio_prestacao_mensal : 9.857612267250822   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. renda : 3652 2. total_renda : 3652.0 3. ratio2_emprestimo : 2.6013143483023002          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 성공한 경우   df2[df2['Right / wrong']==True][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       0       414       False       False       True                 1       569       False       False       True                 15       108       False       False       True                 23       225       False       False       True                 24       369       False       False       True            explain(23)   이 대출에 대한 예측결과는 False 으로, 확률은 53% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_emprestimo : 5.230769230769231 2. per_man_emprestimo : 85.0 3. imovel : Rural   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. total_renda : 3250.0 3. ratio2_prestacao_mensal : 11.076923076923077          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 True 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       11       510       False       True       False                 13       1       False       True       False                 16       469       False       True       False                 21       199       False       True       False                 48       18       False       True       False            explain(48)   이 대출에 대한 예측결과는 True 으로, 확률은 91% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. ratio2_prestacao_mensal : 7.366482504604052 3. per_man_emprestimo : 66.5   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. imovel : Rural 2. renda : 4887 3. educacao : Not Graduate          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       4       109       True       False       False                 30       155       True       False       False                 33       130       True       False       False                 38       604       True       False       False                 42       407       True       False       False            explain(30)   이 대출에 대한 예측결과는 False 으로, 확률은 94% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 0.0 2. per_man_renda : 7999.8 3. per_man_prestacao_mensal : 36.0   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_prestacao_mensal : 0.450011250281257 2. ratio2_prestacao_mensal : 0.450011250281257 3. ratio_emprestimo : 1.5000375009375235          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["XGboost","GridSearch"],
        "url": "http://localhost:4000/pj/section_2/",
        "teaser": null
      },{
        "title": "진짜 시작 그리고 m1맥북 세팅",
        "excerpt":"반성  지난 6월 코드스테이츠를 시작하면서 블로그를 개설했는데 꾸준히 쓰겠다고 했는데,  꾸준히는 개뿔 이게 테스트용 첫번째 글 이후 이게 5개월만에 두번째 글을 올리게 되었다.   왜 필요성을 느꼈는가  사실 대학교를 다닐때도 기록보단 필기를 하였고, 시험공부를 위한 필기였기에 그 필요성을 느끼지 못하다가  이번에 section4 딥러닝 파트를 마무리하면서 그 필요성을 느끼게 되었다.  그 전환점으로 지난 2년간 쓴 아이폰에 용기를 얻어 맥북을 구매하면서, 개발세팅을 새로하게되었는데  m1 칩셋을 사용하다보니 세팅하는게 하나하나가 쉽지 않았다.  그와 동시에 이 과정들을 기록해두지 않으면 다시 찾아봐야한다는 공포가 엄습했고  이를 계기로 맥북을 사용하여 지금까지 해온 코드스테이츠  전체적인 복습 과  완성하지 못한 section 4 프로젝트를 마무리하는 과정을 불로깅   하는게 1차 목표다.   터미널 창 띄우기     cmd + space를 눌러 spotlight에서 ‘터미널’을 검색하면 된다.    맥북 m1 세팅  homebrew 설치     macOS용 패키지 관리 어플리케이션! https://brew.sh 위 공식 홈페이지를 참고한다면 어렵지 않게 설치가능하다.  단, m1의 경우 ㅜㅜ finder에서 터미널 어플을 찾아 우클릭하여 ‘정보가져오기’에서 ‘rossetta로 사용하여 열기’를 꼭! 체크해야한다.  로제타는 기존의 intel 프로세서에서 돌아가는 친구들을 m1 칩셋, 즉 apple silicon에서 돌아가게 변환해주는 에뮬이라고 보면 된다.    miniforge 설치     알아보니 아나콘다를 설치안하고 miniforge만 설치해도 되는거였다.. m1에서 conda 를 좀더 에러없게 실행하는 인스톨러이며, anaconda와 동급의 카테고리다. (conda의 인스톨러 종류 : 아나콘다, miniforge, miniconda) 이렇게  https://developer.apple.com/metal/tensorflow-plugin/  위에서 apple silicon 이라되어있는곳을 따라 진행하면 된다 ( 설치파일 받고 아래 코드 3줄)     (brew install miniforge 로 설치해주면 된다고도 하는데 tensorfolw-deps (의존성)설치에서 에러가나더라)    아나콘다 가상환경 생성, 삭제, 패키지 설치     생성        conda create --name 이름 python=3.8      삭제        conda env remove --namve 이름      git 설치     git에 기능이 많겠지만 본인은, github 레포와 연동하여 프로젝트들을 관리하기 위한 용도이다.        brew install git      만으로 설치가 가능하다.        git --version      으로 확인까지!    가상환경 세팅  m1 tensorflow-gpu사용하기     m1은 gpu까지 하나로 싸잡아서 만든 칩셋이라 설정방법도 다르다.. 가상환경에서        conda install -c apple tensorflow-deps     pip install tensorflow-macos     pip install tensorflow-metal      로 설치해준다. 파이썬 내에서        import tensorflow as tf     print(len(tf.config.experimental.list_physical_devices('GPU')))      위 결과가 1이 나오면 사용할 수 있다. 단, 확인을 했으면 아래와같이 tensorflow 2.0을 사용하자.(혹시모르니)        import tensorflow.compat.v2 as tf      jupyter notebook과 pandas 설치        conda install -c conda-forge -y pandas jupyter      로 설치해준다. pandas 필요없다면 pandas나 jupyter만 지우면 된다.    그 외 패키지 설치        conda install 패키지이름      단,conda 명령어로 설치가 되는지 안되는지는 잘 검색해보고 쓰는게 좋다. (pip으로만 설치가 되는 패키지도 있음)        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/setting/",
        "teaser": null
      },{
        "title": "openCV 이미지 읽기",
        "excerpt":"OpenCV vs pillow(PIL) vs scikit-image     https://github.com/ethereon/lycon  위 링크에서는 세 라이브러리의 속도비교를 해 놓았는데,  속도측이나 기능측이나 빠르 openCV를 사용한 이미지 읽기를 정리해보려한다.  만약 다른 라이블리르 이용할 일이 생길때마다 업데이트를 해야겠다.    openCV 설치  conda install -c anaconda opencv   local 경로로 이미지열기  import cv2 cv2.imread('파일경로', flags) # ndarray형식으로 리턴.     flags 는 컬러 (3차원)가 default, 숫자 0 을 넣으면 흑백(1차원)    링크이미지(.jpeg) 열기     크롤링의 개념을 가져와서 사용한다.    import cv2 from google.colab.patches import cv2_imshow import numpy as np import urllib.request  def url_to_image(url):   '''   jpg, png 이미지링크에서 numpy ndarray로 return   '''   resp = urllib.request.urlopen(url)   image = np.asarray(bytearray(resp.read()), dtype='uint8')   image = cv2.imdecode(image, cv2.IMREAD_COLOR)    return image       requests.urlopen : url 에서 request(응답) 객체르 리턴  -&gt; .read()로써 호출할 수 있음.  numpy.asarray : 배열로써 이미지르 읽음 image를 cv2로 다시 읽어 리턴한다.(3차원)    svg 파일 열기     cairosvg 라이브러리 내 함수, svg2png, svg2pdf, svg2svg, svg2ps 등 지원    m1 mac에서는 어떻게 설치하는 업데이트 할 예정 아직공부중    이미지 확인하기    img = 이미지의 ndarray형식 cv2.imshow('name', img)   # name은 이미지르 띄운 window 이름      위는 주피터노트북에서 쓰면 되느 방식이고 .py르 터미널에서 실행할때는  cv2.waitKey(0) 을 이용해서 키보드입력이 있을때까지 띄워놓아야 한다!  cv2.destroyAllWindows() 또한 띄워놓으 윈도우르 전부 파괴        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/image/",
        "teaser": null
      },{
        "title": "정렬",
        "excerpt":"선택정렬          제일 작은걸 차례대로 찾아서, 맨앞부터 계속 교체해주는 정렬.  n개에 대해서 교환을 하며, 그 과정에서 최솟값을 찾기위해 n번 비교  (비교대상이 1씩 줄긴하지만 n이 최고차항이긴함.)를 하므로 O(n^2)        def selection_sort(li):     n = len(li)     for i in range(n):      # i번쨰가 주체         for j in range(i,n):# i번째 뒤로 쭉             if li[i] &gt; li[j]:                 li[i], li[j] = li[j], li[i]     # 교환             else :                 pass     return li          삽입정렬      순서대로 자리를 찾아서 끼워 넣어주는 정렬  n개에 대해서 하며, 최악의 경우 탐색을 계속 n개에 대해 길게하게 되기때문에 O(n^2)  아래 사진은 31의 자리를 찾아 넣어주고있다.        def insertion_sort(li):     n=len(li)     if n ==0:         return None     for i in range(1,n): # 두번째부터 끝까지 수행하면 댐.         temp = li[i]         for j in range(i-1, -1 , -1): # i-1부터 0까지 비교를 해야댐.             if li[j] &gt; temp:                 li[j+1] = li[j]       # j에서 temp보다 크다면 옆으로복사                 if j==0:              # 그와중에 0에 도착햇다면 그냥 0(맨앞)에 temp대입                     li[0] = temp             else:                 li[j+1] = temp        # j보다 temp가 크다면 그 오른쪽에 temp넣고 break                 break     return li          버블정렬      계속해서 옆과 비교,교환을 하는 방식.  (오름차순에서) 만약 맨앞에 제일큰숫자가 있다면, 끊임없이 교환을 거듭하여 끝까지 갈 것이다.(n번)  그 과정을 (1번부터 n까지), (1번부터 n-1까지) … (1과 2) 반복하면 (n번) O(n^2)이다.         def bubble_sort(li):     n = len(li)     for i in range(n):      # 0부터 n-1까지         for j in range(0,n-i):# j는 항상 0부터이되, n-i까지만반복             if j==(n-1):      # indexError 방지.                 pass             elif li[j] &gt; li[j+1]:                 li[j], li[j+1] = li[j+1], li[j]     # 교환             else :                 pass     return li     퀵정렬          특정값을 잡아 그보다 큰값과 작은값으로 계속해서 반으로 쪼갠다 1개짜리 리스트가 될때까지.  이상적으로 쪼개어진다면 아래그림에서 한층에 대해 모든수를 비교하므로 n , 그 층은 2^x의 해 이므로 log n 이므로 O(nlogn)     그러나 아래처럼 계속 최악의 경우로 된다면 층은 n층이 되기때문에 최악의 경우 O(n^2)        def quick_sort(li):     if len(li)&lt;=1:         return li     else:         L1, L2 = [], []         for x in li[1:]:             if x&lt;=li[0]:                 L1.append(x)             else :                 L2.append(x)         return quick_sort( L1 ) + quick_sort([ li[0] ]) + quick_sort( L2 )  병합정렬  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/5th/",
        "teaser": null
      },{
        "title": "단순선형회귀, 회귀의 평가지표 그리고, 모델의 적합",
        "excerpt":"선형회귀     엑셀에서 점들의 추세선을 그어본적이 있다. 거기서 R값을 표시할 수도 있었는데, 이게 단순 선형회귀의 예시가 아닌가 싶다.  이 또한, 머신러닝이다. 특정 평가지표가 최소가 되도록 모델에 학습데이터를 fit을 시켜주는 데, 평가지표의 종류는 아래와 같다.  y는 실제값(관측값, 측정값)     y^는 예측값 즉 ax+b, y평균은 y전체의 평균.      # 라이브러리 import from sklearn.linear_model import LinearRegression  # 데이터 정의 df = '타겟을 포함한 데이터 프레임' df_test = '타겟 미포함 \"테스트\" 데이터 프레임'  # 모델 클래스 정의 model = LinearRegression()  # feature, target 정의 feature = ['feature_name'] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 모델 학습 model.fit(X_train, y_train)  # 예측값. X_test = [[x] for x in df_test['feature_name']] y_pred = model.predict(X_test)  # 계수확인 print('절편', model.intercept_) print('계수(여러개일 수 있기때문에 array)', model.coef_)  # 시각화. import matplotlib.pyplot as plt ## train 데이터에 대한 그래프를 검정색 점으로. plt.scatter(X_train, y_train, color='black', linewidth=1)  ## test 데이터에 대한 예측을 파란색 점으로. plt.scatter(X_test, y_pred, color='blue', linewidth=1);   다중선형회귀     x에 대해서 y의 추세선을 그으면 단순선형회귀, x와 y에 대해서 z의 추세선을 그으면 다중선형회귀가 될 것이다.   그렇다면 4개이상 n개의 특성(feature)에 대해서 target의 추세선을 그릴 수 있을까?  위 평가지표(MSE, MAE, R square)에서 예측값 y^ 는 ax+b일수도 있지만  ax+bw+c의 형태로도 될 수있고 일반화하면 아래와 같다.      # 위 단순 선형회귀에서 X만 여러개 넣어주면 된다! (model도 동일)  # feature, target 정의 feature = ['feature_name_1', 'feature_name_2 ] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 나머지는 동일!!   평가지표     MSE (Mean Square Error),  MAE(Mean Absolute Error),  RMS or RMSE (Root Mean Square Error)  R-score    과적합 &amp; 과소적합  편향은 잘못된 가정을 했을 때 발생하는 오차,  과소적합 문제를 야기.  분산은 트레이닝 셋의 복잡도에 의해 발생하는 오차, 큰 노이즈 까지 모델링에 포함시켜   과적합 문제 야기.     분산과 편향은 트레이드오프(trade-off)관계이다.  예를 들어, 고분산 모델은 트레이닝 셋의 특성을 잘 담는다고도 할 수 있지만, 일반화에 실패했다고도 할 수 있다.  고편향은 지나친 일반화를 하여 과소적합이 되는 것.  즉 적절한 일반화는 편향과 분산의 적절한 분배를 의미하기도 한다.      위 표에서 가로축은 모델의 복잡도, 세로축은 score 성능.  즉, 가로축은 고편향에서 고분산으로 향한다.   복잡도 up! -&gt; 트레이닝셋 score up!  너무 복잡해지면 고분산, 즉 과적합 ( 트레이닝셋에만 과하게 학습됨.)  best model은 트레이닝셋과 독립적인 검증데이터에서 의 score로 찾아야 할 것.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/regression/",
        "teaser": null
      },{
        "title": "가설검정",
        "excerpt":"가설 검정?  ‘서울시는 사람이 많이 산다.’ 는 사실확인이 가능할까? 안된다. ‘많이’는 너무 주관적이기 때문이다.  그렇지만 ‘사람의 머리카락은 평균 11만 가닥이다.’ 는 사실확인을 할 수 있을까?  데이터 직군에서는 이를 통계학을 기반으로하여 p value 값을 통해 ‘통계적으로 유의하다.’ or ‘통계적으로 유의하지않다’ 를 결정한다. (고 약속했다.)   귀무 가설  귀무 가설은 하나의 가설 검정이 하나만 갖는다. 이 가설이 기각되는지, 기각되지 않는지에 따라 통계적으로 어떤 의미를 갖는지 명확해지므로  처음 검정을 할 때, 귀무가설이 무엇인지 정확히 짚고 실행해야 할 것이다.   p-value  pvalue &lt; 0.01 : 귀무가설이 옳을 확률이 1%이하 -&gt; 틀렸다 (깐깐한 기준)  pvalue &lt; 0.05 (5%) : 귀무가설이 옳을 확률이 5%이하 -&gt; 틀렸다 (일반적인 기준)  0.05 ~ pvalue ~ 0.1 사이인 경우: (애매함)  pvalue &gt; 0.1 (10%) : 귀무가설이 옳을 확률이 10%이상인데 -&gt; 귀무가설이 맞다 ~ 틀리지 않았을것이다.  p-value : 0.85 –&gt; 귀무가설은 틀리지 않았다. (귀무가설이 옳다와 톤이 약간 다름)   독립표본 T-Test     one_sample_t_test    from scipy import stats pv1=stats.ttest_1samp(df['AGE'],35).pvalue      귀무가설 : df데이터의 사람들의 나이는 평균이 35라고 할 수 있다.  예) pv1이 0.05 보다 작다면 ‘사람들의 나이 평균은 35라는 가설은 기각한다.’ 즉 ‘신뢰도 95%에서 평균은 35라고 할 수는 없다.’   예) pv1이 0.85 정도가 나왔다면 ‘신뢰도 95%에서 사람들의 나이평균은 35이다’      two_sample_t_test    pv2=stats.ttest_ind(df1['AGE'],df2['AGE']).pvalue pv3=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='greater').pvalue pv4=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='less').pvalue      pv2 귀무가설 : df1 나이평균과 df2나이평균은 통계적으로 같다. (나이에 있어서 두 샘플은 같은 통계적 분포를 가진다)  pv3 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 크다.  pv4 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 작다.   대응표본(쌍체표본) t-Test  데이터 수가 같은 두 표본 (같은 집단에 대한 약물의 전후 효과 비교 등) 의 평균을 비교한다.   import scipy.stats scipy.stats.ttest_rel(dat_M, dat_F) # dat_M과 dat_F는 쌍이되는 두 표본의 어떤 feature값들,   귀무가설 : dat_M 과 dat_F 의 평균은 통계적으로 같다.   chi-square-Test  카이제곱 검정은 두 다른 feature에 대한 검정을 하는데 사용하거나  두 독립 표본이 통계적으로 차이가 있는지를 검정하는데 사용.      적합도 검정 (일원카이제곱)    두 표본이 통계적으로 같은 결과라고 볼 수 있는지 검정한다.  예 : 주사위를 실제로 던져 나온 숫자 데이터셋 vs 같은 횟수만큼 3.5를 적은 데이터셋  혹은 실제 관측 데이터 vs 기대 데이터  혹은 표본이 모집단을 대표할 수 있는지에 대한 검정도 가능 ( 적합도 라는 단어가 이제야 어울린다.. )   귀무가설 : 두 데이터는 통계적으로 같은 데이터이다.  from scipy.stats import chi2_contingency chi_res  = chi2_contingency(pd.crosstab(df['실제관측'], df['기댓값']))      행과 열의 독립성 검정 (이원카이제)       df가 위와같은 데이터셋일 때,  귀무가설 : cut(품질)과 color(색상)은 독립적이다.   from scipy.stats import chi2_contingency  chi2 = chi2_contingency(df) chi2  chi2의 첫번째 값 : chi suare, 두번째 값 : p-value  만약 p-value가 0.05아래라면 신뢰도 95%에서 귀무가설은 기각, 품질과 색상은 관련이 있다.  만약 큰 값을 가진다면 품질과 색상은 독립적이다.      동질성 검정 (이원카이제곱)    두 표본이 같은 모집단에서 나온 것인지 아닌지 판단할 수 있는지 검정  이는 위 독립성 검정의 방법을 그대로 따라하되  그 데이터가 어떤 구성인지의 차이 + 해석의 차이를 두면 된다.   좋은 예시가 있어서 가져와봤다. (출처 : https://hsm-edu.tistory.com/1213)      위와 아래의 차이는 아무런 코멘트가 없다면 ‘모른다’가 정답이다.  그러나 위 데이터는, 한번에 200명의 표본을 추출한것이고.  아래 데이터는 모집단에서 A 데이터 100명과 B 데이터 100명을 추출했다고 하면 차이가 느껴질 것이다.   위 데이터에서 이원카이제곱을 한다면  귀무가설 : 성별과 흡연유무는 독립적이다.   아래 데이터에서 이원카이제곱을 한다면  귀무가설 : A데이터와 B데이터는 다른 모집단에서 왔다. (독립적이다.)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/hypoo/",
        "teaser": null
      },{
        "title": "데이터 다루기",
        "excerpt":"pandas document   pandas 의 모든것이 들어있는 공식문서  https://pandas.pydata.org/docs/reference/index.html   DataFrame ?   데이터프레임을 먼저 알아야 하는데, 열(특성)과 행(단일데이터) 들로 이루어진 2차원 표라고 생각하면 된다.   데이터 불러오기     csv 불러오기    정확히는 csv 파일을 dataFrame의 객체타입으로 변환하는 것  import pandas as pd path = #csv의 경로 df = pd.read_csv(path) #tsv 파일이라면 comma(',')로 구분되는 것이 아닌 tab('\\t')으로 구분되기때문에 sep을 설정해줘야함. df = pd.read_csv(path, sep='\\t')  #csv 파일의 첫번째 줄은 feature의 이름인 경우가 많다. 그땐 index_col=0 으로 하면 0번째 행을 설정하는것 df = pd.read_csv(path, index_col=0)  # 내가 딱 필요한 feature를 정해서 불러온다. 아주 애용한다. df = pd.read_csv(path, usecols=['A', 'B'])    # skiprows를 이용하면 1,2번째 행은 제외할 수 있다. df = pd.read_csv(path, skiprows = [1, 2])  # nrows를 5로 하면 위에서 5개 데이터만 불러온다. df = pd.read_csv(path, nrows = 5)  #na_values를 이용해서 결측값을 제외하고 부를 수 있지만 dataframe의 내장함수를 사용하는 걸 개인적으로 선호한다. df = pd.read_csv(path, na_values = [0, '?', 'N/A', 'NA', 'nan', 'NaN', 'null'])   데이터 프레임 객체의 기본 내장함수  import pandas as pd df = pd.dataFrame(data)  # 데이터프레임 모양확인 df.shape  # 데이터 타입 확인 df.dtypes  # 인덱스 객체 반환 (인덱스는 각 행들의 이름) df.index  # features 의 인덱스 객체 반환 df.columns  # 전치형태의 데이터프레임객체 반환 (전치는 행열 반전) df.T  # 결측치 처리 df.fillna(0, inplace=True)  # 이 예시에서 결측치는 0으로 처리 및, inplace (default=false) 가 True이면 df원본을 손실하고 바로 변경된다. (false이면 결측치 처리된 df 수정본을 return할 뿐.)  # 컬럼 별 결측치 확인 df.isnull().sum()  # 요약통계량 확인 # count : 데이터의 수, mean/std : 평균/표준편차,  min/max : 최소/최대, 25%등 : 백분위수 df.describe()  # 각 feature들이 어떻게 분포되어있는지 한번에 확인가능!! ***** 버전 주의 !pip install pandas==1.2.1 !pip install pandas_profiling==2.8.0  from pandas_profiling import ProfileReport ProfileReport(df) #혹은 import pandas_profiling df.profile_report()  데이터 프레임의 데이터 슬라이싱   # 열접근 df.NAME df['NAME'] # 둘은 같은 결과를 도출한다.  # loc 사용 # feature의 '이름' 으로 추출 df.loc[:,['NAME','GENDER']]  # iloc 사용 # 행과 열의 번호(순서)로 추출 df.iloc[2:4, 0:3]     # 이 예시에서 2이상 4미만 행, 0이상 3미만의 열을 추출  # 조건부 슬라이싱 df[df.GENDER == 'M']  # 이 예시에서 'GENDER' 라는 features 가 'M'인 행들을 추출   feature engineering  데이터들을 변형시키거나 특성들간 함수처리를 통해 내가 원하는 특성을 만들어내보자   # apply (중요!) df.apply(함수)              # 여기서 함수는 파이썬 내장함수일 수도, 사용자 정의함수일 수도 있다. lambda도 사용 가능 df.apply(lambda x: x+1)                     #lambda사용 예시 모든 데이터에 1을 더하겠다. df['AGE'] = df['AGE'].apply(lambda x:x+1)   # 모든 AGE에 1을더한걸 원본df에도 적용하겠다.  # 특성간 계산 df['GENDER_AGE'] = df['GENDER'] + df['AGE'].apply(str)  # GENDER가 'M'이고 나이가 23 이면 GENDER_AGE라는 feature는 'M23'을 가짐.      🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/Data/",
        "teaser": null
      },{
        "title": "정규화, 표준화 그리고 PCA",
        "excerpt":"정규화와 표준화     정규화 (normalization)    정규화는 모든 값들을 0과 1사이의 값으로 단순하게 축소한다.  예를 들어 0~100까지의 값중 35 는 0.35가 될 뿐이다.   식 : x = (원래값 - 최댓값) / (최댓값 - 최솟값)   from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler()  df_scaled = scaler.fit_transform(df)      표준화 (standardization)    표준화는 데이터를 0을 중심으로 양쪽으로 데이터를 분포시킨다.  정확히는 0의 평균을 갖고, 1의 표준편차를 갖도록 변환하는 것.   식 : x = (원래값 - 평균) / 표준편차   from sklearn.preprocessing import StandardScaler sclaer = StandardScaler()  df_scaled = scaler.fit_transform(df)      fit 과 fit transform 의 차이    fit은 해당 데이터에 맞춤으로 모델(객체)을 설정해주는 것.  fit_transform 은 fit함과 동시에 해당 데이터를 모델을 사용해서 변형해주어 return함.   분산과 공분산      분산 (variance)    하나의 feature가 갖는 ‘평균으로 부터 퍼져있는 정도’ 이다.  식 : Var(x) = E [(X-X평균)^2]   분산의 양의 제곱근이 표준편차(Standard Deviation)      공분산 (Covariance)    두 feature가 갖는 공동 변화량이다. 직관적으로는 이해하기 힘들더라.. 그 의미를 파악하자면  0보다크면 X가 증가할때 Y도 증가한다.(양의 상관관계)  0보 작으면 서로 음의 상관관계  식 : Cov(X,Y) = E[(X-X평균)(Y-Y평균)]      상관계수 (Correlation coefficient)    공분산과 자연스럽게 이어지는데, 상관계수는 얼만큼 상관관계를 갖는지도 알려준다.   식 : Corr(X,Y) = Cov(X,Y) / (sd(X)*sd(Y))  (sd는 표준편차)   PCA  표준화or정규화 필수!!!!  PCA는 고 차원(feature 종류가 많을 때)의 데이터셋을 차원축소 하고자 할 때 사용한다.  여기서 중요한건 PCA는 특정한 feature를 선택(selection)하는 것이 아니라  모든 feature의 특징을 담아내는 feature로 추출(Extraction) 한다는 것이다.   어떻게 Extraction할 것이냐 !! 바로 축을 고르는 것 이다.   어떠한 축에 모든 feature들을 projection시켰을 때, ‘가장 그 정보들을 많이 담는다’면 그 축은 모든 feature의 특징을  잘 담고 있는 축이 될 것이다.  ‘정보를 많이 담는다’는 것은 공분산이 가장 큰 것이다라고 이해했으며,  그 축에 projection시킨 값들의 집합하나가 하나의 차원이 될 것이다.  그리고 그 다음 축은 첫번째 축과 직교인 축으로 고르게 될 것이다.     features = df.loc[:,'bill_length_mm':'body_mass_g'] species = df['species']  ↪️ df에서 ‘bill_length_mm’부터 ‘body_mass_g’ 까지의 feature의 데이터만 가져오고, ‘species’만 가져온다.  import pandas as pd from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() features = pd.DataFrame(scaler.fit_transform(features), columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])  ↪️ 표준화를 진행한다. (필수!!!!!)  import numpy as np from sklearn.decomposition import PCA  pca = PCA(n_components=2) extracted_df = pd.DataFrame(pca.fit_transform(features), columns=['PC1', 'PC2'])  ↪️ pca를 실행한 후 ‘PC1’, ‘PC2’로 저장된(자동으로 이름 이렇게 지음) 두 차원만 불러온다.  두 차원으로 차원축소 성공 !     🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/pca/",
        "teaser": null
      },{
        "title": "신용카드 데이터로 k-means-cluster, pca 연습 ",
        "excerpt":"데이터 선정 :  kaggle credit customer dataset  (https://www.kaggle.com/arjunbhasin2013/ccdata)   from google.colab import drive drive.mount('/content/drive')   Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).   import pandas as pd import numpy as np df = pd.read_csv('/content/drive/MyDrive/dataset/CC GENERAL.csv.xls') print(df.shape) df.head()   (8950, 18)                                       CUST_ID       BALANCE       BALANCE_FREQUENCY       PURCHASES       ONEOFF_PURCHASES       INSTALLMENTS_PURCHASES       CASH_ADVANCE       PURCHASES_FREQUENCY       ONEOFF_PURCHASES_FREQUENCY       PURCHASES_INSTALLMENTS_FREQUENCY       CASH_ADVANCE_FREQUENCY       CASH_ADVANCE_TRX       PURCHASES_TRX       CREDIT_LIMIT       PAYMENTS       MINIMUM_PAYMENTS       PRC_FULL_PAYMENT       TENURE                       0       C10001       40.900749       0.818182       95.40       0.00       95.4       0.000000       0.166667       0.000000       0.083333       0.000000       0       2       1000.0       201.802084       139.509787       0.000000       12                 1       C10002       3202.467416       0.909091       0.00       0.00       0.0       6442.945483       0.000000       0.000000       0.000000       0.250000       4       0       7000.0       4103.032597       1072.340217       0.222222       12                 2       C10003       2495.148862       1.000000       773.17       773.17       0.0       0.000000       1.000000       1.000000       0.000000       0.000000       0       12       7500.0       622.066742       627.284787       0.000000       12                 3       C10004       1666.670542       0.636364       1499.00       1499.00       0.0       205.788017       0.083333       0.083333       0.000000       0.083333       1       1       7500.0       0.000000       NaN       0.000000       12                 4       C10005       817.714335       1.000000       16.00       16.00       0.0       0.000000       0.083333       0.083333       0.000000       0.000000       0       1       1200.0       678.334763       244.791237       0.000000       12                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             데이터 확인   # 결측치 확인 df.isnull().sum()   CUST_ID                               0 BALANCE                               0 BALANCE_FREQUENCY                     0 PURCHASES                             0 ONEOFF_PURCHASES                      0 INSTALLMENTS_PURCHASES                0 CASH_ADVANCE                          0 PURCHASES_FREQUENCY                   0 ONEOFF_PURCHASES_FREQUENCY            0 PURCHASES_INSTALLMENTS_FREQUENCY      0 CASH_ADVANCE_FREQUENCY                0 CASH_ADVANCE_TRX                      0 PURCHASES_TRX                         0 CREDIT_LIMIT                          1 PAYMENTS                              0 MINIMUM_PAYMENTS                    313 PRC_FULL_PAYMENT                      0 TENURE                                0 dtype: int64   # 데이터 타입 확인 df.dtypes   CUST_ID                              object BALANCE                             float64 BALANCE_FREQUENCY                   float64 PURCHASES                           float64 ONEOFF_PURCHASES                    float64 INSTALLMENTS_PURCHASES              float64 CASH_ADVANCE                        float64 PURCHASES_FREQUENCY                 float64 ONEOFF_PURCHASES_FREQUENCY          float64 PURCHASES_INSTALLMENTS_FREQUENCY    float64 CASH_ADVANCE_FREQUENCY              float64 CASH_ADVANCE_TRX                      int64 PURCHASES_TRX                         int64 CREDIT_LIMIT                        float64 PAYMENTS                            float64 MINIMUM_PAYMENTS                    float64 PRC_FULL_PAYMENT                    float64 TENURE                                int64 dtype: object   # 결측치 처리 (5%이하의 갯수이므로 드랍하겠다.) df.dropna(inplace=True) df.shape   (8636, 18)   import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings('ignore') #경고문을 무시한다.  i=1 plt.figure(figsize= (20,40)) for col in df.drop('CUST_ID', axis=1).columns:     plt.subplot(9,2,i)      sns.distplot(df[col])      i=i+1 plt.show()      # 상관 계수 (correlation coefficient) 확인  plt.figure(figsize=(10,10)) sns.heatmap(df.corr(), annot=True, cmap='coolwarm')   &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff838ba2890&gt;      표준화 및 PCA  +축소할 차원 수 결정.   # 상관관계가 어느정도 있는 feature가 보이므로 PCA 사용이 유의미할 것으로 판단. df.drop('CUST_ID', axis=1, inplace = True) from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler  # PCA전 표준화 ss= StandardScaler() df= ss.fit_transform(df)  #PCA 진행 pca= PCA() pca.fit(df)   PCA()   # PCA 의 축소 차원갯수에 대한 정확도 차이 (기존 차원수를 유지하는게 당연히 100%일것임..) plt.plot(pca.explained_variance_ratio_.cumsum())   [&lt;matplotlib.lines.Line2D at 0x7ff8393998d0&gt;]      # 차원 축소 7개로 하겠다! pca= PCA(n_components=7) df_pca= pca.fit_transform(df)   df_pca.shape   (8636, 7)   K-means Clustering  +K-means 와 실루엣 스코어(silhouette_score)      k-means : 각 데이터들과 해당 centroid까지 거리 합   sil_score : 1에 가까울수록 군집과 군집이 잘 분리되어있다는 뜻    기본적으로 0이상이고 만약 음수라면 군집끼리 겹쳤다는 의미   from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score plt.figure(figsize=(15,10)) distortions=[] sil_scores=[] for i in range(2,30):     # n_cluster : 군집 갯수, n_iter : 중심점 업데이트의 최소 횟수     kmeans= KMeans(n_clusters=i, n_init=10, init= 'k-means++', algorithm='full', max_iter=300)     kmeans.fit(df_pca)     # inertia_ : k-means 구하는 중에 centroid로부터 데이터들의 거리 데이터 (클 수록 중심점으로부터 멀다는 거겟쥐)     distortions.append(kmeans.inertia_)     label= kmeans.labels_     sil_scores.append(silhouette_score(df_pca, label)) plt.plot(np.arange(2,30,1), distortions, alpha=0.5) plt.plot(np.arange(2,30,1), distortions,'o' ,alpha=0.5) plt.show()      plt.figure(figsize=(15,10)) # sil_scores 확인 plt.plot(np.arange(2,30,1), sil_scores) plt.plot(np.arange(2,30,1), sil_scores,'o' ,alpha=1) plt.show()      sil_scores # k-means 는 군집을 늘릴수록 감소 (이상적) # 실루엣 계수는 3에서 0.28정도로 그나마 크나, 전체적으로는 0.25근처로 별로 크게 나오진 않음.   [0.24692450845604266,  0.28065750721461125,  0.2507007863496503,  0.2408710781955889,  0.25789671860000646,  0.25543152274952236,  0.26596171359898996,  0.25980372706530475,  0.2606025124276636,  0.24195602184083095,  0.25144013095228146,  0.23962807370519182,  0.23713138793264468,  0.23981648640294187,  0.2199707245532794,  0.2259075081873923,  0.23446181289456078,  0.21832255569020825,  0.24002990422966186,  0.23109689399081063,  0.22381807371604162,  0.21453215289991348,  0.21537478705286658,  0.21174420647359052,  0.2161573025700017,  0.21208142071199876,  0.2196422007478946,  0.20723949847328374]   # k-means 는 군집 3개로(실루엣계수 따라), PCA는 2개로하여 시각화.  kmeans= KMeans(n_clusters=3, n_init=10, init= 'k-means++', algorithm='full', max_iter=300) kmeans.fit(df_pca) labels= kmeans.labels_  pca= PCA(n_components=2) temp = pca.fit_transform(df) df_pca2 = pd.DataFrame(data=temp, columns=['pca1','pca2']) df_pca2['labels']= labels df_pca2.head()                                       pca1       pca2       labels                       0       -1.696397       -1.122594       2                 1       -1.215688       2.435597       1                 2       0.935860       -0.385170       2                 3       -1.614640       -0.724592       2                 4       0.223706       -0.783584       2                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             plt.figure(figsize=(10,10)) ax = sns.scatterplot(x='pca1', y='pca2', hue='labels', data=df_pca2, palette='bright')            🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["cluster","pca"],
        "url": "http://localhost:4000/pj/Credit_Card_Cluster/",
        "teaser": null
      },{
        "title": "Ridge 회귀와 로지스틱 회귀",
        "excerpt":"CV (교차 검증, cross validation)  먼저 데이터라고 하면 용도에 따라 3가지로 분류할 수 있다.  학습데이터(training_data),  검증데이터(validation_data),  테스트데이터(test_data)   세가지 데이터는 서로 누출되어서는 안되고,  학습데이터로 학습시킨 ‘모델’ 의  점수는 ‘검증데이터’로 체크를 하되,  마지막으로 타겟 데이터로써 테스트 데이터를 사용한다.   단, CV는 ‘검증데이터’를 따로 두지 않고,  학습데이터를 일부 추출하여 검증데이터로 사용한다는 방식이다.   만약 그저 데이터를 처음부터 ‘학습데이터’와 ‘검증데이터’로 나눈다면 아래와 같을 것이다.     그러나 CV를 이용하게 되면 아래와 같이 학습데이터와 검증데이터가 수시로 바뀌게 되며. 더 ‘일반화’ 된 모델을 얻을 수 있다.     릿지 회귀 (Ridge regression)  간단하게 Ridge 회귀는 단순회귀보다 더 일반화된 모델을 만든다고 이해해도 좋을 듯 하다.  즉, 단순선형회귀의 과적합 방지 모델.   from sklearn.linear_model import RidgeCV  # 수행해볼 알파 값들을 정의한다. alphas = [0.01, 0.05, 0.1, 0.2, 1.0, 10.0, 100.0]  # RidgeCV 모델 객체를 정의한다. ridge = RidgeCV(alphas=alphas, normalize=True, cv=3) ridge.fit(ans[['x']], ans['y'])  #결정된 알파와 베스트 스코어를 출력. print(\"alpha :\", ridge.alpha_) print(\"best score :\", ridge.best_score_)  # 예측해보고 싶은 X_test에 대해 y y_pred = ridge.predict(X_test)   로지스틱 회귀 (logistic Regression)  로지스틱 회귀는 sigmoid라는 ‘비선형’을 사용한 ‘2진 분류(classification)’ 모델이다.   예시 ) 환자들의 생체 데이터 + 암에 걸리지 않았다면 0, 걸렸다면 1을 갖는 feature (target)  from sklearn.linear_model import LogisticRegression  model = LogisticRegressionCV(penalty=\"l1\", Cs=[1.0], solver='liblinear', cv=3) model.fit(X, y) # 여기서 X는 여러 feature를 갖고 있을 수 있고, # y는 0이나 1의 값을 갖는 feature일 것이다!  # 스코어 프린트 print('best score: ', model.scores_)  # 테스트 데이터 확인 y_pred = model.predict(X_test) y_pred_proba = model.predict_proba(X_test) # proba는 y_pred를 결정했던 근거가 된 각 확률을 보여준다.  # 계수 확인 coefficients = pd.Series(model.coef_[0], X.columns) coefficients    # 계수가 -1부터 1사이의 값을 갖는데 -1에 가까운 값일 수록 0이 나오게 하는 feature임을 의미      sigmoid    x값이 어떤값이든지, y값은 0부터 1사이의 값을 비선형으로 갖도록 하는 함수       임계값 (Classification Threshold)    임계값이 로지스틱 회귀에서 등장했는데,  어떤 데이터 하나가 A에 속할 확률이 0.41이 나왔다고 했을때,  임계값이 0.5(default)라면 당연히 ‘A가 아니다’ 라고 하겠지만,  임계값을 0.4으로 조정한다면 ‘A 이다’ 라고 판단을 내리게 된다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/ridge_logistic/",
        "teaser": null
      },{
        "title": "특성 선택과 특성 추출",
        "excerpt":"차원을 축소하는 데에는 어떤 방법이 있을까.  첫번째로 영향도 높은 특성을 ‘고르는 것’ 과  두번째는 특성 모두를 특정 축에 ‘투영시키는 것’ 이 있다.   특성 선택 (feature selection)  특성을 줄이는데,   영향도가 큰 특성을 골라야  모델의 정확도를 유지하면서 복잡도를 줄이거나 일반화를 할 수 있을 것이다.  1. Filter method (전처리 과정에서 통계값으로 선택.)  통계값 종류 : 카이제곱, ANOVA_f_score, 상관계수 등.  from sklearn.feature_selection import chi2, SelectKBest selector1 = SelectKBest(chi2, k=14330) X_train1 = selector1.fit_transform(X_train, y_train) X_test1 = selector1.transform(X_test)  ↪️ 카이제곱으로 선택한 예시  2. Wrapper method (모델학습과 검정(validation)을 반복하면서 특성을 선택)  ‘무식한 정답’ 이지만 비합리적인 방법.(greedy 나 grid 단어가 떠오른다.. 왜 발음도 비슷하지ㅎ)  시간과 비용이 많이 소모되며, 상대적으로 filter metthod 보다 과적합되기 쉽다.  forward step, backward step, stepwise 방식이 있는데,  각각 feature를 하나씩 ‘추가’, ‘소거’, ‘추가소거 병합’ 방법이다.  from mlxtend.feature_selection import SequentialFeatureSelector  feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),            k_features=15,            forward=True,            verbose=2,            scoring='roc_auc',            cv=4) features = feature_selector.fit(np.array(X, y) filtered_features= train_features.columns[list(features.k_feature_idx_)] #이후 filtered_features로 모델에 fit  ↪️ mlxtend 라이브러리의 SequentialFeatureSelector 를 사용하여, forward step예시  roc_auc 를 평가지표로, RandomForestClassifier 모델을 이용하여 forward step 기법을 사용.  forward 변수만 False로 바꾸면 backward로 사용  from mlxtend.feature_selection import ExhaustiveFeatureSelector from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import roc_auc_score  feature_selector = ExhaustiveFeatureSelector(RandomForestClassifier(n_jobs=-1),            min_features=2,            max_features=4,            scoring='roc_auc',            print_progress=True,            cv=2) features = feature_selector.fit(X, y) filtered_features= train_features.columns[list(features.k_feature_idx_)] #이후 filtered_features로 모델에 fit  ↪️ mlxtend의 ExhaustiveFeatureSelector를 사용한 stepwise 예시  추가소거를 동시에 하는 기법, 최종feature의 최소갯수와 최대갯수를 정한다.  3. Embedded method (학습과 동시에)  l1 norm(LASSO회귀에서 사용, 절댓값으로 loss함수 보정),  l2 norm(RIDGE회귀에서 사용, 제곱값으로 loss함수 보정),  elastic_net(l1 + l2),  selectfrommodel(sklearn의 함수를 이용하여, decision Tree기반 모델, 로지스틱등의 모델 등을 통해 feature_importance계산하여 feature 선택)  from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier from sklearn.linear_model  import LogisticRegression  # 3가지 모델을 이용해서 feature_importance를 통한 feature선택 모델 설정. RFselector = SelectFromModel(estimator=RandomForestClassifier()).fit(X, y) GBMselector = SelectFromModel(estimator=GradientBoostingClassifier()).fit(X, y) LRselector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)  # feature 보기 (get_support 내부변수를 보면 True, False로 선정 feature를 구별한다) columns = data.columns RF_selected = columns[RFselector.get_support()] GBM_selected = columns[GBMselector.get_support()] LR_selected = columns[LRselector.get_support()]   특성 추출 (feature extraction)  대표적으로 pca와 같이 특성들을   특정 축( 특성들의 특징을 잘 나타내야 한다는 이유로 보통 분산을 최대로하는 축을 선택한다. )에  투영(projection)시켜 차원을 축소하는 방식이 있다.  방법에 대해서는 https://chan9480.github.io/ds/pca/ 에서 조금 다루어놓았다.   한계점을 말하자면 무조건 직선에 투영시킨다는 것. 분산이 큰 특성을 무조건 중요 특성으로 판단한다는 것.  그래서 차원축소… 선택할거야, 추출할거야?  답은 없겠지만 개인적으로 pca는 시각화나 비지도 군집화에 유용하게 사용되며,  그 외 분류, 회귀 등에는 선택이 많이 쓰인다고 봐도 될 것 같다.  그 중에서도 filter method와 embedded method 의 응용을 앞으로 좀 더 공부해보고자 한다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/feature_Selection_Extraction/",
        "teaser": null
      },{
        "title": "파이프라인(PipeLine)과 결정트리모델(Decision Tree)",
        "excerpt":"사이킷런(sklearn) 파이프라인 (PipeLine)  사이킷런에서 제공하는 파이프라인은 각 기능을 하는 모델들을 한번에 묶는 기능과  하이퍼 파라미터를 연결시키는 기능이 있다.   from sklearn.pipeline import make_pipeline from category_encoders import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.tree import DecisionTreeClassifier  pipe = make_pipeline(     OneHotEncoder(use_cat_names=True),       SimpleImputer(),     StandardScaler(),     DecisionTreeClassifier(random_state=1, criterion='entropy', min_samples_leaf=10, max_depth=6)      # min_samples_leaf는 말단 노드에 최소 존재해야할 데이터 수.     # max_depth 는 최대 깊이를 제한하여 복잡도를 개선. )  # pipe fit pipe.fit(x_train, y_train) print('검증세트 정확도', pipe.score(X_val, y_val))  # 테스트 셋 y_pred = pipe.predict(X_test)  # feature_importance 띄우기 ## 먼저 파이프 내의 학습된 모델들 떼어서 가져온다. model_dt = pipe.named_steps['decisiontreeclassifier'] enc = pipe.named_steps['onehotencoder']  encoded_columns = enc.transform(X_val).columns  importances = pd.Series(model_dt.feature_importances_, encoded_columns)   결정트리 (Decision_Tree)  결정트리는 ‘분류’에 있어서 마치 ‘회귀’의 선형회귀 와 같은 느낌이다.  데이터들을 계속해서 두가지씩 분류하여 결과적으로 모든 데이터들을 정해진 갯수의 class들로 분류하게 된다.     결정트리를 발전시킨  ‘랜덤포레스트 (Random_Forest)’,  ‘그래디언트 부스트 트리 (Gradient Boosted Tree)\u001c’  같은 모델들을 더 많이 사용할 것이다.  그러나 그 기초는 결정트리에 있다.      트리학습에서의 비용함수       지니지수 (Gini Impurity   엔트로피 (Entropy)   두가지 모두 불순도를 나타내는 척도 이며  클수록 골고루 섞여 있다는 뜻.(10개의 공들 중에 5개씩 빨간공, 파란공이라면 0.5) 즉, 0에 가까울수록 치우쳐져 있다는 뜻. (전부 특정공만 10개 있다면 0)   즉 지니지수, 엔트로피가 작아지는 방향으로 트리 노드를 생성한다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/decision_tree/",
        "teaser": null
      },{
        "title": "랜덤포레스트(Random Forest)와 ...",
        "excerpt":"랜덤 포레스트 모델  단순 선형회귀와 릿지(Ridge) 회귀가 있었다면,  결정트리와 랜덤포레스트가 있다.   릿지 회귀에서 과적합을 방지하는 장치가 있었다.  랜덤포레스트 또한 결정트리에서 더 일반화시켜주는 장치가 있다.   결정트리를 여러개 만들어 모두의 의견을 합산하여 판단을 내린다.      숲(포레스트)이 만들어지는 과정       많은 feature중에서 n개 (pram: max_features) 를 ‘랜덤’으로 고른다.   n개의 feature중 가장 영향도가 큰 feature를 골라 첫번째 node를 생성하고, 나머지 feature 중 랜덤하게 골라 트리를 완성한다.   위와같은 트리를 m개(pram: n_estimators)만든다.   트리들의 분류 결과로 투표를 해서 최종 결정을 한다.   from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score  # bootstrap을 False로 하고 max_features 수정가능. classifier = RandomForestClassifier(n_estimators = 50, bootstrap = False, max_features = 5)  # 모델 fit classifier.fit(X_train, y_train)  # 결과값 예측 y_pred = classifier.predict(X_test)  # 같은지 다른지 확인. print(\"정확도 : {}\".format(accuracy_score(y_test, y_pred))      CV(cross validation)은 GridsearchCV, RandomizedSearchCV 등을 이용.    # CV(cross validation)은 gridsearch 등을 이용. from sklearn.model_selection import GridSearchCV  grid = {     'n_estimators' : [100,200],     'max_depth' : [6,8,10,12],     'min_samples_leaf' : [3,5,7,10],     'min_samples_split' : [2,3,5,10] }  classifier_grid = GridSearchCV(classifier, param_grid = grid, scoring=\"accuracy\", n_jobs=-1, verbose =1)  classifier_grid.fit(X_train, y_train)  print(\"최고 평균 정확도 : {}\".format(classifier_grid.best_score_)) print(\"최고의 파라미터 :\", classifier_grid.best_params_)     feature_importances 내부변수로 확인가능    import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  feature_importances = model.feature_importances_  ft_importances = pd.Series(feature_importances, index = X_train.columns) ft_importances = ft_importances.sort_values(ascending=False)  plt.figure(fig.size(12,10)) sns.barplot(x=ft_importances, y= X_train.columns) plt.show()       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/random_forest/",
        "teaser": null
      },{
        "title": "이진분류의 평가지표",
        "excerpt":"이진분류 모델의 종류  로지스틱 회귀,  Decision_Tree(base model),  RandomForest,  Gradient_boost,  KNN(K-Nearest-Neighbors),  그외 딥러닝 모델도 될 수 있겠다.   평가지표의 종류  위와 같은 이진분류 모델 (군집x, 지도학습의 분류o) 의 평가지표는 어떤 게 있을까.  (회귀에서는 MSE, MAE, RMS, R-score 등이 있었쥬!)   가장 단순하게 볼 수 있는 것은 validation 데이터에 대해서 얼마나 많이 맞춘 비율(0~1)이 있을 것이다.  바로 정확도(Accuracy)!    그러나 모델을 하나의 값으로 평가하면 안될 것이다.      정확도 (Accuracy)  먼저 정확도를 체크할 때에도 베이스 모델은 반드시 필요하다.  그 이유를 예로 들어보자면,  암을 예측하는 모델의 학습데이터에서 암에 걸린 데이터(1)가 5%, 나머지 95%가 건강(0)하다면,  입력에 상관없이 항상 출력을 ‘0’으로만 한다면 그 모델은  CV나 hold-out 으로 비슷한 비율로 생성된 validation 데이터에서도 정확도를 대략 0.95로 가질 것이다..!  이러한 모델은 정확도 검증에 있어서 0.95는 최소한 넘는 것으로 고려해야 할 것이다.       TP, TN, FP, FN  (T,F는 True, False,   P,N은 Positive, Negative,)  True는 맞춘거, False는 못맞춘거.  P와 N은 예측한 거 기준. (FP는 실제 Negative인데 모델이 Positive로 예측한거야)       정밀도(precision)와 재현율(recall, sensitivity), specificity(TN-rate), Fall-out  precision = TP/(TP+FP) = Positive로 예측한 것들중 맞춘비율  recall = TP/(TP+FN) = 실제 Positive 중 맞춘비율  (TPRate) specificity = TN/(TN+FP) = 실제 Negative 중 맞춘비율 (TNR) Fall-out = FP/(FP+TN) = 실제 Negative 중 틀린비율 (FPR)       confusion Metrics  TP, TN, FP, FN 을 한번에 나타낸 행렬  아래 사진을 참고하면 한번에 이해될것.ㅎㅎ    from sklearn.metrics import plot_confusion_matrix import matplotlib.pyplot as plt  # pipe는 파이프라인 fig, ax = plt.subplots() matrix = plot_confusion_matrix(pipe, X_val_cleaned, y_val,                             cmap = plt.cm.Blues,                             ax = ax);        AUC, ROC 와 임계값(Threshold)  ROC 커브는 임계값에 따른 FPR과 TPR 값이 그리는 커브인데. 모두 일대일 관계이다.   AOC수치는 ROC curve의 아래 면적을 뜻한다.  AOC는 클수록 좋은 모델인 것은 사실이다.  이유 : FPR (틀린비율, 낮을수록 좋음) 이 낮음에도 TPR(맞춘비율, 높을수록 좋음)은 높아야 AOC가 높은 것이기 때문.  결과적으로 tpr-fpr이 최대가 되는 점의 임계점을 고르는게 최선이라고 할 수 있다.             🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/_classfication_metrics/",
        "teaser": null
      },{
        "title": "크롤링(Crawling)",
        "excerpt":"크롤링이란 ?  web에 있는 데이터들을 긁어모으는 것을 말한다.  크게 두 종류로 먼저 나눌 수 있다.     정적크롤링 : 항상 같은 값을 주는 HTML로 부터 파싱을 해서 크롤링.   동적크롤링 : 같은 HTML이라도 동작, 명령을 통해 변화된 상태에서 데이터들을 크롤링.   정적크롤링은 멈춰있는 페이지에서 정보를 찾아 긁어모은다면,  동적크롤링은 검색, 스크롤, 페이지 클릭 등을 해서 나오는 정보를 긁어모을 수 있다.   대표적으로 정적크롤링 관련 라이브러리로 beautifulsoup(bs4)가 있다.  그리고 동적크롤링의 방법에도 여러 종류가 있는데 그 중 2가지를 적어보자면 다음과 같다.     openAPI를 이용하여 명령 후, response된 정보로부터 크롤링.   selenium을 통해 webdriver (크롬, 사파리 등)를 제어한 후 나온 페이지(HTML)로부터 크롤링.   크롤링 예시  먼저 아래 함수 두개를 지정하겠다.     diScrollDown : 특정 시간동안 스크롤을 내리는 함수.   createDirectory : 입력값으로 받은 문자열(경로)에 해당하는 폴더를 생성한다.   crawling_img : 입력값으로 받은 문자열을 크롬에서 검색해서 함수내에 지정되어 있는(직접변경) 경로로 이미지를 저장. (이름은 번호순으로 증가)   from selenium import webdriver from selenium.webdriver.common.keys import Keys import time import urllib.request import os  def doScrollDown(driver, whileSeconds):     start = datetime.datetime.now()     end = start + datetime.timedelta(seconds=whileSeconds)     while True:         driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')         time.sleep(1)         if datetime.datetime.now() &gt; end:             break  def createDirectory(directory):     try:         if not os.path.exists(directory):             os.makedirs(directory)     except OSError:         print(\"Error: Failed to create the directory.\")  def crawling_img(name):     ## 입력값 name 문자열을 검색하여 나오는 이미지를 저장하는 함수.     ## 저장하는 경로는 함수 내에서 별도로 지정해야함.      # 크롬을 드라이버로 채택. 버전오류가 날 수 있다.     # 사파리는 webdriver.Safari()를 사용하면 된다.     driver = webdriver.Chrome()     driver.get(\"https://www.google.co.kr/imghp?hl=ko&amp;tab=wi&amp;authuser=0&amp;ogbl\")      # q로 태그되어 있는 곳이 구글홈페이지의 검색창이다.     # driver.set_window_size(a,b) 로 창 크기 지정 가능     elem = driver.find_element_by_name(\"q\")     elem.send_keys(name)     elem.send_keys(Keys.RETURN)       SCROLL_PAUSE_TIME = 1   # 1초씩 기다렸다가 내렸다를 반복할거임.       # Get scroll height     last_height = driver.execute_script(\"return document.body.scrollHeight\")  # 브라우저의 높이를 자바스크립트로 찾음     while True:         # Scroll down to bottom         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # 브라우저 끝까지 스크롤을 내림         # Wait to load page         time.sleep(SCROLL_PAUSE_TIME)         # Calculate new scroll height and compare with last scroll height         new_height = driver.execute_script(\"return document.body.scrollHeight\")         if new_height == last_height:             try:                 # 더보기 버튼을 클릭할 거임.                 driver.find_element_by_css_selector(\".mye4qd\").click()             except:                 # 더보기 버튼이 없어서 클릭을 못하면 끝.                 break         last_height = new_height      imgs = driver.find_elements_by_css_selector(\".rg_i.Q4LuWd\")      # 경로와 폴더 명 지정.     dir = \".\\tree_flower_dog_cat\" + \"\\\\\" + name      createDirectory(dir) #폴더 생성     count = 1     for img in imgs:         try:             img.click()             time.sleep(3)             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img').get_attribute(                 \"src\")             path = \".\\idols\\\\\" + name + \"\\\\\"             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \".jpg\")              # 이 아래는 관련이미지 저장             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[1]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_1\" + \".jpg\")             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[2]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_2\" + \".jpg\")              imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[3]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_3\" + \".jpg\")              count = count + 1             if count &gt;= 500:                 break         except:             pass     driver.close()   이제 위 함수를 사용하여 for문을통해 검색 및 저장을 동시에 해주면 된다.   searching_keyword = [\"나무\", \"꽃\", \"강아지얼굴\", \"고양이얼굴\"]  for i in range(len(searching_keyword)) :     searching_keyword[i] += '_사진' # 사진을 뒤에 붙이면 검색이 잘될 것 같아!  for keyword in searching_keyword:     crawling_img(keyword)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["de"],
        "tags": [],
        "url": "http://localhost:4000/de/crawling/",
        "teaser": null
      },{
        "title": "데이터 인코딩(encoding)",
        "excerpt":"인코딩은 왜 하는거?  데이터에는 연속적인 숫자만 있는게 아니다.  문자열의 카테고리형 feature일 수도,  숫자라 하더라도 비연속적인 카테고리형 feature도 있다.  (예: 사는지역(“수유동”, “인수동” …), 평점(“매우별로”,”별로”, … , “매우좋음”))   인코딩은 이러한 카테고리형 feature들에 대하여 ‘데이터’로써 유의미하도록 숫자로 바꿔주는 역할을 한다.   인코딩의 종류와 간단 정리  본래 데이터        One Hot Encoding  하나의 feature가 갖는 범주 전체에 대하여 ‘이다’, ‘아니다’로 분류하여 0, 1을 갖는 feature를 생성.  (사는지역의 종류가 “수유동”, “쌍문동” 등 16개의 동이 있다면, 16개의 feature (사는지역_수유동, 사는지역_쌍문동… 등)     pd.get_dummies(df, prefix=[\"지  \"], columns=[\"사는지역\"]) #또는 아래처럼 sklern.preprocessing의 함수 사용 from sklearn.preprocessing import OneHotEncoder                 Ordinal Encoding categorical_feature의 값들이 어떤 ‘순서’를 갖고있을 때 사용한다.  (‘매우 그렇다’, ‘그렇다’, ‘보통’, ‘아니다’, ‘매우 아니다’) 같은거!     from sklearn.preprocessing import OrdinalEncoder enc = OrdinalEncoder(categories = [['불만족', '보통', '만족']]) df['선호도_enc']=enc.fit_transform(df[['선호도']])                 Binary Encoding OneHotEncoding의 이진수 버전이라고 이해했다.  사는지역의 종류가 16개의 동이 있다면. 4자리 이진수로 표현이 가능하므로 4개의 feature를 생성 한다.(cardinality가 너무 큰 특성에 대해서 사용하면 장점일 듯!)  ‘수유동은’ 4개의 feature에 0이나 1이 채워져 0000부터 1111중 하나의 형태를 가질 것!     import category_encoders as ce encoder = ce.BinaryEncoder(cols=['사는지역']) dfbin = encoder.fit_transform(df['사는지역']) df = pd.concat([df, dfbin], axis=1) df.drop(['선호도','성별(1남, 0여)'], axis=1)                 Frequency Encoding 빈도로써 표현하는 방법  ‘수유동’이 10개 데이터중 2번있다면 0.2로 매핑됨.     # Frequency Encoding fe = df.groupby(\"사는지역\").size()/len(df) df.loc[:, \"사는지역_freq_encode\"] = df[\"사는지역\"].map(fe) df                      Mean Encoding  여기부터는 지도학습에만 해당하는 내용이라고 생각한다. ‘target’ 이 존재할 때만 가능하기 때문!  어떻게 인코딩을 시킬 지를 ‘타겟의 평균값’에 따라 결정한다.       먼저, 단순하게 target의 평균자체로 매핑을 하는 방법이 있는데,  (target을 gender라 하면, 수유동에사는 사람들의 target평균을 수유동의 인코딩 매핑값으로 삼는다.)  과적합이 되기 쉽다.  ————————————————————————————–       두번째로, smoothing mean target encoding은 과적합을 좀더 방지한다.  (수유동 사람들의 target평균 / weight) + (전체 target평균 / 수유동의 갯수)   의미 : weight로 개별평균을 분산시켜주고, 수유동의 갯수가 클수록 수유동평균에 힘을, 수유동 갯수가 적다면 전체평균의 힘을 실어준다.  weight가 클수록 편차가 작아진다!       # smoothing target encoding  # 1. 평균을 계산  mean = df['성별(1남, 0여)'].mean()  # 2. 각 그룹에 대한 값들의 빈도와 평균을 계산  Agg = df.groupby('사는지역')['성별(1남, 0여)'].agg(['count', 'mean'])  counts = Agg['count']  means = Agg['mean']  weight = 10  # 3. “smooth”한 평균을 계산  smooth = (counts * means + weight * mean) / (counts + weight)  # smooth한 평균에 따라 각 값을 대체하는 것  print(smooth)  df.loc[:, '사는지역_smean_enc'] = df['사는지역'].map(smooth)  df                 Probability Ratio Encoding (확률비율 인코딩)  (target이 1인 확률 / target이 0인 확률) 의 비율로 매핑을 하는 방법이다.  주의할 점은 0으로나뉘는걸 꼭 방지하자!    (예 : ‘수유동’들 중에 target이 1인갯수는 3개, 0인갯수는 1개라면, ‘수유동’은 3으로 매핑됨.)   Weight of Evidence Encoding  위 PRE의 비율에 log_2를 취하고 weight을 곱해준 것으로 인코딩하는 방법.  (6번 인코딩 예시에서 ‘수유동’은 w*ln(3)의 값으로 매핑 될 것.)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/encoding/",
        "teaser": null
      },{
        "title": "사람얼굴에 스타일 바꿔보기",
        "excerpt":"목표  임의의 초상권 없는 얼굴을 특정 스타일을 입혀 만들기.   방법  임시로 생성한 사람얼굴(실존x) 이미지에 원하는 특정 스타일을 가진 사람(실존o)의 이미지를 적용하여 새롭게 생성.      styleGAN2-ada 모델의 ffhq preTrained 가중치를 사용하여 얼굴생성.   원하는 스타일의 사람사진의 스타일 벡터들을 추출 (PSP 모델의 일부 사용)   1번에서 생성한 이미지를 inversion하여 다시 이미지를 생성하는 과정에서 2번의 스타일 벡터들을 inject하여 최종 생성.      StyleGAN2 - ada 간단 요약  GAN (latent vector로 부터 이미지 생성) 모델 중에서  styleGAN (latent vector를 style 별로 생성된 여러 w-vector로 만들어 이미지 생성) 이 있다.   ada는 데이터 증강기법(적은 데이터로 다양한 데이터생성, 일반화 효과 + 데이터 수 늘리는 효과)   PSP(pixel2style2pixel) 간단 요약  구조를 두 부분으로 나눌 수 있는데,     psp encoder : 이미지를 매핑하여 w-vector를 생성함.   styleGAN generator  : w-vector를 사용하여 이미지를 생성(styleGAN 방식과 같이 해상도를 올리면서 이미지 생성.)   이 구조를 이용해서 여러 기능으로 사용가능 (ffhq_encode, celeb_seg_to_face, toonify 등)   test 이미지  styleGAN2-ada, ffhq-pretrained 로 생성한 이미지 들          결과 이미지     스타일이미지         결과 이미지  순서대로 “스타일 적용전” ::::::: “w벡터 중 랜덤으로 1 개만 적용” ::::::: “모든 w벡터를 적용한 경우”                   결론   결과를 해석하자면     스타일이미지의 얼굴형, 머리스타일 등에 원본이미지의 눈코입, 피부 등을 적용시켜 이미지를 생성한다.   랜덤한 w벡터를 적용했을 때, 스타일 이미지의 정확히 어떤 특징을 담은 w벡터가 적용된 것인지 알기 힘들다..  (물론 스타일이미지와 무언가 닮아지긴 한다.)   성공적인가의 여부는 스타일 이라는 애매한 단어에 어떤 걸 포함시키느냐에 따라 해석이 다를 것 같다.  얼굴형, 머리스타일을 ‘스타일’이라고 한다면, 성공에 가깝다고 볼 수 있겠다!  다만 원하는 스타일(머리스타일, 얼굴형 중 하나를 택하고 싶은 경우)을 적용하는데에는 실험적인 시도가 필요해 보인다.   레포지토리 링크  https://github.com/chan9480/Style_image_GAN       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["styleGAN2","PSP"],
        "url": "http://localhost:4000/pj/stylegan/",
        "teaser": null
      },{
        "title": "CNN 구조 이해",
        "excerpt":"CNN 의 목적  컴퓨터에 이미지를 이해시키는 방법을 생각해보자.  이미지는 RGB, BGR의 3자리 숫자가 모든 점들을 구성한다.  그렇다면 강아지 사진 1의 모든 RGB 숫자들을 그대로 컴퓨터에게 알려준다면,  컴퓨터는 구도만 살짝 바뀐 강아지 사진 2는 전혀 다른 사진이라고 생각할 것이다.   이를 위해 CNN에서는 ‘필터’라는 개념을 이용하여 ‘특징’을 추출한다.   이것만 이해하자. No.1    필터는 위와같이 적용된다. 필터 하나하나가 가중치의 역할을 하며, 필터의 모양에 따라 이미지의 어떤 특징들을 추출하는지 결정된다.   이것만 이해하자. No.2    —————————————————————————-  CNN의 Layer를 하나씩 쌓고 있다.  첫번째 Conv2D 코드를 보자,  (32,32) 사이즈의 3겹(rgb, bgr) 이미지를 3by3 필터 가중치를 이용하여 32개의 feature map을 만들겠다고 되어있다.  그 결과, (30,30) 사이즈의 32겹 피쳐맵을 만들었고, param수는 896개이다.  가중치의 갯수는 신경망의 원리에서 각 가중치를 필터로 갖는다고 생각하면 쉽다.  3 (=input) x 32(=output) x 9(=3*3필터) + 32(퍼셉트론 신경망 node와 같이 각 feature맵 가중치)  =896   이것만 이해하자. No.3    No.2의 예시에서 (32,32)가 (30,30)으로 되는 건 3*3필터를 대입해보면 얼추 바로 알 수 있는데,  stride나 padding이 들어가면 헷갈릴 수 있다. 위 식에 대입해서 정확히 구할 수 있다.   키워드 정리     필터 (kernel, filter)   피쳐맵 (feature map)   패딩 (padding, zero padding) : 가장자리에서 필터때문에 소실되는 데이터를 막기 위해 사용, 이미지를 한번 0으로 감싸는 것.            사이즈에 따라 분류 : valid padding(패딩안함),   full padding (필터사이즈-1 만큼두께로 모두 감싸는 것, edge의 데이터 손실을 막는다는 의의를 가진다),   same padding(input이미지와 output이미지 사이즈가 같도록 패딩, 사이즈가 점점 작아지는 현상을 막는 다는 의미가 있음.\u001f)           스트라이드 (stride) : 커널을 대입할 때, 이동하는 칸 수를 말한다. (1이면 한칸씩 이동)   풀링(pulling) :  이미지, 피쳐맵의 사이즈를 줄임. 내부의 정보 등을 줄이기 때문에 오버피팅 효과 및 메모리 절약       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/CNN/",
        "teaser": null
      },{
        "title": "도커(docker)로 리눅스환경을 만들고 py파일을 실행해보자.",
        "excerpt":"도커가 뭐?  컨테이너를 사용하게 해주는 플랫폼이며, 이 컨테이너는 VM(Virtual Machine) 과 가장 비교를 많이한다.  OS 먼저 정확히 말하면, 컴퓨터를 사용자가 편하게 쓸 수 있도록 하는 소프트웨어이며, 안드로이드, iOS, 윈도우, macOS 등이 있다.  VM은 사용자 OS의 각종 소스들을 각각 전용으로 할당하여 가상OS를 만드는 방식이라면,  컨테이너는 docker라는 툴을 이용하여, 그 위에서 이미지(어플리케이션)을 실행하는 방식이다. (소스를 할당하는 개념이아님, 각각 제한은 할 수 있음)      목적  다른 OS에서도 작동하는 서비스를 이미지로 만들고 배포하게 한다.       장점  VM보다 가볍고 빠르다.  배포가 용이하다.  적은 용량의 이미지로 관리 가능하다.    도커파일(Dockerfile)  각각 독립으로 작동하는 파일을 이미지(image)라고 하는데, 이 이미지를 어떻게 빌드할 것인지 적어놓는 문서. (이름이 무조건 Dockerfile)          간단 명령어   FROM : 베이스 이미지  RUN : 컨테이너 생성 전 명령어를 실행한다.  COPY a b: a 파일을 함께 컨테이너 b경로에 이미지로 빌드한다.  (..은 dockerfile과 같은경로내 전부)  ENV : 환경변수 설정 ENTRYPOINT : 컨테이너를 실행하면서 수행하는 명령\u001e  CMD : ENTRYPOINT처럼 실행하면서 수행하는 명령인데 docker run 수행시(이미지 실행) 인자값을 추가해서 다른값을 넣을 수 있다.  WORKDIR : 작업 디렉토리 설정       FROM ubuntu:18.04  RUN apt update -y RUN apt install python3.6 -y RUN apt install python3-pip -y  RUN apt update -y  # requirements 있으면 아래꺼 실행 # COPY ./ubuntu1804/requirements.txt /requirements.txt # RUN pip3 install -r /requirements.txt # RUN apt update -y # 같은 폴더내 파일 복사 COPY . .  # test.py 파이썬으로 실행  RUN apt update -y   # 각종 한글 폰트 설치 RUN apt install language-pack-ko -y RUN apt install fonts-nanum -y RUN apt install fonts-nanum-coding -y RUN apt install fonts-noto-cjk -y  # 환경변수 지정 ENV LANG ko_KR.UTF-8 ENV LANGUAGE ko_KR.UTF-8 ENV LC_ALL ko_KR.UTF-8  # 컨테이너를 실행할 때 수행하는 명령어 CMD [\"test.py\"]  ENTRYPOINT [\"python3\"] WORKDIR /root RUN echo 'alias python=python3.6' &gt;&gt; .bashrc  WORKDIR /      위 내용의 Dockerfile 문서와 함께 있는 test.py가 실행됨과 동시에 리눅스 환경 컨테이너가 생성된다.        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["de"],
        "tags": [],
        "url": "http://localhost:4000/de/docker/",
        "teaser": null
      },{
        "title": "비행기 푯값 예측 MLP(Multi Layer Perceptron)",
        "excerpt":"판다스 프로파일링 코랩에서 작동하도록 버전지정 설치   !pip install pandas==1.2.1 !pip install pandas_profiling==2.8.0   Collecting pandas==1.2.1   Downloading pandas-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB) \u001b[K     |████████████████████████████████| 9.9 MB 9.8 MB/s \u001b[?25hRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.1) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.1) (2.8.2) Requirement already satisfied: numpy&gt;=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.1) (1.21.5) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas==1.2.1) (1.15.0) Installing collected packages: pandas   Attempting uninstall: pandas     Found existing installation: pandas 1.3.5     Uninstalling pandas-1.3.5:       Successfully uninstalled pandas-1.3.5 \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\u001b[0m Successfully installed pandas-1.2.1     Requirement already satisfied: pandas_profiling==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (2.11.3) Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (0.5.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (1.21.5) Requirement already satisfied: htmlmin&gt;=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (0.1.12) Requirement already satisfied: visions[type_image_path]==0.4.4 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (0.4.4) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (1.0.1) Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (1.2.1) Requirement already satisfied: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (1.7.3) Requirement already satisfied: phik&gt;=0.9.10 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (0.12.2) Requirement already satisfied: tangled-up-in-unicode&gt;=0.0.6 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (0.1.0) Requirement already satisfied: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (3.2.2) Requirement already satisfied: requests&gt;=2.23.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (2.27.1) Requirement already satisfied: tqdm&gt;=4.43.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (4.63.0) Requirement already satisfied: confuse&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (1.7.0) Requirement already satisfied: astropy&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (4.3.1) Requirement already satisfied: ipywidgets&gt;=7.5.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling==2.8.0) (7.7.0) Requirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.4-&gt;pandas_profiling==2.8.0) (21.4.0) Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.4-&gt;pandas_profiling==2.8.0) (2.6.3) Requirement already satisfied: imagehash in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.4-&gt;pandas_profiling==2.8.0) (4.2.1) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.4.4-&gt;pandas_profiling==2.8.0) (7.1.2) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from astropy&gt;=4.0-&gt;pandas_profiling==2.8.0) (4.11.3) Requirement already satisfied: pyerfa&gt;=1.7.3 in /usr/local/lib/python3.7/dist-packages (from astropy&gt;=4.0-&gt;pandas_profiling==2.8.0) (2.0.0.1) Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from confuse&gt;=1.0.0-&gt;pandas_profiling==2.8.0) (6.0) Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.5.0) Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (3.6.0) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.2.0) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.10.1) Requirement already satisfied: traitlets&gt;=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.1.1) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.2.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (1.1.0) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.3.5) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.4.2) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (2.6.1) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (1.0.18) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.8.1) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (57.4.0) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.7.5) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.8.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas_profiling==2.8.0) (2.0.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling==2.8.0) (1.4.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling==2.8.0) (3.0.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling==2.8.0) (0.11.0) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling==2.8.0) (2.8.2) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&gt;=3.2.0-&gt;pandas_profiling==2.8.0) (3.10.0.2) Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from missingno&gt;=0.4.2-&gt;pandas_profiling==2.8.0) (0.11.2) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.3.3) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.9.2) Requirement already satisfied: importlib-resources&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.4.0) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.18.1) Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.4.0-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (3.7.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;pandas_profiling==2.8.0) (2018.9) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.2.5) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (1.15.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.23.0-&gt;pandas_profiling==2.8.0) (2021.10.8) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.23.0-&gt;pandas_profiling==2.8.0) (2.10) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.23.0-&gt;pandas_profiling==2.8.0) (1.24.3) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.23.0-&gt;pandas_profiling==2.8.0) (2.0.12) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.3.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.13.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (5.6.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (1.8.0) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.7.0) Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.4.4-&gt;pandas_profiling==2.8.0) (1.3.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (1.5.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.7.1) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.4) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (4.1.0) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.6.0) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.8.4) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (21.3) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets&gt;=7.5.1-&gt;pandas_profiling==2.8.0) (0.5.1)   데이터 불러오기   # 데이터 불러오기 import pandas as pd from pandas_profiling import ProfileReport import datetime CSV_PATH_1 = '/content/drive/MyDrive/dataset/airplane/Data_Train.csv' CSV_PATH_2 = '/content/drive/MyDrive/dataset/airplane/Test_set.csv' CSV_PATH_3 = '/content/drive/MyDrive/dataset/airplane/Sample_submission.csv' target = 'Price'   데이터 엔지니어   import re  # feature 수 10개. df_train = pd.read_csv(CSV_PATH_1) X_train = df_train.drop(target, axis=1) y_train = df_train[target] X_test = pd.read_csv(CSV_PATH_2) y_test = pd.read_csv(CSV_PATH_3) \"\"\" 전처리 1. Airline : one-hot encoding 2. Date_of_Journey : 삭제 3. Source : one-hot encoding 4. Destination : one-hot encoding 5. Route : 삭제하자 6. Dep_Time : h*60 + min 으로 치환 7. Arrival_Time : 위와 동 8. Duration : 위와동 9. Total_Stops : 숫자만 남기자 non-stop은 0 10. Additional_Info : one-hot encoding \"\"\"  # Route  Date_of_journey 삭제 X_train=X_train.drop(['Date_of_Journey'], axis=1) X_test=X_test.drop(['Date_of_Journey'], axis=1) X_train=X_train.drop(['Route'], axis=1) X_test=X_test.drop(['Route'], axis=1)  # 탑승, 도착 시간계산 def hour_cal(a):     hour = int( re.findall(r'(\\d+):', a)[0] )     minute = int( re.findall(r':(\\d+)', a)[0] )     return hour*60+minute X_train['Dep_Time']=X_train['Dep_Time'].apply(hour_cal) X_test['Dep_Time']=X_test['Dep_Time'].apply(hour_cal) X_train['Arrival_Time']=X_train['Arrival_Time'].apply(hour_cal) X_test['Arrival_Time']=X_test['Arrival_Time'].apply(hour_cal)  # 소요시간 계산 def hour_cal2(a):     try:         hour = int( re.findall(r'(\\d+)h', a)[0] )     except:         hour=0     try:         minute = int( re.findall(r'(\\d+)m', a)[0] )     except:         minute = 0     return hour*60+minute X_train['Duration']=X_train['Duration'].apply(hour_cal2) X_test['Duration']=X_test['Duration'].apply(hour_cal2)  # stop에서 숫자만 남기자 X_train['Total_Stops']=X_train['Total_Stops'].apply(lambda x: 0 if str(x)[0]=='n' else int(str(x)[0])) X_test['Total_Stops']=X_train['Total_Stops'].apply(lambda x: 0 if str(x)[0]=='n' else int(str(x)[0])) print(X_train.info())  X_train.head()   &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10683 entries, 0 to 10682 Data columns (total 8 columns):  #   Column           Non-Null Count  Dtype ---  ------           --------------  -----  0   Airline          10683 non-null  object  1   Source           10683 non-null  object  2   Destination      10683 non-null  object  3   Dep_Time         10683 non-null  int64  4   Arrival_Time     10683 non-null  int64  5   Duration         10683 non-null  int64  6   Total_Stops      10683 non-null  int64  7   Additional_Info  10683 non-null  object dtypes: int64(4), object(4) memory usage: 667.8+ KB None                                       Airline       Source       Destination       Dep_Time       Arrival_Time       Duration       Total_Stops       Additional_Info                       0       IndiGo       Banglore       New Delhi       1340       70       170       0       No info                 1       Air India       Kolkata       Banglore       350       795       445       2       No info                 2       Jet Airways       Delhi       Cochin       565       265       1140       2       No info                 3       IndiGo       Kolkata       Banglore       1085       1410       325       1       No info                 4       IndiGo       Banglore       New Delhi       1010       1295       285       1       No info                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             df_train = X_train df_train['price'] = y_train df_train.profile_report()   Output hidden; open in https://colab.research.google.com to view.   인코딩 (one hot encoding 사용)   # 원핫 인코딩  #X_train=pd.get_dummies(data = X_train, columns = ['Airline', 'Source', 'Destination', 'Total_Stops',  'Additional_Info']) #X_test=pd.get_dummies(data = X_test, columns = ['Airline', 'Source', 'Destination', 'Total_Stops',  'Additional_Info'])   temp = pd.concat([X_train, X_test]) temp = pd.get_dummies(data = temp, columns = ['Airline', 'Source', 'Destination', 'Total_Stops',  'Additional_Info']) X_train_encoded = temp[0:len(X_train)] X_test_encoded = temp[len(X_train):]  print(X_train_encoded.info()) print(X_test_encoded.info())   &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 10683 entries, 0 to 10682 Data columns (total 42 columns):  #   Column                                        Non-Null Count  Dtype   ---  ------                                        --------------  -----    0   Dep_Time                                      10683 non-null  int64    1   Arrival_Time                                  10683 non-null  int64    2   Duration                                      10683 non-null  int64    3   price                                         10683 non-null  float64  4   Airline_Air Asia                              10683 non-null  uint8    5   Airline_Air India                             10683 non-null  uint8    6   Airline_GoAir                                 10683 non-null  uint8    7   Airline_IndiGo                                10683 non-null  uint8    8   Airline_Jet Airways                           10683 non-null  uint8    9   Airline_Jet Airways Business                  10683 non-null  uint8    10  Airline_Multiple carriers                     10683 non-null  uint8    11  Airline_Multiple carriers Premium economy     10683 non-null  uint8    12  Airline_SpiceJet                              10683 non-null  uint8    13  Airline_Trujet                                10683 non-null  uint8    14  Airline_Vistara                               10683 non-null  uint8    15  Airline_Vistara Premium economy               10683 non-null  uint8    16  Source_Banglore                               10683 non-null  uint8    17  Source_Chennai                                10683 non-null  uint8    18  Source_Delhi                                  10683 non-null  uint8    19  Source_Kolkata                                10683 non-null  uint8    20  Source_Mumbai                                 10683 non-null  uint8    21  Destination_Banglore                          10683 non-null  uint8    22  Destination_Cochin                            10683 non-null  uint8    23  Destination_Delhi                             10683 non-null  uint8    24  Destination_Hyderabad                         10683 non-null  uint8    25  Destination_Kolkata                           10683 non-null  uint8    26  Destination_New Delhi                         10683 non-null  uint8    27  Total_Stops_0                                 10683 non-null  uint8    28  Total_Stops_1                                 10683 non-null  uint8    29  Total_Stops_2                                 10683 non-null  uint8    30  Total_Stops_3                                 10683 non-null  uint8    31  Total_Stops_4                                 10683 non-null  uint8    32  Additional_Info_1 Long layover                10683 non-null  uint8    33  Additional_Info_1 Short layover               10683 non-null  uint8    34  Additional_Info_2 Long layover                10683 non-null  uint8    35  Additional_Info_Business class                10683 non-null  uint8    36  Additional_Info_Change airports               10683 non-null  uint8    37  Additional_Info_In-flight meal not included   10683 non-null  uint8    38  Additional_Info_No Info                       10683 non-null  uint8    39  Additional_Info_No check-in baggage included  10683 non-null  uint8    40  Additional_Info_No info                       10683 non-null  uint8    41  Additional_Info_Red-eye flight                10683 non-null  uint8   dtypes: float64(1), int64(3), uint8(38) memory usage: 813.7 KB None &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 2671 entries, 0 to 2670 Data columns (total 42 columns):  #   Column                                        Non-Null Count  Dtype   ---  ------                                        --------------  -----    0   Dep_Time                                      2671 non-null   int64    1   Arrival_Time                                  2671 non-null   int64    2   Duration                                      2671 non-null   int64    3   price                                         0 non-null      float64  4   Airline_Air Asia                              2671 non-null   uint8    5   Airline_Air India                             2671 non-null   uint8    6   Airline_GoAir                                 2671 non-null   uint8    7   Airline_IndiGo                                2671 non-null   uint8    8   Airline_Jet Airways                           2671 non-null   uint8    9   Airline_Jet Airways Business                  2671 non-null   uint8    10  Airline_Multiple carriers                     2671 non-null   uint8    11  Airline_Multiple carriers Premium economy     2671 non-null   uint8    12  Airline_SpiceJet                              2671 non-null   uint8    13  Airline_Trujet                                2671 non-null   uint8    14  Airline_Vistara                               2671 non-null   uint8    15  Airline_Vistara Premium economy               2671 non-null   uint8    16  Source_Banglore                               2671 non-null   uint8    17  Source_Chennai                                2671 non-null   uint8    18  Source_Delhi                                  2671 non-null   uint8    19  Source_Kolkata                                2671 non-null   uint8    20  Source_Mumbai                                 2671 non-null   uint8    21  Destination_Banglore                          2671 non-null   uint8    22  Destination_Cochin                            2671 non-null   uint8    23  Destination_Delhi                             2671 non-null   uint8    24  Destination_Hyderabad                         2671 non-null   uint8    25  Destination_Kolkata                           2671 non-null   uint8    26  Destination_New Delhi                         2671 non-null   uint8    27  Total_Stops_0                                 2671 non-null   uint8    28  Total_Stops_1                                 2671 non-null   uint8    29  Total_Stops_2                                 2671 non-null   uint8    30  Total_Stops_3                                 2671 non-null   uint8    31  Total_Stops_4                                 2671 non-null   uint8    32  Additional_Info_1 Long layover                2671 non-null   uint8    33  Additional_Info_1 Short layover               2671 non-null   uint8    34  Additional_Info_2 Long layover                2671 non-null   uint8    35  Additional_Info_Business class                2671 non-null   uint8    36  Additional_Info_Change airports               2671 non-null   uint8    37  Additional_Info_In-flight meal not included   2671 non-null   uint8    38  Additional_Info_No Info                       2671 non-null   uint8    39  Additional_Info_No check-in baggage included  2671 non-null   uint8    40  Additional_Info_No info                       2671 non-null   uint8    41  Additional_Info_Red-eye flight                2671 non-null   uint8   dtypes: float64(1), int64(3), uint8(38) memory usage: 203.5 KB None   특성 선택 (SelectKBest사용)   from sklearn.feature_selection import chi2, SelectKBest selector1 = SelectKBest(chi2, k=5) X_train1 = selector1.fit_transform(X_train_encoded, y_train) columns = X_train_encoded.columns X_train_encoded = X_train_encoded[columns[selector1.get_support()]] X_test_encoded = X_test_encoded[columns[selector1.get_support()]]   ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)  &lt;ipython-input-33-6ef92159fc82&gt; in &lt;module&gt;()       1 from sklearn.feature_selection import chi2, SelectKBest       2 selector1 = SelectKBest(chi2, k=5) ----&gt; 3 X_train1 = selector1.fit_transform(X_train_encoded, y_train)       4 columns = X_train_encoded.columns       5 X_train_encoded = X_train_encoded[columns[selector1.get_support()]]   /usr/local/lib/python3.7/dist-packages/sklearn/base.py in fit_transform(self, X, y, **fit_params)     853         else:     854             # fit method of arity 2 (supervised transformation) --&gt; 855             return self.fit(X, y, **fit_params).transform(X)     856     857   /usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py in fit(self, X, y)     405             )     406 --&gt; 407         self._check_params(X, y)     408         score_func_ret = self.score_func(X, y)     409         if isinstance(score_func_ret, (list, tuple)):   /usr/local/lib/python3.7/dist-packages/sklearn/feature_selection/_univariate_selection.py in _check_params(self, X, y)     604             raise ValueError(     605                 \"k should be &gt;=0, &lt;= n_features = %d; got %r. \" --&gt; 606                 \"Use k='all' to return all features.\" % (X.shape[1], self.k)     607             )     608   ValueError: k should be &gt;=0, &lt;= n_features = 4; got 5. Use k='all' to return all features.   스케일링 및 모델 구성, fit   from sklearn.preprocessing import StandardScaler # StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train_encoded) X_train_scaled=pd.DataFrame(data=X_train_scaled)  X_test_scaled = scaler.fit_transform(X_test_encoded) X_test_scaled=pd.DataFrame(data=X_test_scaled)   # modeling MLP from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout from tensorflow.keras.optimizers import Adam, SGD from tensorflow.keras import regularizers  model = Sequential() model.add(Dense(128, activation='relu', input_dim=4, kernel_initializer='he_normal')) model.add(Dense(128, activation='tanh', kernel_initializer='he_normal',                 kernel_regularizer=regularizers.l1(0),                    activity_regularizer=regularizers.l2(0))          ) Dropout(0.3) model.add(Dense(128, activation='sigmoid', kernel_initializer='he_normal')) Dropout(0.5) model.add(Dense(1, activation='linear'))  model.compile(optimizer='Adam', loss='mse', metrics=['mae'])  # model fit history = model.fit(X_train_scaled, y_train, batch_size = 256, validation_split=0.2, epochs=4000, verbose=0)   loss 변동 그래프로 확인   import matplotlib.pyplot as plt val_loss = history.history['val_loss'] val_mae = history.history['val_mae'] epochs = range(1, len(val_loss) + 1)   plt.plot(epochs, val_loss, 'b', label='val loss')  plt.show()         ..? 너무 모양 완벽한 그래프를 갖는다.. 왜지    어떤 예측값을 갖지…?   model.predict(X_test_encoded[0:10])   array([[9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562],        [9079.562]], dtype=float32)   # mae가 최소로하는 하나의 값으로 수렴해버림.. # 딥러닝까지 필요없는 데이터에 딥러닝을 써버리면.. # 복잡도만 올라가고 성능은 나오지 않는 것 같다.   소잡는칼…!       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["MLP"],
        "url": "http://localhost:4000/pj/airplane_price/",
        "teaser": null
      },{
        "title": "wiki데이터를 이용한 이미지 캡셔닝",
        "excerpt":"mac gpu 사용가능확인 (1이면 사용가능)   import tensorflow as tf   print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))   Num GPUs Available:  0   from google.colab import drive drive.mount('/content/drive')   Mounted at /content/drive   import   # tensorflow.compat.v2 를 사용 import tensorflow.compat.v2 as tf import pandas as pd from matplotlib import pyplot as plt import re import cv2 import time   dataset     각 train 데이터 ( train_1 ~ train_10) 는 small_data.ipynb 에서 따로 처리하여 만들어짐.  영어만 추출, image_url 과 caption_feature 만 추출.    출처 : https://www.kaggle.com/c/wikipedia-image-caption/code?competitionId=29705&amp;searchQuery=tensor   # caption_feature를 보면 # 1. [SEP] 으로 나뉘어져 있는데, 그 뒤에 있는 내용에 대해 할 것. (BERT)에서 사용하는 스페셜 토큰인데 파인튜닝은 안할 것이므로.. # 2. 숫자정보는 제외하자 대부분 특수한 경우에 쓰이거나 주소 등이다.  df = pd.read_csv('/content/drive/MyDrive/dataset/wiki/train_1.tsv', delimiter = '\\t') p = re.compile('\\[SEP\\].+') # \\로 감싸진 곳은 df['caption_title_and_reference_description'] = df['caption_title_and_reference_description'].apply(                                 lambda x: '&lt;start&gt; ' +                                         re.sub('\\d+', '', p.search(x).group().replace('[SEP] ', '')).lower()                                         +' &lt;end&gt;'                                         if p.search(x).group() not in['[SEP] ', ''] else None) df=df.dropna(axis=0) print(df.shape) df.head()   (301484, 2)                                       image_url       caption_title_and_reference_description                       0       https://upload.wikimedia.org/wikipedia/commons...       &lt;start&gt; downtown deer park &lt;end&gt;                 1       https://upload.wikimedia.org/wikipedia/commons...       &lt;start&gt; jürgen ovens's justitia, -, museumsber...                 2       https://upload.wikimedia.org/wikipedia/commons...       &lt;start&gt;  mv agusta  raid &lt;end&gt;                 4       https://upload.wikimedia.org/wikipedia/commons...       &lt;start&gt; seth macfarlane's logo &lt;end&gt;                 6       https://upload.wikimedia.org/wikipedia/commons...       &lt;start&gt; erskine river at lorne &lt;end&gt;                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             import cv2 import numpy as np from urllib import request  def url_to_image(url):     '''     url 에서 이미지를 추출하여, (512,512,3) 의 rgb ndarray로 리턴     '''     resp = request.urlopen(url)     image = np.asarray(bytearray(resp.read()), dtype='uint8')     image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0     return image     #return cv2.resize(image, (512,512))   # 가장 긴 캡션과 그의 이미지 출력해보기 (모델상에서는 실행안해도됨.) from google.colab.patches import cv2_imshow long_caption = max(df['caption_title_and_reference_description'], key = lambda x: len(x)) a = df[df['caption_title_and_reference_description'] == long_caption ]['image_url'] #a = df[df['caption_title_and_reference_description'] == 'Downtown Deer Park']['image_url'] x = url_to_image(str(a.iloc[0])) print(long_caption) cv2_imshow(x) print(x.shape) print(len(long_caption))   &lt;start&gt; figure : genomic context scheme of smrc and its closest homologues in other organisms. the αr rna genes are represented by red arrows and the flanking orfs by arrows on different colors depending on their product function (legend). numbers indicate the αr rna gene's and flanking orfs coordinates in each organism genome database. the gene strand is represented with the file direction. on the left of the figure identification names are used which correspond to a certain organism: αr_smrc = sinorhizobium meliloti  (nc_), αr_smedrc = sinorhizobium medicae wsm chromosome (nc_), αr_sfrc = sinorhizobium fredii ngr chromosome (nc_), αr_atrc = agrobacterium tumefaciens str. c chromosome linear (nc_), αr_reciatrc = rhizobium etli ciat  (nc_), αr_arrc = agrobacterium radiobacter k chromosome  (nc_), αr_rltrc = rhizobium leguminosarum bv. trifolii wsm (nc_), αr_avrc = agrobacterium vitis s chromosome  (nc_), αr_rlvrc = rhizobium leguminosarum bv. viciae  (nc_), αr_rltrc = rhizobium leguminosarum bv. trifolii wsm (nc_), αr_recfnrc = rhizobium etli cfn  (nc_), αr_mlrc = mesorhizobium loti maff chromosome (nc_), αr_mcrc = mesorhizobium ciceri biovar biserrulae wsm chromosome (nc_), αr_bcrcii = brucella canis atcc  chromosome ii (nc_), αr_bsrcii = brucella suis atcc  chromosome ii (nc_), αr_bmmrcii = brucella melitensis bv.  str. m chromosome ii (nc_), αr_basrcii = brucella abortus s chromosome  (nc_), αr_bmrcii = brucella melitensis atcc  chromosome ii (nc_), αr_bsrcii = brucella suis  chromosome ii (nc_), αr_barcii = brucella abortus bv.  str. - chromosome ii (nc_), αr_bmarcii = brucella melitensis biovar abortus  chromosome ii (nc_), αr_borcii = brucella ovis atcc  chromosome ii (nc_), αr_bmircii = brucella microti ccm  chromosome  (nc_), αr_oarc = ochrobactrum anthropi atcc  chromosome  (nc_), αr_msbncrc = mesorhizobium sp. bnc (nc_), αr_bahrc = bartonella henselae str. houston- (nc_), αr_bacrc = bartonella clarridgeiae  (nc_), αr_batrc = bartonella tribocorum cip  (nc_), αr_baqrc = bartonella quintana str. toulouse (nc_), αr_babrc = bartonella bacilliformis kc (nc_), αr_bagrc = bartonella grahamii asaup (nc_), αr_acrc = azorhizobium caulinodans ors  (nc_), αr_stnrc = starkeya novella dsm  chromosome (nc_), αr_xarc = xanthobacter autotrophicus py chromosome (nc_), αr_mesrc = methylocella silvestris bl chromosome (nc_), αr_beirc = beijerinckia indica subsp. indica atcc  chromosome (nc_), αr_rhprc = rhodopseudomonas palustris bisa chromosome (nc_). &lt;end&gt;      (7992, 1080, 3) 2492   전처리     위에서 확인했듯, caption 이 제일 긴 행에 대해         이미지가 너무 크다(7992,1080). &gt;  512,512 로 압축하게 되면 심각하게 찌그러 질것,     caption이 너무 길다. 3011자.         해결방법         caption이 너무 긴 행(100자 이상)은 삭제.     image 파일이 너무 큰(가로 세로 비율이 2:1 혹은 1:2 를 초과) 경우는 삭제      # 위 url_to_image 다시 정의 def url_to_image(url):     '''     url 에서 이미지를 추출하여, (512,512,3) 의 rgb ndarray로 리턴     '''     resp = request.urlopen(url)     image = np.asarray(bytearray(resp.read()), dtype='uint8')     image = cv2.imdecode(image, cv2.IMREAD_COLOR)#/255.0     if type(image) == type(None):         return np.array([None])     image = image/255.0     if ( image.shape[0]/image.shape[1] &gt; 2 ) or ( image.shape[0]/image.shape[1] &lt; 1/2 ):  # 가로세로 비율이 1:2, 2:1을 벗어난다면         return np.array([None])     else:         return cv2.resize(image, (299,299))   X_pre_train=[] y_train=[] for i,j in enumerate((df.iloc[1500:2000]['image_url'])):     try:         temp = url_to_image(j)     except:         pass     if None in temp:         pass     else:         try:           X_pre_train.append(temp)           y_train.append( df.iloc[i]['caption_title_and_reference_description'] )         except:           pass print(len(X_pre_train)) print(X_pre_train[2].shape) print(len(y_train)) print(y_train[2])  # png 파일일 경우 libpng경고가 나오는데 무시해도 좋을듯하다. # srv 파일의 경우   388 (299, 299, 3) 388 &lt;start&gt;  mv agusta  raid &lt;end&gt;   image_model = tf.keras.applications.InceptionV3(include_top=False,                                                 weights='imagenet') new_input = image_model.input hidden_layer = image_model.layers[-1].output  image_features_extract_model = tf.keras.Model(new_input, hidden_layer)   Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 87916544/87910968 [==============================] - 1s 0us/step 87924736/87910968 [==============================] - 1s 0us/step   # imagenet 가중치를 사용하여 특성추출 BATCH_SIZE = 64 image_dataset = tf.data.Dataset.from_tensor_slices(X_pre_train) image_dataset = image_dataset.batch(BATCH_SIZE) np_batch_features = np.array([np.zeros((64,2048))])   # 여기서 64는 batch_size가 아니라 InceptionV3의 마지막 레이어 아웃풋 모양 for img in image_dataset:     batch_features = image_features_extract_model(img)     batch_features = tf.reshape(batch_features,                               (batch_features.shape[0], -1, batch_features.shape[3]))     for temp_img in batch_features:       np_batch_features = np.append(np_batch_features, np.array(temp_img).reshape(1,64,2048), axis=0)  np_batch_features = np_batch_features[1:]   np_batch_features.shape   (388, 64, 2048)   # y_train 은 캡션 문장인데, 토크나이저를 통해 문장들을 단어별 벡터화 해준다. (cap_vector)  top_k = 5000 tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,                                                   oov_token=\"&lt;unk&gt;\",                                                   filters='!\"#$%&amp;*+.-;?@[]^`{}~ ') tokenizer.fit_on_texts(y_train) train_seqs = tokenizer.texts_to_sequences(y_train) cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post') print(y_train[3]) print(len(cap_vector)) cap_vector[3]   &lt;start&gt; erskine river at lorne &lt;end&gt; 388      array([  2, 304,  31,   9, 305,   3,   0,   0,   0,   0,   0,   0,   0,          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,          0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)   #size_test = int( 0.01 * len(cap_vector)) size_test = 1       # test 는 딱 마지막에 확인용으로 사용하자. X_test = np.array(np_batch_features)[0:size_test] X_train = np.array(np_batch_features)[size_test:] y_test = cap_vector[0:size_test] y_train = cap_vector[size_test:] len(X_test), len(y_test), len(X_train), len(y_train)   (1, 1, 387, 387)   X_test.shape   (1, 64, 2048)   전역변수   BUFFER_SIZE = 1000 embedding_dim = 512 units = 512 vocab_size = top_k + 1 num_steps = len(X_train) // BATCH_SIZE features_shape = 2048 attention_features_shape = 64   데이터셋 지정   dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)) dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)   optimizer, loss func..   optimizer = tf.keras.optimizers.Adam() loss_object = tf.keras.losses.SparseCategoricalCrossentropy(     from_logits = True, reduction='none') def loss_function(real, pred):     mask = tf.math.logical_not(tf.math.equal(real, 0))     loss_ = loss_object(real, pred)      mask = tf.cast(mask, dtype = loss_.dtype)     loss_ *= mask      return tf.reduce_mean(loss_)   model 정의   class BahdanauAttention(tf.keras.Model):   def __init__(self, units):     '''     W1, W2, V 는 학습가능한 가중치벡터     '''     super(BahdanauAttention, self).__init__()   # 부모클래스 (tf.keras.Model.init()사용)     self.W1 = tf.keras.layers.Dense(units)      # units(전역) 수 의 node를 갖는 W1 가중치     self.W2 = tf.keras.layers.Dense(units)      # 위와 동 w2 가중치     self.V = tf.keras.layers.Dense(1)           # V가중치는 하나로.    def call(self, features, hidden):     '''     feautures : 이미지의 피쳐맵     hidden : hidden state     '''     hidden_with_time_axis = tf.expand_dims(hidden, 1)     attention_hidden_layer = (tf.nn.tanh(self.W1(features) +                                          self.W2(hidden_with_time_axis)))     score = self.V(attention_hidden_layer)     attention_weights = tf.nn.softmax(score, axis=1)     context_vector = attention_weights * features     context_vector = tf.reduce_sum(context_vector, axis=1)     return context_vector, attention_weights   class CNN_Encoder(tf.keras.Model):     def __init__(self, embedding_dim):         super(CNN_Encoder, self).__init__()         self.fc = tf.keras.layers.Dense(embedding_dim)     def call(self, x):         x = self.fc(x)         x = tf.nn.relu(x)         return x   class RNN_Decoder(tf.keras.Model):   def __init__(self, embedding_dim, units, vocab_size):     super(RNN_Decoder, self).__init__()     self.units = units      self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)     self.gru = tf.keras.layers.GRU(self.units,                                    return_sequences=True,                                    return_state=True,                                    recurrent_initializer='glorot_uniform')     self.fc1 = tf.keras.layers.Dense(self.units)     self.fc2 = tf.keras.layers.Dense(vocab_size)      self.attention = BahdanauAttention(self.units)    def call(self, x, features, hidden):     context_vector, attention_weights = self.attention(features, hidden)     x = self.embedding(x)     x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)     output, state = self.gru(x)     x = self.fc1(output)     x = tf.reshape(x, (-1, x.shape[2]))     x = self.fc2(x)     return x, state, attention_weights    def reset_state(self, batch_size):     return tf.zeros((batch_size, self.units))   trian step 정의   encoder = CNN_Encoder(embedding_dim) decoder = RNN_Decoder(embedding_dim, units, vocab_size)   checkpoint_path = \"./drive/MyDrive/dataset/wiki/checkpoints/train\" ckpt = tf.train.Checkpoint(encoder=encoder,                            decoder=decoder,                            optimizer=optimizer) ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)   start_epoch = 0 if ckpt_manager.latest_checkpoint:   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])   # restoring the latest checkpoint in checkpoint_path   ckpt.restore(ckpt_manager.latest_checkpoint)   # adding this in a separate cell because if you run the training cell # many times, the loss_plot array will be reset loss_plot = []   @tf.function def train_step(img_tensor, target):   loss = 0    # initializing the hidden state for each batch   # because the captions are not related from image to image   hidden = decoder.reset_state(batch_size=target.shape[0])    dec_input = tf.expand_dims([tokenizer.word_index['&lt;start&gt;']] * target.shape[0], 1)    with tf.GradientTape() as tape:       features = encoder(img_tensor)        for i in range(1, target.shape[1]):           # passing the features through the decoder           predictions, hidden, _ = decoder(dec_input, features, hidden)            loss += loss_function(target[:, i], predictions)            # using teacher forcing           dec_input = tf.expand_dims(target[:, i], 1)    total_loss = (loss / int(target.shape[1]))    trainable_variables = encoder.trainable_variables + decoder.trainable_variables    gradients = tape.gradient(loss, trainable_variables)    optimizer.apply_gradients(zip(gradients, trainable_variables))    return loss, total_loss   EPOCHS = 20  for epoch in range(start_epoch, EPOCHS):     start = time.time()     total_loss = 0      for (batch, (img_tensor, target)) in enumerate(dataset):         batch_loss, t_loss = train_step(img_tensor, target)         total_loss += t_loss          if batch % 100 == 0:             average_batch_loss = batch_loss.numpy()/int(target.shape[1])             print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')     # storing the epoch end loss value to plot later     loss_plot.append(total_loss / num_steps)      if epoch % 5 == 0:       ckpt_manager.save()      print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')     print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')   Epoch 5 Batch 0 Loss 1.7554 Epoch 5 Loss 1.809353 Time taken for 1 epoch 157.57 sec  Epoch 6 Batch 0 Loss 1.1847 Epoch 6 Loss 1.441257 Time taken for 1 epoch 46.38 sec  Epoch 7 Batch 0 Loss 1.0604 Epoch 7 Loss 1.354600 Time taken for 1 epoch 45.60 sec  Epoch 8 Batch 0 Loss 1.2198 Epoch 8 Loss 1.334983 Time taken for 1 epoch 45.94 sec  Epoch 9 Batch 0 Loss 1.0771 Epoch 9 Loss 1.477467 Time taken for 1 epoch 46.14 sec  Epoch 10 Batch 0 Loss 1.2247 Epoch 10 Loss 1.347626 Time taken for 1 epoch 45.70 sec  Epoch 11 Batch 0 Loss 1.0726 Epoch 11 Loss 1.260275 Time taken for 1 epoch 46.71 sec  Epoch 12 Batch 0 Loss 1.2278 Epoch 12 Loss 1.200646 Time taken for 1 epoch 46.94 sec  Epoch 13 Batch 0 Loss 1.2482 Epoch 13 Loss 1.216185 Time taken for 1 epoch 46.13 sec  Epoch 14 Batch 0 Loss 1.0949 Epoch 14 Loss 1.133379 Time taken for 1 epoch 45.98 sec  Epoch 15 Batch 0 Loss 1.1599 Epoch 15 Loss 1.292254 Time taken for 1 epoch 46.25 sec  Epoch 16 Batch 0 Loss 0.9927 Epoch 16 Loss 1.213316 Time taken for 1 epoch 46.76 sec  Epoch 17 Batch 0 Loss 1.0226 Epoch 17 Loss 1.102528 Time taken for 1 epoch 46.03 sec  Epoch 18 Batch 0 Loss 0.8826 Epoch 18 Loss 1.238099 Time taken for 1 epoch 46.58 sec  Epoch 19 Batch 0 Loss 0.8167 Epoch 19 Loss 1.090817 Time taken for 1 epoch 47.73 sec  Epoch 20 Batch 0 Loss 0.8715 Epoch 20 Loss 1.050230 Time taken for 1 epoch 48.49 sec   예측 모델   def calc_max_length(tensor):     return max(len(t) for t in tensor)  max_length = max_length = calc_max_length(y_train)  def evaluate(image):     attention_plot = np.zeros((max_length, attention_features_shape))      hidden = decoder.reset_state(batch_size=1)      temp_input = tf.expand_dims(url_to_image(image), 0)     img_tensor_val = image_features_extract_model(temp_input)     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],                                                  -1,                                                  img_tensor_val.shape[3]))      features = encoder(img_tensor_val)      dec_input = tf.expand_dims([tokenizer.word_index['&lt;start&gt;']], 0)     result = []      for i in range(max_length):         predictions, hidden, attention_weights = decoder(dec_input,                                                          features,                                                          hidden)          attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()          predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()         result.append(tokenizer.index_word[predicted_id])          if tokenizer.index_word[predicted_id] == '&lt;end&gt;':             return result, attention_plot          dec_input = tf.expand_dims([predicted_id], 0)      attention_plot = attention_plot[:len(result), :]     return result, attention_plot   # captions on the validation set image_url = df['image_url'][20] real_caption = df['caption_title_and_reference_description'][20] result, attention_plot = evaluate(image_url)  cv2_imshow(url_to_image(image_url)*255) print('Real Caption:', real_caption) print('Prediction Caption:', ' '.join(result))      Real Caption: &lt;start&gt; entering a mining settlement from jamieson &lt;end&gt; Prediction Caption: a np on either may river the wyndham's &lt;end&gt;   결과 해석     광산 입구 길에 대한 이미지를 ‘wyndham의 river’ 정도의 캡션을 생성했다.   길의 색깔만 다르다면 충분히 강으로 인식할 수 있는 수준일 것. 물론 성공적인 결과라고 보긴 힘들다.   개선할 점     kaggle의 wiki학습데이터가 너무 방대하다 보니, 일부를 쪼개어 사용하기도 했고, 캡션의 경우 너무 디테일(지명이 들어간다던가 등)한 부분을 없애는 과정이 필요했다고 생각한다. (통계적 언어모델)   학습데이터의 증강이 더 필요했다고 생각한다. (색채 변경, 회전 등)   전체 구조를 좀 더 쪼개서 사용할 걸 싶다. 예를 들면 training.py 으로 가중치파일을 생성한다거나, main.py result.py 등 으로 하여 가시성, 접근성을 높이는 게 필요해보인다. (이 프로젝트는 몇달 전에 시도했던 것이기 떄문에 부족한점이 많이 보이는 듯 하다.)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["attention","RNN"],
        "url": "http://localhost:4000/pj/image_captioning/",
        "teaser": null
      },{
        "title": "클라우드 데이터베이스, SQL",
        "excerpt":"클라우드 데이터베이스  말 그대로 온라인으로 접근할 수 있는 데이터베이스를 말한다.     관계형 데이터베이스  (oracle DB, mySQL, PostgreSQL 등)   NoSQL  (MongoDB, Redis, Hbase Neo4j등)   관계형 데이터베이스 (RDB : Relational DataBase)  스키마의 형태로 데이터들의 연관성을 고려해서 데이터를 저장한다.  데이터를 관리하기 위해 SQL언어를 사용해야한다.      스키마 이미지      NoSQL  말그대로 SQL에 한정되지 않겠다는 의지를 담은 데이터베이스   형식의 자유로운 데이터들을 딕셔너리와 같이 Key와 값의 관계로 저장해서,  Key로 하여금 데이터를 불러올 수 있는 구조를 말한다.   그 형태에 따라 Key-value(key하나의 value에 여러 column값들을 가짐), document(하나의 doc은 여러 key:value 세트를 갖는 dict형), wide-column(key-value와 비슷한데 좀 더 형식이 더해져, 덜 유연한 구조), graph 등의 종류가 있다.   데이타 베이스 구조의 유연함의 의미  유연 up &lt;-&gt; 확장성 up , 무결성, 안정성, 관리용이성 down   확장성에 대해서는 수평적으로 용이한지, 수직적으로 용이한지 애매할 수 있다.   RDB의 경우 상대적으로 덜 유연하더라도 수직적 확장으로는 더 강점이 있다고 할 수 있다.   SQL 문법 이모저모   FROM WHERE GROUP BY HAVING SELECT ORDER BY   python에서 클라우드 데이터베이 사용          🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["de"],
        "tags": [],
        "url": "http://localhost:4000/de/CloudDB/",
        "teaser": null
      },{
        "title": "YOLO를 이해하기 위한 CNN 객체인식 모델",
        "excerpt":"유명 이미지 데이터  cifar10, 100 : 클래스 10, 100개의 갖는 사물 이미지(32,32) 데이터 셋 (학습용 50000개)  imagenet :  클래스 훨씬 다양(몇천, 몇만), 백만개가 넘음.   객체 인식 모델의 전반적인 생김새   &lt; 2stage와 1stage 구조 그림&gt;           2-stage detector : region proposal network를 포함한다. 이는 자주 탐지되는 부분을 학습한다는 의미. + localization과 classification을 ‘각각’ 진행한다는 의미.            single(1-stage) detector : localization과 classification을 동시에 진행.       객체 분류 (Backbone)          VGGnet : 3x3 conv만 사용하여 구조가 단순한데 비해 기능은 준수함.            Resnet : 3x3 conv를 포함하며 conv를 무시 후 지나가는 shortcut을 갖는 residual block 으로 구성됨. (Resnet152에서는 1x1 conv를 포함하는 residual block을 사용함) 기울기 손실 방지를 통해 깊이를 깊게 쌓을 수 있게 됨.             Dense net : Resnet같이 relu가 아닌, concat으로 이전 정보를 가져간다는 차이가 있음.         CSP-Darknet : Densenet을 기초로 CSP를 적용시킴   CSP 방법 : 분리된 feature map을 다시 병합해주다?   Neck             additional blocks : 다양한 크기의 피쳐맵들을 만들어 정보를 수집\u001c SPP : 아하  (사진 예시(좌) : SPP, 모든 피쳐맵들을 몇가지 경우의 grid로 나누어 flatten한 후 평균을 냄 )            path-aggregation blocks : bottom-up path 와 top-down path에서 정보를 수집  (사진 예시(우) : PAN, 기존 FPN 네트워크에서 빨간선, 초록선과 같이 low-level-feature를 high-level-feature에 전달하\b여 성능을 향상시킴.)       Head     영역 제한 faster R-CNN  자주 생성되는 위치를 학습한다. region proposal net   사이즈 제한 YOLO    정해진 그리드로 나누어 CNN으로 처리한다.   미완, 참고중 :  https://herbwood.tistory.com/24  https://deep-learning-study.tistory.com/528  https://ganghee-lee.tistory.com/34       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/YOLO/",
        "teaser": null
      },{
        "title": "1x1 conv, 그리고 conv2d conv3d의 이해",
        "excerpt":"1by1 conv는 어떻게 생긴거냐  처음엔 1by1 사이즈의 필터가 CNN에서 의미가 있나 생각했다.  근데 그게 아니라 1by1byN 모양의 필터였던 것!  피쳐맵의 길이를 줄일 수 있고, 이를 포함하는 CNN구조들은 비교적 더 이후에 나온 구조이기도 하고,  연산량을 줄이고 속도를 높이는 역할을 할까?   2차원 필터는 conv2d, 3차원 필터는 conv3d  내가 이해한게 맞다면 conv2d는 피쳐맵길이는 같거나, 증가할 수 밖에 없고  conv3d는 감소, 동일, 증가 모두 가능하다.   1by1 conv의 예시  3by3 필터를 사용하는  input이 (7,7,32), output 사이즈가 (7,7,64)라고 해보자(same padding)  conv2d를 사용하면 파라미터는  (3x3)x32x64+64 = 18496.  conv3d를 사용해도 식의 의미는 다르지만 (3x3x32)x64+64 = 18496.   1x1 conv를 사용하여 (7,7,16) 으로 변환 후 (7,7,64) 로 바꾼다면  (1x1x32x16+16) + (3x3x16x64+64) = 528 + 9280 = 9808.   파라미터의 수를 상당히 줄일 수 있다   VGGnet에서 Resnet은 VGGnet 대비 기울기 소실을 막음으로써 모델을 더 깊게 쌓을 수 있었다면,  1by1 conv의 경우에는 파라미터의 수를 줄임으로써 모델을 더 깊게 쌓을 수 있을 것.   fire module   1x1 conv를 사용하는 대표적인 예시가 fire module인데,  3x3 convㄹ   미완 참고중 :  https://hoya012.github.io/blog/deeplearning-classification-guidebook-3/       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/1by1conv/",
        "teaser": null
      }]
