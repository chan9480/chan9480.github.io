var store = [{
        "title": "test게시물",
        "excerpt":"test_제목  test 게시물입니다.   test_제목 2  test2 목차 테스트 입니다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Example_sub"],
        "tags": ["Test","Test_2"],
        "url": "http://localhost:4000/example_sub/test/",
        "teaser": null
      },{
        "title": "파이썬, 자료구조",
        "excerpt":"서론     옛날에 RTOS관련 대학원에 진학해보려 했다가 받은 질문중에 하나가 스택과 큐의 차이점과 예시를 설명하라는 내용이었다.  스택은 쌓아놓은 책을 위에서 부터 다시꺼낸다면 큐는 만화책 반납통같이 위에서 넣으면 밑에서 꺼내는 개념이라고 말씀드렸다. 물론 틀린건 아니지만 지금 생각해보면 그냥 얘는 개념만 알고 실사용은 해보지 않은 친구라고 생각하셨을 것 같다. 오늘 각각을 구현해보면서 이해해보는 시간을 가졌다.    큐(queue) 를 연결리스트로 구현     연결리스트   큐        class Node():     def __init__(self, data):         self._data = data         self._next = None     node 들을 선언했다.    class Queue():     def __init__(self):         self._front = None         self._rear = None      ‘맨앞’ (꺼내는곳) 와 ‘맨뒤’ (넣는곳) 자리를 마련했다. 큐를 들여다볼수 있는 창문 두개라고 비유하여 이해했다.        def enqueue(self,item):         new_node = Node(item)         if self._front == None: # 빈 큐 라면 '맨앞' '맨뒤'에 모두 같은 노드를 넣어준당.             self._front = new_node             self._rear = self._front         else: # 뭔가 있는 큐라면 뒤의 노드에 대해서만 새로운 노드를 연결해주면 된다.             self._rear._next = new_node           #기존 '맨뒤노드'의 next에 새로운 노드를 넣어준당. (연결!)             self._rear = self._rear._next         #큐의 '맨뒤' 에 새로운 노드를 위치시킨당. (맨뒤 업데이트!)      값 item을 큐에 추가하는 함수를 정의했다. (내부에서는 item을 value로 갖는 node를 연결해준것)        def dequeue(self):         if self._front == None: # 빈큐라면 None을 리턴             return None         else : #뭔가 있는 큐라면 맨앞에서 뽑은 값 리턴 + '맨앞' 업데이트             temp = self._front._data       # '맨앞'노드의 데이터를 잠깐 빼두자.             self._front = self._front._next  #'맨앞' 노드의 next 에 있는 노드를 큐의 '맨앞'으로 둔다.         if self._front ==None:             # 이렇게 되는 경우는 1개짜리의 큐에서 dequeue를 진행한 후가 될거다.             self._rear = None           #그러면 '맨뒤'도 비워주자 (이거 안하면 '맨뒤'에 꺼낸 노드가 아직 남아있음)         return temp      빈큐라면 None을 리턴, 빈큐가 아니라면 제일 먼저 넣은 하나를 큐에서 제거하는 함수를 정의했다.        def return_queue(self):         result = []         temp = self._front      # 초기값은 '맨앞' node!         while temp!= None:      # temp에 None이 들어가버리면 다 끝난거다!             result.append(temp._data)             temp = temp._next   # temp에는 그 다음 node 넣장. (None이 들어갈수도 있음 그러면 loop끝)         return result      현재 큐를 list로 리턴하는 함수를 정의했다.    스택(stack)을 list로 구현     스택의 경우 넣는곳과 빼는곳이 같은 자료구조이다.  그런데 파이썬에서는 list의경우 list.append(x) 와 list.pop() 으로 구현이 가능하다.  (물론 큐도 가능하다. 그러나 pop(0)를 사용하여야 하는데, 그렇게 되면 리스트 끝에서부터 index 0 의 방향으로 탐색을 하기때문에, 비효율적이다.)        class Stack():     def __init__(self):         self._data = []      def push(self, item):         self._data.append(item)       def pop(self):         return self._data.pop() if self._data else None       def return_stack(self):         return self._data     간단해서 코드 리뷰는 생략했다.    데크 구현    동작은 하는데, pop에서 비효율적으로 탐색하는 부분이 있다.(top에써 꺼낼라해도 bottom부터 탐색을 해야하는 부분) 양방향 연결리스트로 수정을 해서 업데이트를 해야겠다  https://user-images.githubusercontent.com/84547813/143423521-f8877649-559d-4a49-aa82-c6f69335bd04.png    class Node:     def __init__(self, value, next=None):         self.value = value         self.next = next  class Deque:     def __init__(self):         self.top = None         self.bottom = None       def append(self, item):         if self.top == None:        # self.top이 비어있다면             self.top = Node(item)   # self.top 에도 bottom에도 뉴 노드를 넣어준다.             self.bottom = self.top         else :             node = Node(item)             self.top.next = node    # 현재 top의 next에 뉴 노드를 넣어주고             self.top = node         # 이제 top은 뉴 노드로 한다.       def appendleft(self, item):         node = Node(item)         if self.bottom == None :        # bottom이 비었다면 top, bottom에 둘다 넣어줘             self.top = node             self.bottom = node         else :             node.next = self.bottom     # 뉴 노드의 next 를 bottom으로 설정 후, 앞으로 bottom은 뉴노드다!             self.bottom = node      top 방향과 bottom방향에 값을 추가할 수 있는 함수 두개를 정의했다. ````     def pop(self):         # top 추출         if self.top == None:        # top이 None 이면 return None             return None         elif self.top == self.bottom:   # top과 bottom이 같다면(None은 아님)             result = self.top.value     # value를 리턴하고, top bottom을 비우자 (None)             self.top =None             self.bottom = None             return result         else :                          # 이제 정상적으로 길이 1이상의 데크라면             node = self.bottom          # 초기값 bottom부터 next가 top인 곳까지 node를 찾아서             result = self.top.value     # (리턴값은 어쨋든 top value)             while node !=None:          # top을 업데이트해주자.                 if node.next == self.top:                     self.top = node                 node = node.next             return result    def popleft(self):     # bottom 추출     if self.bottom == None:         return None     elif self.top == self.bottom:         self.top = None         result = self.bottom.value         self.bottom = None         return result     else :         result = self.bottom.value         self.bottom = self.bottom.next         return result ```` &gt; top에서 추출하는 pop과 bottom에서 추출하는 popleft를 정의했다. ````  def ord_desc(self):     node = self.bottom      # 초기값은 bottom 부터     result = []     while node != None :    # 노드에 None이 들어있다면 종료         result.append(node.value)         node = node.next     return result ```` &gt; 현재 데크를 리스트로 리턴하는 함수를 정의했다.  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/4th/",
        "teaser": null
      },{
        "title": "게임 데이터 분석 기초",
        "excerpt":"   목차  1. 샘플데이터 불러오기  2. 데이터 전처리  3. 필수포함 주제  4. 다음 분기에 어떤 게임을 설계해야 할까   1. 샘플 데이터 불러오기  링크 : https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv   import pandas as pd df=pd.read_csv('https://ds-lecture-data.s3.ap-northeast-2.amazonaws.com/datasets/vgames2.csv') print(df.columns) df.head() # 의미가 있을만한 feature : 년도, 플랫폼. 장르. 제작한 회사??, 출고량     Index(['Unnamed: 0', 'Name', 'Platform', 'Year', 'Genre', 'Publisher',        'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales'],       dtype='object')                           Unnamed: 0       Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales                       0       1       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       0.04       0       0       0                 1       2       The Munchables       Wii       2009.0       Action       Namco Bandai Games       0.17       0       0       0.01                 2       3       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       0.02       0                 3       4       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       0.04       0       0       0                 4       5       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       0.12       0.09       0       0.04            2. 데이터 전처리, EDA   import numpy as np  # 첫번째 열 삭제 try:   df=df.drop(['Unnamed: 0'], axis=1) except:   pass  # 출고량 numerical value 화 (전부 (M이 있을 수 있음) 1000을 곱해주되, K가 들어가면 안곱해줌, ) def to_int(x):   x=str(x)   if 'K' in x:     x=x.replace('K','')     x=int(x)   else:     x=x.replace('M','')     x=int(float(x)*1000)   return x try:   df['NA_Sales']=df['NA_Sales'].apply(to_int)   df['EU_Sales']=df['EU_Sales'].apply(to_int)   df['JP_Sales']=df['JP_Sales'].apply(to_int)   df['Other_Sales']=df['Other_Sales'].apply(to_int) except:   pass # 각 판매량을 각게임의 전체판매량의 비중으로 계산하여 feature 추가( 예를 들어 30, 30, 20 , 20 이면 0.3, 0.3, 0.2, 0.2) # 이유 : 특정 게임의 흥행을 고려하지 않는 feature 필요. df['Game_sales']=df['NA_Sales']+df['EU_Sales']+df['JP_Sales']+df['Other_Sales'] df['NA_Sales(rate)']=df['NA_Sales']/df['Game_sales'] df['EU_Sales(rate)']=df['EU_Sales']/df['Game_sales'] df['JP_Sales(rate)']=df['JP_Sales']/df['Game_sales'] df['Other_Sales(rate)']=df['Other_Sales']/df['Game_sales']  # 년도가 이상하게 표기되어있는 경우. 수정 (0 이면 2000년, 98이면 1998년 등) def re_year(x):   if x&gt;1900 and x&lt;2100:     return x   elif x&lt;=21:     return 2000+x   elif x&gt;21:     return 1900+x   else:     return np.nan df['Year']=df['Year'].apply(re_year) df=df.dropna() df.head()                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Candace Kane's Candy Factory       DS       2008.0       Action       Destineer       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 1       The Munchables       Wii       2009.0       Action       Namco Bandai Games       170       0       0       10       180       0.944444       0.00       0.0       0.055556                 2       Otome wa Oanesama Boku ni Koi Shiteru Portable       PSP       2010.0       Adventure       Alchemist       0       0       20       0       20       0.000000       0.00       1.0       0.000000                 3       Deal or No Deal: Special Edition       DS       2010.0       Misc       Zoo Games       40       0       0       0       40       1.000000       0.00       0.0       0.000000                 4       Ben 10 Ultimate Alien: Cosmic Destruction       PS3       2010.0       Platform       D3Publisher       120       90       0       40       250       0.480000       0.36       0.0       0.160000            3-1 지역에 따라서 선호하는 게임 장르가 다를까      가정1 : 지역에 따른 ‘전체 게임 출고량’은 오직 인구수에만 종속된다. (선호도 이외의 영향은 없다)   가정2 : 게임의 전지역 출고량은 지역 장르선호도와 독립적이다. (특정 게임의 흥행은 선호도에 가중치를 주지않는다.)   from scipy.stats import chi2_contingency  # 년도에 상관없이 지역별로 각 장르 게임들의 출고량 rate 합을 그룹화 grouped_df1 grouped_df1=df.groupby('Genre').sum()[['NA_Sales(rate)','EU_Sales(rate)','JP_Sales(rate)', 'Other_Sales(rate)']] grouped_df1_T=grouped_df1.transpose() grouped_df1_T['Total_sales']=grouped_df1_T.sum(axis=1) grouped_df1_T.head()                    Genre       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Total_sales                       NA_Sales(rate)       1525.619292       342.188702       348.445844       807.461452       501.594820       319.931602       675.419334       432.557585       692.748044       409.577643       1210.555555       195.972163       7462.072036                 EU_Sales(rate)       823.705975       208.330453       143.982251       327.132529       211.535672       120.129892       375.877842       232.452620       379.688596       196.063888       538.379128       207.948389       3765.227234                 JP_Sales(rate)       658.357007       669.495053       294.008705       436.876869       106.652981       99.585575       72.600294       725.787645       108.868986       185.525061       388.643571       224.475923       3970.877670                 Other_Sales(rate)       232.317726       47.985792       48.563200       109.529150       53.216527       26.352931       94.102530       74.202151       98.694374       54.833408       161.421746       40.603524       1041.823060            # 2 sample 카이제곱검정을 통해 두변수가 독립적이라는 귀무가설을 검정. chi2 = chi2_contingency(grouped_df1_T[grouped_df1_T.columns.difference(['Total_sales'])]) chi2   (1834.559710433277,  0.0,  33,  array([[1488.73850969,  582.62976243,  383.67180728,  772.3979737 ,           401.13232067,  260.069752  ,  559.65540272,  673.14873972,           588.14360876,  388.72616642, 1056.3610598 ,  307.39693302],         [ 751.19065507,  293.98449093,  193.59388796,  389.73811456,           202.40414873,  131.22651567,  282.39204255,  339.65873755,           296.76667854,  196.1442266 ,  533.02077654,  155.10695933],         [ 792.21943666,  310.04143385,  204.16766346,  411.02496081,           213.45912599,  138.39388924,  297.81582526,  358.2103317 ,           312.97557992,  206.85729735,  562.13348299,  163.57864294],         [ 207.85139859,   81.34431278,   53.5666413 ,  107.83895093,            56.00440462,   36.30984309,   78.13672947,   93.98219103,            82.11413278,   54.27230963,  147.48468066,   42.91746471]]))   p값이 0 이므로 지역에 따른 장르는 독립적이지 않다.  즉, 지역에 따라 선호하는 게임이 유의미하게 다르다.   3-2 연도별 게임의 트렌드가 있을까     트렌드? 게임의 흥망을 고려해야 하는가?  -&gt; 어떤 게임이 대박이 났다면 대박난 게임은 트렌드를 잘탔기때문인가? 아니면 그저 잘 만들어졌기 때문인가?  예시   2000년도에 A라는 게임이 대박을 쳤고 action 장르라고 해보자.  데이터 분석을 할 때, 이 A 게임으로인한 2000년 action 장르 트렌드영향도를  더 크게 ?  -&gt; 대답 : 영향을 더 크게 고려해야함. (즉 rate 가 아닌 출고량으로 분석하겠다)           가정1 : 각 년도에 나오는 각 장르당 게임수는 트렌드와 독립적이다.       ( 매년 무조건 1만가지의 게임만 출시할수 있다는 제한이 있는 경우와 없는경우를 생각해보면 편할 것)       # 년도와 장르별로 게임 판매량을 그룹화함 grouped_df2=df.groupby(['Year', 'Genre']).sum()[['Game_sales']]  # 년도별 출시게임의 갯수를 저장 count=df.groupby(['Year']).count() count=count.reset_index()['Game_sales']  # 년도index, 장르columns 대하여 정리 grouped_df2=grouped_df2.pivot_table(values='Game_sales', index='Year', columns='Genre') grouped_df2=grouped_df2.reset_index() grouped_df2=grouped_df2.fillna(0) grouped_df2['Count']=count grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       0       1980.0       340.0       0.0       770.0       2700.0       0.0       0.0       0.0       0.0       7070.0       0.0       0.0       0.0       8                 1       1981.0       14790.0       0.0       0.0       0.0       6920.0       2250.0       480.0       0.0       10020.0       440.0       780.0       0.0       46                 2       1982.0       6500.0       0.0       0.0       870.0       5030.0       10040.0       1570.0       0.0       3810.0       0.0       1060.0       0.0       36                 3       1983.0       2860.0       400.0       0.0       2140.0       6930.0       780.0       0.0       0.0       490.0       0.0       3200.0       0.0       17                 4       1984.0       1850.0       0.0       0.0       1450.0       690.0       3140.0       5950.0       0.0       31100.0       0.0       6170.0       0.0       14            # 특정 년도는 게임수가 적어 묶어주겠음 (A개 이상으로) A=1200 temp=0 for i in grouped_df2.T.columns:   grouped_df2.T[i]=grouped_df2.T[i]+temp   if int(grouped_df2['Count'][i]) &lt; A :     temp=grouped_df2.T[i]     grouped_df2=grouped_df2.T.drop([i],axis=1).T   else:     temp=0 #마지막에도 80개이하면 버려지게됨. 끝에 붙여줌 if temp[temp.index=='Count'][0] &lt; A:   grouped_df2.iloc[-1]=grouped_df2.iloc[-1]+temp  #년도도 더해져버림... 적절히 평균으로 만들어줌. def qq(x):   i=1   while x/i&gt;2021 :     i+=1   return x/i grouped_df2['Year']=grouped_df2['Year'].apply(qq) grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       123220.0       32930.0       84759.0       39350.0       280500.0       94000.0       96939.0       122690.0       106629.0       33980.0       106290.0       25550.0       1255.0                 21       1999.5       157599.0       28570.0       84349.0       64460.0       106010.0       19460.0       139800.0       126390.0       53669.0       41480.0       164670.0       48020.0       1541.0                 23       2002.5       154630.0       13190.0       48710.0       39420.0       88770.0       7040.0       82330.0       75380.0       74740.0       32029.0       121270.0       13460.0       1600.0                 25       2004.5       161670.0       16980.0       36440.0       85790.0       70150.0       28670.0       102009.0       82460.0       88490.0       60170.0       120639.0       12480.0       1674.0                 27       2006.5       172480.0       35920.0       40160.0       158300.0       85230.0       34570.0       73010.0       95650.0       109230.0       70580.0       234359.0       13610.0       2200.0            # 각 출시년도에 대하여 Count로 나누어줌. (가정1) for i in grouped_df2.columns.difference(['Year','Count']):   grouped_df2[i]=grouped_df2[i]/grouped_df2['Count'] grouped_df2.head()                    Genre       Year       Action       Adventure       Fighting       Misc       Platform       Puzzle       Racing       Role-Playing       Shooter       Simulation       Sports       Strategy       Count                       17       1988.5       98.183267       26.239044       67.537052       31.354582       223.505976       74.900398       77.242231       97.760956       84.963347       27.075697       84.693227       20.358566       1255.0                 21       1999.5       102.270604       18.539909       54.736535       41.829981       68.792992       12.628164       90.720311       82.018170       34.827385       26.917586       106.859182       31.161583       1541.0                 23       2002.5       96.643750       8.243750       30.443750       24.637500       55.481250       4.400000       51.456250       47.112500       46.712500       20.018125       75.793750       8.412500       1600.0                 25       2004.5       96.577061       10.143369       21.768220       51.248507       41.905615       17.126643       60.937276       49.259259       52.861410       35.943847       72.066308       7.455197       1674.0                 27       2006.5       78.400000       16.327273       18.254545       71.954545       38.740909       15.713636       33.186364       43.477273       49.650000       32.081818       106.526818       6.186364       2200.0            chi2_1=chi2_contingency(grouped_df2[grouped_df2.columns.difference(['Count'])]) chi2_1   (1279.7439040830805,  1.724383653993331e-199,  108,  array([[ 118.47019129,   16.78986204,   32.5745123 ,   54.55699306,            62.97423775,   18.65798994,   52.46450991,   64.28669272,            70.50976375,   27.45568949,   91.15531716,   12.84964063,          2279.56894258],         [ 109.02005572,   15.45056757,   29.97610713,   50.20508837,            57.95090591,   17.16967856,   48.27951852,   59.15866891,            64.88533773,   25.26560281,   83.8840357 ,   11.82464991,          2097.73218419],         [ 100.89920463,   14.29966228,   27.74320144,   46.46533568,            53.63417102,   15.89071753,   44.68320059,   54.75196835,            60.05206038,   23.38358031,   77.63555455,   10.94383748,          1941.47313078],         [ 102.93759729,   14.58854787,   28.303677  ,   47.40404079,            54.71770286,   16.21174604,   45.58590253,   55.85808222,            61.26524813,   23.85598164,   79.20396875,   11.1649278 ,          1980.69528914],         [ 102.74194399,   14.56081945,   28.24988025,   47.31394002,            54.61370102,   16.18093241,   45.4992575 ,   55.75191287,            61.14880139,   23.81063862,   79.05342591,   11.14370664,          1976.93058537],         [ 101.38163783,   14.3680338 ,   27.87585106,   46.68750216,            53.89061412,   15.96669642,   44.89684607,   55.01375601,            60.33918958,   23.49538511,   78.00675638,   10.99616366,          1950.75597005],         [ 101.03873223,   14.31943644,   27.78156589,   46.52958987,            53.70833858,   15.91269187,   44.74499037,   54.82768164,            60.13510286,   23.41591609,   77.74291222,   10.95897107,          1944.15788038],         [ 101.30462451,   14.3571193 ,   27.85467552,   46.65203657,            53.84967678,   15.95456751,   44.86274074,   54.97196549,            60.29335365,   23.47753713,   77.94749951,   10.98781056,          1949.27410224],         [ 102.16087759,   14.47846941,   28.09011049,   47.04635174,            54.30482828,   16.08941967,   45.24193231,   55.43660286,            60.80296879,   23.67597539,   78.60633208,   11.0806824 ,          1965.74987477],         [ 102.35553281,   14.50605638,   28.1436328 ,   47.13599288,            54.40829957,   16.12007612,   45.32813535,   55.54223062,            60.9188215 ,   23.7210871 ,   78.75610695,   11.1017953 ,          1969.49537382]]))   p값 거의 0으로 년도와 장르는 독립적이지않다. 즉, 유의미한 연관이 있으며 연도별 트랜드는 존재한다.   심지어 게임 갯수를 2000개이상씩 묶어도 결과는 같다   3-3 출고량이 높은 게임에 대한 분석 및 시각화 프로세스   출고량 10000이상의 것으로 하겠다.   sorted_df=df.sort_values(by=['Game_sales'], ascending=False) sorted_df=sorted_df.reset_index(drop=True) i=0 while sorted_df['Game_sales'].iloc[i]&gt;10000:   i+=1 sorted_df=sorted_df.iloc[0:i] sorted_df #61개임.                           Name       Platform       Year       Genre       Publisher       NA_Sales       EU_Sales       JP_Sales       Other_Sales       Game_sales       NA_Sales(rate)       EU_Sales(rate)       JP_Sales(rate)       Other_Sales(rate)                       0       Wii Sports       Wii       2006.0       Sports       Nintendo       41490       29020       3770       8460       82740       0.501450       0.350737       0.045564       0.102248                 1       Super Mario Bros.       NES       1985.0       Platform       Nintendo       29080       3580       6810       770       40240       0.722664       0.088966       0.169235       0.019135                 2       Mario Kart Wii       Wii       2008.0       Racing       Nintendo       15850       12880       3790       3310       35830       0.442367       0.359475       0.105777       0.092381                 3       Wii Sports Resort       Wii       2009.0       Sports       Nintendo       15750       11010       3280       2960       33000       0.477273       0.333636       0.099394       0.089697                 4       Pokemon Red/Pokemon Blue       GB       1996.0       Role-Playing       Nintendo       11270       8890       10220       1000       31380       0.359146       0.283301       0.325685       0.031867                 ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...                 56       Super Mario All-Stars       SNES       1993.0       Platform       Nintendo       5990       2150       2120       290       10550       0.567773       0.203791       0.200948       0.027488                 57       Pokemon FireRed/Pokemon LeafGreen       GBA       2004.0       Role-Playing       Nintendo       4340       2650       3150       350       10490       0.413727       0.252622       0.300286       0.033365                 58       Super Mario 64       DS       2004.0       Platform       Nintendo       5080       3110       1250       980       10420       0.487524       0.298464       0.119962       0.094050                 59       Just Dance 3       Wii       2011.0       Misc       Ubisoft       6050       3150       0       1070       10270       0.589094       0.306719       0.000000       0.104187                 60       Call of Duty: Ghosts       X360       2013.0       Shooter       Activision       6720       2630       40       820       10210       0.658178       0.257591       0.003918       0.080313          61 rows × 14 columns    import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(40, 6)) plt.subplot(131) plt.pie(sorted_df['Publisher'].value_counts(), labels=sorted_df['Publisher'].value_counts().index, autopct='%.1f%%'); plt.title('Publisher');  plt.subplot(132) plt.pie(sorted_df['Platform'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Platform');  plt.subplot(133)  #plt.bar(sorted_df['Genre'].value_counts(), labels=sorted_df['Platform'].value_counts().index, autopct='%.1f%%'); plt.title('Genre'); grouped_df3=sorted_df.groupby(['Genre']).count() grouped_df3 plt.bar(grouped_df3.index, grouped_df3['Name']);      4. 다음 분기에 어떤 게임을 설계해야 할까   4-1. 장르는 트렌드와 시장규모 측면에서 유리한 것으로 선정  4-2. 퍼블리셔는 그 장르의 큰 규모 게임 사례가 있으면 좋겠다. (운영 노하우가 있을거라 가정)  4-3. 플랫폼은 퍼블리셔가 가장 많이 사용하는 플랫폼을 우선으로 (+ 장르에 상관없이 큰 규모의 게임 사례가 있으면 좋겠다.)      4-1. 장르의 트렌드    트렌드와 시장의 크기를 고려하여 Action 장르로 선정. (그외 후보 sports, shooting)    print('y축은 출고량') plt.figure(figsize=(30,10)) ax1 = plt.subplot2grid((3,7), (0,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Action']) plt.title('Action') ax1 = plt.subplot2grid((3,7), (0,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Adventure']) plt.title('Adventure') ax1 = plt.subplot2grid((3,7), (0,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Fighting']) plt.title('Fighting') ax1 = plt.subplot2grid((3,7), (0,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Misc']) plt.title('Misc') ax1 = plt.subplot2grid((3,7), (1,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Platform']) plt.title('Platform') ax1 = plt.subplot2grid((3,7), (1,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Puzzle']) plt.title('Puzzle') ax1 = plt.subplot2grid((3,7), (1,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Racing']) plt.title('Racing') ax1 = plt.subplot2grid((3,7), (1,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Role-Playing']) plt.title('Role') ax1 = plt.subplot2grid((3,7), (2,0)) plt.scatter(grouped_df2['Year'],grouped_df2['Shooter']) plt.title('Shooter') ax1 = plt.subplot2grid((3,7), (2,1)) plt.scatter(grouped_df2['Year'],grouped_df2['Simulation']) plt.title('Simulation') ax1 = plt.subplot2grid((3,7), (2,2)) plt.scatter(grouped_df2['Year'],grouped_df2['Sports']) plt.title('Sports') ax1 = plt.subplot2grid((3,7), (2,3)) plt.scatter(grouped_df2['Year'],grouped_df2['Strategy']) plt.title('Strategy');  ax1 = plt.subplot2grid((3,7), (0,4), colspan=3) plt.bar(grouped_df1.index, grouped_df1.sum(axis=1)) plt.title('Genre market size');   y축은 출고량         4-2. 퍼블리셔 별 Action 게임 데이터    action 게임중에서 총 출고량 2000 이상인 것들 중 각 퍼블리셔의 빈도수을 봄.  Ubisoft, Nintendo, Take-Two Interactive 세개의 퍼블리셔가 후보였는데  출고량이 가장 많은 게임을 운영했던 Take-Two Interactive 를 채택. (게임명 : Grand Theft Auto V)   action_df=df[df['Genre']=='Action'].sort_values(by=['Game_sales'], ascending=False) i=0 while action_df['Game_sales'].iloc[i]&gt;2000:   i+=1 action_df=action_df.iloc[0:i] print(action_df.groupby('Publisher').count()['Name'].sort_values(ascending=False)[0:3])  plt.figure(figsize=(30,5)) plt.subplot(131) plt.scatter(action_df[action_df['Publisher'] == 'Ubisoft']['Year'], action_df[action_df['Publisher'] == 'Ubisoft']['Game_sales']) plt.subplot(132) plt.scatter(action_df[action_df['Publisher'] == 'Nintendo']['Year'], action_df[action_df['Publisher'] == 'Nintendo']['Game_sales']) plt.subplot(133) plt.scatter(action_df[action_df['Publisher'] == 'Take-Two Interactive']['Year'], action_df[action_df['Publisher'] == 'Take-Two Interactive']['Game_sales'])  print(action_df[action_df['Publisher'] == 'Take-Two Interactive'][['Year','Game_sales']][0:5])   Publisher Ubisoft                 22 Nintendo                21 Take-Two Interactive    20 Name: Name, dtype: int64          Year  Game_sales 3483   2013.0       21390 14669  2004.0       20810 10913  2013.0       16380 5340   2002.0       16150 9786   2001.0       13100         4-3. 플랫폼 : 퍼블리셔와 밀접한 플랫폼    ‘Take-Two Interactive’ 가 사용했던 플랫폼 中   print(pd.DataFrame(df[df['Publisher']=='Take-Two Interactive'].groupby(['Platform']).count()).sort_values(by='Name', ascending=False)['Name'][0:3])  print(df[df['Publisher']=='Take-Two Interactive'][df['Platform']=='PS3'].sort_values(by='Year').iloc[-1]['Year'])   Platform X360    70 PS2     60 PS3     53 Name: Name, dtype: int64 2016.0   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.   This is separate from the ipykernel package so we can avoid doing imports until   결론 : Action 게임을,Take-Two Interactive 퍼블리셔와 함께, X360 혹은 PS3 플랫폼에서 설계할것이다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["pandas","시각화","가설검정"],
        "url": "http://localhost:4000/pj/section_1/",
        "teaser": null
      },{
        "title": "대출 승인 시스템의 승인 기준 머신러닝 예측",
        "excerpt":"AI SECTION2 PROJECT   목차  1) 데이터 선정 이유 및 문제 정의   2) 데이터를 이용한 가설 및 평가지표, 베이스라인 선택   3) EDA와 데이터 전처리   4) 머신러닝 방식 적용 및 교차검증   5) 머신러닝 모델 해석   # 설치 (처음1회) '''!pip install pandas_profiling==2.11 !pip install category_encoders !pip install shap'''   '!pip install pandas_profiling==2.11\\n!pip install category_encoders\\n!pip install shap'   1. 데이터 선정 이유 및 문제 정의  대출승인시스템을 선정하였다.  (kaggle url : https://www.kaggle.com/caiocampiao/loan-approval-systemlas)  이유     일단 기본적으로는 명확한 가이드라인을 따라 승인/거절이 나누어질 것이라 생각하였고, 그 외 분명 승인허가에 영향을 끼치는 적당한 외부 교란요인(특별한 경우)도 어느정도 있을 것이라 추측했다.    문제    매번 데이터를 확인해가며 수작업으로 승인/거절을 내리는 것(문제를 위한 가정) 보다 정확도 높은 모델을 사용하여 자동으로 결정되도록 해보자.    import pandas as pd  df=pd.read_csv(\"https://ai-bootcamp-chanwoo.s3.ap-northeast-2.amazonaws.com/HI/clientes.csv\") print(df.shape) df.head()   (614, 13)                           cod_cliente       sexo       estado_civil       dependentes       educacao       empregado       renda       renda_conjuge       emprestimo       prestacao_mensal       historico_credito       imovel       aprovacao_emprestimo                       0       LP001002       Male       No       0       Graduate       No       5849       0       NaN       360.0       1.0       Urban       Y                 1       LP001003       Male       Yes       1       Graduate       No       4583       1508       128.0       360.0       1.0       Rural       N                 2       LP001005       Male       Yes       0       Graduate       Yes       3000       0       66.0       360.0       1.0       Urban       Y                 3       LP001006       Male       Yes       0       Not Graduate       No       2583       2358       120.0       360.0       1.0       Urban       Y                 4       LP001008       Male       No       0       Graduate       No       6000       0       141.0       360.0       1.0       Urban       Y            2. 데이터를 이용한 가설 및 평가지표 선택  target     대출 승인 승인/거절 (aprovacao_emprestimo) (YorN)    Baseline model          Boolean target 이기 때문에 하나의 값이 70% 이상의 불균형이 아닌이상 둘중(Y,N) 큰 비율과              DummyClassifier를 사용한 결과의 평가지표를 비교하여 큰 것을 베이스라인 모델로 사용해보겠다.      target = ['aprovacao_emprestimo']   3. EDA와 데이터 전처리   EDA   # profiling # 결측치 확인 # 데이터 형 확인 # 데이터 분포 확인 from pandas_profiling import ProfileReport  ProfileReport(df, minimal=True, title = 'Section2 Project LAS', dark_mode=True)   Summarize dataset:   0%|          | 0/21 [00:00&lt;?, ?it/s]    Generate report structure:   0%|          | 0/1 [00:00&lt;?, ?it/s]    Render HTML:   0%|          | 0/1 [00:00&lt;?, ?it/s]     데이터 전처리   import numpy as np # 결측치 있는 feature # sexo                성별            # estado_civil        결혼유무        # dependentes         자녀수           # empregado           현재 직장유무 # emprestimo          대출금 # prestacao_mensal    한달분납금 # historico_credito   과거 신용  # 형변환 필요 # dependentes         자녀수        숫자로 변환필요 # renda_conjuge       배우자 수입   숫자로 변환 필요 # historico_credito   과거 신용     숫자인데 boolean 이라 인코딩필요(문자로 변환하자)  # high cardinality categorical 변수는 없음  #cod_cliente 고객 코드. -&gt; 마지막 숫자만 남겨놓자 #df['cod_cliente']=df['cod_cliente'].apply(lambda x : int(x[4:7])) df=df.drop('cod_cliente', axis=1)  # 성별 새롭게 'Dont_know'으로 처리 df['sexo']=df['sexo'].fillna('Dont_know')  #결혼 유무 새롭게 'Dont_know'으로 처리 df['estado_civil']=df['estado_civil'].fillna('Dont_know')  #현재 직장유무  새롭게 'Dont_know'으로 처리 df['empregado']=df['empregado'].fillna('Dont_know')  # 자녀수 숫자로 변환 # 자녀수 3이상은 3으로처리 def to_int(x):   try :     return int(x)   except :     return 3 df['dependentes']=df['dependentes'].apply(to_int)  #대출금 평균 df['emprestimo']=df['emprestimo'].fillna(df['emprestimo'].mean())  #한달분납금 평균 df['prestacao_mensal']=df['prestacao_mensal'].fillna(df['prestacao_mensal'].mean())  #과거 신용 새롭게 'Dont_know' 으로 처리 df['historico_credito']=df['historico_credito'].fillna(2) #과거 신용 문자로 변환 df['historico_credito']=df['historico_credito'].apply(str)  #배우자 수입 숫자로 변환 #변환 안되면 평균값 def to_float(x):   try:     return float(x)   except:     return np.nan df['renda_conjuge']=df['renda_conjuge'].apply(to_float) df['renda_conjuge']=df['renda_conjuge'].fillna(df['renda_conjuge'].mean())  # target을 True과 False 으로 df['aprovacao_emprestimo']=df['aprovacao_emprestimo']=='Y'  # 결측치 확인 print(\"결측치 갯수\", df.isnull().sum().sum())  # 데이터 형확인 df.info()   결측치 갯수 0 &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 614 entries, 0 to 613 Data columns (total 12 columns):  #   Column                Non-Null Count  Dtype   ---  ------                --------------  -----    0   sexo                  614 non-null    object  1   estado_civil          614 non-null    object  2   dependentes           614 non-null    int64    3   educacao              614 non-null    object  4   empregado             614 non-null    object  5   renda                 614 non-null    int64    6   renda_conjuge         614 non-null    float64  7   emprestimo            614 non-null    float64  8   prestacao_mensal      614 non-null    float64  9   historico_credito     614 non-null    object  10  imovel                614 non-null    object  11  aprovacao_emprestimo  614 non-null    bool    dtypes: bool(1), float64(3), int64(2), object(6) memory usage: 53.5+ KB   feature enginnering   # sexo                  성별                bool # estado_civil          결혼유무            bool # dependentes           자녀수              int # educacao              (대학?) 졸업유무    bool # empregado             현재 직장유무       bool # renda                 수입                int # renda_conjuge         배우자 수입         int # emprestimo            대출금              int # prestacao_mensal      한달 분할금         int # historico_credito     과거 신용           bool # imovel                거주지역            object 3가지 # aprovacao_emprestimo  대출허가            타겟.Bool  # total_renda = 본인수입 + 배우자 수입 df['total_renda']=df['renda']+df['renda_conjuge']  # ratio_emprestimo = 대출금 / total_renda *100 df['ratio_emprestimo']=df['emprestimo']/df['total_renda']*100  # ratio2_emprestimo = 대출금 / renda *100 df['ratio2_emprestimo']=df['emprestimo']/df['renda']*100  # ratio_prestacao_mensal = 한달분납금 / total_renda *100 df['ratio_prestacao_mensal']=df['prestacao_mensal']/df['total_renda'] *100  # ratio2_prestacao_mensal = 한달분납금 / renda *100 df['ratio2_prestacao_mensal']=df['prestacao_mensal']/df['renda'] *100  # family = 전체 가족수 (본인+배우자+자녀) df['family']=df['estado_civil'].apply(lambda x: 1 if x=='Yes' else 0) + df['dependentes']+1  # per_man_renda = total_renda / 가족수 df['per_man_renda']=df['total_renda']/df['family']  # per_man_emprestimo = 대출금 / 가족수 df['per_man_emprestimo'] = df['emprestimo']/df['family']  # per_man_prestacao_mensal = 한달분납금/가족수 df['per_man_prestacao_mensal']=df['prestacao_mensal']/df['family']   Data Leakage 는 없는 것으로 보임. (일단 profileReport 상 high correlation은 없음)  모델의 한계     데이터가 적어서 (614개) 결측치 (크게는 한feature에 50개) 에 대한 처리가 영향력이 클것으로 보임.    4. 머신러닝 방식 적용 및 교차검증      모델 성능 개선을 위해 어떤 방법을 적용했나요? 그 방법을 선택한 이유는 무엇인가요?       최종 모델에 관해 설명하세요.    from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(df.drop(target, axis=1), df[target], test_size=0.2, random_state=2)   Baseline model      test 데이터셋의 다수비율은 0.683       DummyClassifier model 은 정확도 0.601, 정밀도 0.701, 재현율 0.726, roc_auc : 0.530    y_test.value_counts(normalize=True).max()   0.6829268292682927   # Decision Tree model from sklearn.dummy import DummyClassifier from sklearn.pipeline import make_pipeline from category_encoders import OrdinalEncoder from sklearn.impute import SimpleImputer from sklearn.metrics import f1_score from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score baseline_pipe = make_pipeline(     OrdinalEncoder(),     SimpleImputer(),     DummyClassifier(random_state=12) )  baseline_pipe.fit(X_train, y_train)  y_pred=baseline_pipe.predict(X_test) # 정확도 print('정확도 : ', accuracy_score(y_test, y_pred)) # 정밀도 print('정밀도 : ', precision_score(y_test, y_pred)) # 재현율 print('재현율 : ', recall_score(y_test, y_pred)) #ROC print('roc_auc : ', roc_auc_score(y_test, y_pred))   정확도 :  0.6016260162601627 정밀도 :  0.7011494252873564 재현율 :  0.7261904761904762 roc :  0.5297619047619048   The default value of strategy will change from stratified to prior in 0.24.   #오차행렬 from sklearn.metrics import plot_confusion_matrix import matplotlib.pyplot as plt  fig, ax = plt.subplots() matrix = plot_confusion_matrix(baseline_pipe, X_test, y_test,                             cmap = plt.cm.Blues,                             ax = ax); plt.title(f'Baseline_model_Confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      main model      OrdinalEncoder, SimpleImputer로 processing_pipe를 구성       model= Xgboost Classifie       {base_score, min_child_weigh,t max_depth, gamma} 4개 파라미터에 대해 GridSearchCV (cv=3) 파라미터 튜닝을 하였음.       {각각 0.6, 3 , 5 , 8}       정확도 : 0.764, 정밀도 : 0.784, 재현율 : 0.905, ROC 0.683    from xgboost import XGBClassifier processor = make_pipeline(     OrdinalEncoder( ),     SimpleImputer(strategy='median') )  X_train_processed=processor.fit_transform(X_train) X_test_processed=processor.transform(X_test)  model=XGBClassifier(n_estimators=99, seed=2, n_jobs=2, random_state=22 )   from scipy.stats import randint, uniform from sklearn.model_selection import GridSearchCV import multiprocessing dists = {   'xgbclassifier__base_score' :[0.4,0.5, 0.6], #   'xgbclassifier__min_child_weight' : [8,9],    #   'xgbclassifier__max_depth' : [1,5,10],   'xgbclassifier__gamma' : [2,3,4,5], }  clf = GridSearchCV(     model,     param_grid=dists,     cv=3,     scoring= 'roc_auc',     verbose=1,     n_jobs=6 )  clf.fit(X_train_processed, y_train);   Fitting 3 folds for each of 72 candidates, totalling 216 fits   [Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers. [Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   26.7s [Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.7min [Parallel(n_jobs=6)]: Done 216 out of 216 | elapsed:  2.0min finished /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True) /usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().   y = column_or_1d(y, warn=True)   model=clf.best_estimator_  y_pred=model.predict(X_test_processed) # 정확도 print(accuracy_score(y_test, y_pred)) # 정밀도 print(precision_score(y_test, y_pred)) # 재현율 print(recall_score(y_test, y_pred)) #ROC_auc print('roc_auc : ', roc_auc_score(y_test, y_pred)) clf.best_params_   0.7642276422764228 0.7835051546391752 0.9047619047619048 roc_auc :  0.6831501831501832      {'xgbclassifier__base_score': 0.4,  'xgbclassifier__gamma': 2,  'xgbclassifier__max_depth': 1,  'xgbclassifier__min_child_weight': 8}   fig, ax = plt.subplots() matrix = plot_confusion_matrix(model, X_test_processed, y_test, cmap = plt.cm.Blues,ax = ax); plt.title(f'main_model_confusion matrix, n = {len(y_test)}', fontsize=15) plt.show()      5) 머신러닝 모델 해석      모델이 관측치를 예측하기 위해서 어떤 특성을 활용했나요?       어떤 특성이 있다면 모델의 예측에 도움이 될까요? 해당 특성은 어떻게 구할 수 있을까요?    feature importances   import shap  explainer = shap.TreeExplainer(model) shap_values = explainer.shap_values(X_test_processed[:100]) shap.summary_plot(shap_values, X_test_processed[:100], feature_names=X_test.columns)      예측결과는 True 예측이 성공한 경우   def explain(row_number):     positive_class = True     positive_class_index = 1      # row 값을 변환합니다     row = X_test.iloc[[row_number]]     row_processed = processor.transform(row)      # 예측하고 예측확률을 얻습니다     pred = model.predict(row_processed)[0]     pred_proba = model.predict_proba(row_processed)[0, positive_class_index]     pred_proba *= 100     if pred != positive_class:         pred_proba = 100 - pred_proba      # 예측결과와 확률값을 얻습니다     print(f'이 대출에 대한 예측결과는 {pred} 으로, 확률은 {pred_proba:.0f}% 입니다.')      # SHAP를 추가합니다     shap_values = explainer.shap_values(row_processed)      # Fully Paid에 대한 top 3 pros, cons를 얻습니다     feature_names = row.columns     feature_values = row.values[0]     shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))     pros = shaps.sort_values(ascending=False)[:3].index     cons = shaps.sort_values(ascending=True)[:3].index      # 예측에 가장 영향을 준 top3     print('\\n')     print('Positive 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = pros if pred == positive_class else cons     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # 예측에 가장 반대적인 영향을 준 요인 top1     print('\\n')     print('Negative 영향을 가장 많이 주는 3가지 요인 입니다:')      evidence = cons if pred == positive_class else pros     for i, info in enumerate(evidence, start=1):         feature_name, feature_value = info         print(f'{i}. {feature_name} : {feature_value}')      # SHAP     shap.initjs()     return shap.force_plot(         base_value=explainer.expected_value,         shap_values=shap_values,         features=row,         link='logit'     )   df2=pd.DataFrame() df2['y_test']=y_test['aprovacao_emprestimo'] df2['y_pred']=y_pred df2['Right / wrong'] = df2['y_test']==df2['y_pred'] df2=df2.reset_index()   df2[df2['Right / wrong']==True][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       2       265       True       True       True                 3       84       True       True       True                 5       436       True       True       True                 6       542       True       True       True                 7       526       True       True       True            explain(6)   이 대출에 대한 예측결과는 True 으로, 확률은 85% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. imovel : Semiurban 3. ratio_prestacao_mensal : 9.857612267250822   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. renda : 3652 2. total_renda : 3652.0 3. ratio2_emprestimo : 2.6013143483023002          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 성공한 경우   df2[df2['Right / wrong']==True][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       0       414       False       False       True                 1       569       False       False       True                 15       108       False       False       True                 23       225       False       False       True                 24       369       False       False       True            explain(23)   이 대출에 대한 예측결과는 False 으로, 확률은 53% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_emprestimo : 5.230769230769231 2. per_man_emprestimo : 85.0 3. imovel : Rural   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. total_renda : 3250.0 3. ratio2_prestacao_mensal : 11.076923076923077          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 True 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==True].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       11       510       False       True       False                 13       1       False       True       False                 16       469       False       True       False                 21       199       False       True       False                 48       18       False       True       False            explain(48)   이 대출에 대한 예측결과는 True 으로, 확률은 91% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 1.0 2. ratio2_prestacao_mensal : 7.366482504604052 3. per_man_emprestimo : 66.5   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. imovel : Rural 2. renda : 4887 3. educacao : Not Graduate          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.    예측결과는 False 예측이 잘못된 경우   df2[df2['Right / wrong']==False][df2['y_pred']==False].head()   Boolean Series key will be reindexed to match DataFrame index.                           index       y_test       y_pred       Right / wrong                       4       109       True       False       False                 30       155       True       False       False                 33       130       True       False       False                 38       604       True       False       False                 42       407       True       False       False            explain(30)   이 대출에 대한 예측결과는 False 으로, 확률은 94% 입니다.   Positive 영향을 가장 많이 주는 3가지 요인 입니다: 1. historico_credito : 0.0 2. per_man_renda : 7999.8 3. per_man_prestacao_mensal : 36.0   Negative 영향을 가장 많이 주는 3가지 요인 입니다: 1. ratio_prestacao_mensal : 0.450011250281257 2. ratio2_prestacao_mensal : 0.450011250281257 3. ratio_emprestimo : 1.5000375009375235          Visualization omitted, Javascript library not loaded!    Have you run `initjs()` in this notebook? If this notebook was from another   user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing   this notebook on github the Javascript has been stripped for security. If you are using   JupyterLab this error is because a JupyterLab extension has not yet been written.        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["XGboost","GridSearch"],
        "url": "http://localhost:4000/pj/section_2/",
        "teaser": null
      },{
        "title": "진짜 시작 그리고 m1맥북 세팅",
        "excerpt":"반성  지난 6월 코드스테이츠를 시작하면서 블로그를 개설했는데 꾸준히 쓰겠다고 했는데,  꾸준히는 개뿔 이게 테스트용 첫번째 글 이후 이게 5개월만에 두번째 글을 올리게 되었다.   왜 필요성을 느꼈는가  사실 대학교를 다닐때도 기록보단 필기를 하였고, 시험공부를 위한 필기였기에 그 필요성을 느끼지 못하다가  이번에 section4 딥러닝 파트를 마무리하면서 그 필요성을 느끼게 되었다.  그 전환점으로 지난 2년간 쓴 아이폰에 용기를 얻어 맥북을 구매하면서, 개발세팅을 새로하게되었는데  m1 칩셋을 사용하다보니 세팅하는게 하나하나가 쉽지 않았다.  그와 동시에 이 과정들을 기록해두지 않으면 다시 찾아봐야한다는 공포가 엄습했고  이를 계기로 맥북을 사용하여 지금까지 해온 코드스테이츠  전체적인 복습 과  완성하지 못한 section 4 프로젝트를 마무리하는 과정을 불로깅   하는게 1차 목표다.   터미널 창 띄우기     cmd + space를 눌러 spotlight에서 ‘터미널’을 검색하면 된다.    맥북 m1 세팅  homebrew 설치     macOS용 패키지 관리 어플리케이션! https://brew.sh 위 공식 홈페이지를 참고한다면 어렵지 않게 설치가능하다.  단, m1의 경우 ㅜㅜ finder에서 터미널 어플을 찾아 우클릭하여 ‘정보가져오기’에서 ‘rossetta로 사용하여 열기’를 꼭! 체크해야한다.  로제타는 기존의 intel 프로세서에서 돌아가는 친구들을 m1 칩셋, 즉 apple silicon에서 돌아가게 변환해주는 에뮬이라고 보면 된다.    miniforge 설치     알아보니 아나콘다를 설치안하고 miniforge만 설치해도 되는거였다.. m1에서 conda 를 좀더 에러없게 실행하는 인스톨러이며, anaconda와 동급의 카테고리다. (conda의 인스톨러 종류 : 아나콘다, miniforge, miniconda) 이렇게  https://developer.apple.com/metal/tensorflow-plugin/  위에서 apple silicon 이라되어있는곳을 따라 진행하면 된다 ( 설치파일 받고 아래 코드 3줄)     (brew install miniforge 로 설치해주면 된다고도 하는데 tensorfolw-deps (의존성)설치에서 에러가나더라)    아나콘다 가상환경 생성, 삭제, 패키지 설치     생성        conda create --name 이름 python=3.8      삭제        conda env remove --namve 이름      git 설치     git에 기능이 많겠지만 본인은, github 레포와 연동하여 프로젝트들을 관리하기 위한 용도이다.        brew install git      만으로 설치가 가능하다.        git --version      으로 확인까지!    가상환경 세팅  m1 tensorflow-gpu사용하기     m1은 gpu까지 하나로 싸잡아서 만든 칩셋이라 설정방법도 다르다.. 가상환경에서        conda install -c apple tensorflow-deps     pip install tensorflow-macos     pip install tensorflow-metal      로 설치해준다. 파이썬 내에서        import tensorflow as tf     print(len(tf.config.experimental.list_physical_devices('GPU')))      위 결과가 1이 나오면 사용할 수 있다. 단, 확인을 했으면 아래와같이 tensorflow 2.0을 사용하자.(혹시모르니)        import tensorflow.compat.v2 as tf      jupyter notebook과 pandas 설치        conda install -c conda-forge -y pandas jupyter      로 설치해준다. pandas 필요없다면 pandas나 jupyter만 지우면 된다.    그 외 패키지 설치        conda install 패키지이름      단,conda 명령어로 설치가 되는지 안되는지는 잘 검색해보고 쓰는게 좋다. (pip으로만 설치가 되는 패키지도 있음)        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/setting/",
        "teaser": null
      },{
        "title": "openCV 이미지 읽기",
        "excerpt":"OpenCV vs pillow(PIL) vs scikit-image     https://github.com/ethereon/lycon  위 링크에서는 세 라이브러리의 속도비교를 해 놓았는데,  속도측이나 기능측이나 빠르 openCV를 사용한 이미지 읽기를 정리해보려한다.  만약 다른 라이블리르 이용할 일이 생길때마다 업데이트를 해야겠다.    openCV 설치  conda install -c anaconda opencv   local 경로로 이미지열기  import cv2 cv2.imread('파일경로', flags) # ndarray형식으로 리턴.     flags 는 컬러 (3차원)가 default, 숫자 0 을 넣으면 흑백(1차원)    링크이미지(.jpeg) 열기     크롤링의 개념을 가져와서 사용한다. ```` import cv2 from google.colab.patches import cv2_imshow import numpy as np import urllib.request    def url_to_image(url):   ‘’’   jpg, png 이미지링크에서 numpy ndarray로 return   ‘’’   resp = urllib.request.urlopen(url)   image = np.asarray(bytearray(resp.read()), dtype=’uint8’)   image = cv2.imdecode(image, cv2.IMREAD_COLOR)   return image   &gt; requests.urlopen : url 에서 request(응답) 객체르 리턴   &gt; -&gt; .read()로써 호출할 수 있음.   &gt; numpy.asarray : 배열로써 이미지르 읽음 &gt; image를 cv2로 다시 읽어 리턴한다.(3차원)    ## svg 파일 열기 &gt; cairosvg 라이브러리 내 함수, svg2png, svg2pdf, svg2svg, svg2ps 등 지원     &gt; m1 mac에서는 어떻게 설치하는 업데이트 할 예정 아직공부중 ## 이미지 확인하기    img = 이미지의 ndarray형식 cv2.imshow(‘name’, img)   # name은 이미지르 띄운 window 이름 ````     위는 주피터노트북에서 쓰면 되느 방식이고 .py르 터미널에서 실행할때는  cv2.waitKey(0) 을 이용해서 키보드입력이 있을때까지 띄워놓아야 한다!  cv2.destroyAllWindows() 또한 띄워놓으 윈도우르 전부 파괴        🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/image/",
        "teaser": null
      },{
        "title": "정렬",
        "excerpt":"선택정렬          제일 작은걸 차례대로 찾아서, 맨앞부터 계속 교체해주는 정렬.  n개에 대해서 교환을 하며, 그 과정에서 최솟값을 찾기위해 n번 비교  (비교대상이 1씩 줄긴하지만 n이 최고차항이긴함.)를 하므로 O(n^2)        def selection_sort(li):     n = len(li)     for i in range(n):      # i번쨰가 주체         for j in range(i,n):# i번째 뒤로 쭉             if li[i] &gt; li[j]:                 li[i], li[j] = li[j], li[i]     # 교환             else :                 pass     return li          삽입정렬      순서대로 자리를 찾아서 끼워 넣어주는 정렬  n개에 대해서 하며, 최악의 경우 탐색을 계속 n개에 대해 길게하게 되기때문에 O(n^2)  아래 사진은 31의 자리를 찾아 넣어주고있다.        def insertion_sort(li):     n=len(li)     if n ==0:         return None     for i in range(1,n): # 두번째부터 끝까지 수행하면 댐.         temp = li[i]         for j in range(i-1, -1 , -1): # i-1부터 0까지 비교를 해야댐.             if li[j] &gt; temp:                 li[j+1] = li[j]       # j에서 temp보다 크다면 옆으로복사                 if j==0:              # 그와중에 0에 도착햇다면 그냥 0(맨앞)에 temp대입                     li[0] = temp             else:                 li[j+1] = temp        # j보다 temp가 크다면 그 오른쪽에 temp넣고 break                 break     return li          버블정렬      계속해서 옆과 비교,교환을 하는 방식.  (오름차순에서) 만약 맨앞에 제일큰숫자가 있다면, 끊임없이 교환을 거듭하여 끝까지 갈 것이다.(n번)  그 과정을 (1번부터 n까지), (1번부터 n-1까지) … (1과 2) 반복하면 (n번) O(n^2)이다.         def bubble_sort(li):     n = len(li)     for i in range(n):      # 0부터 n-1까지         for j in range(0,n-i):# j는 항상 0부터이되, n-i까지만반복             if j==(n-1):      # indexError 방지.                 pass             elif li[j] &gt; li[j+1]:                 li[j], li[j+1] = li[j+1], li[j]     # 교환             else :                 pass     return li     퀵정렬          특정값을 잡아 그보다 큰값과 작은값으로 계속해서 반으로 쪼갠다 1개짜리 리스트가 될때까지.  이상적으로 쪼개어진다면 아래그림에서 한층에 대해 모든수를 비교하므로 n , 그 층은 2^x의 해 이므로 log n 이므로 O(nlogn)     그러나 아래처럼 계속 최악의 경우로 된다면 층은 n층이 되기때문에 최악의 경우 O(n^2)        def quick_sort(li):     if len(li)&lt;=1:         return li     else:         L1, L2 = [], []         for x in li[1:]:             if x&lt;=li[0]:                 L1.append(x)             else :                 L2.append(x)         return quick_sort( L1 ) + quick_sort([ li[0] ]) + quick_sort( L2 )  병합정렬  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/5th/",
        "teaser": null
      },{
        "title": "단순선형회귀, 회귀의 평가지표 그리고, 모델의 적합",
        "excerpt":"선형회귀     엑셀에서 점들의 추세선을 그어본적이 있다. 거기서 R값을 표시할 수도 있었는데, 이게 단순 선형회귀의 예시가 아닌가 싶다.  이 또한, 머신러닝이다. 특정 평가지표가 최소가 되도록 모델에 학습데이터를 fit을 시켜주는 데, 평가지표의 종류는 아래와 같다.  y는 실제값(관측값, 측정값)     y^는 예측값 즉 ax+b, y평균은 y전체의 평균.      # 라이브러리 import from sklearn.linear_model import LinearRegression  # 데이터 정의 df = '타겟을 포함한 데이터 프레임' df_test = '타겟 미포함 \"테스트\" 데이터 프레임'  # 모델 클래스 정의 model = LinearRegression()  # feature, target 정의 feature = ['feature_name'] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 모델 학습 model.fit(X_train, y_train)  # 예측값. X_test = [[x] for x in df_test['feature_name']] y_pred = model.predict(X_test)  # 계수확인 print('절편', model.intercept_) print('계수(여러개일 수 있기때문에 array)', model.coef_)  # 시각화. import matplotlib.pyplot as plt ## train 데이터에 대한 그래프를 검정색 점으로. plt.scatter(X_train, y_train, color='black', linewidth=1)  ## test 데이터에 대한 예측을 파란색 점으로. plt.scatter(X_test, y_pred, color='blue', linewidth=1);   다중선형회귀     x에 대해서 y의 추세선을 그으면 단순선형회귀, x와 y에 대해서 z의 추세선을 그으면 다중선형회귀가 될 것이다.   그렇다면 4개이상 n개의 특성(feature)에 대해서 target의 추세선을 그릴 수 있을까?  위 평가지표(MSE, MAE, R square)에서 예측값 y^ 는 ax+b일수도 있지만  ax+bw+c의 형태로도 될 수있고 일반화하면 아래와 같다.      # 위 단순 선형회귀에서 X만 여러개 넣어주면 된다! (model도 동일)  # feature, target 정의 feature = ['feature_name_1', 'feature_name_2 ] target = ['target_name'] X_train = df[feature] y_train = df[target]  # 나머지는 동일!!   평가지표     MSE (Mean Square Error),  MAE(Mean Absolute Error),  RMS or RMSE (Root Mean Square Error)  R-score    과적합 &amp; 과소적합  편향은 잘못된 가정을 했을 때 발생하는 오차,  과소적합 문제를 야기.  분산은 트레이닝 셋의 복잡도에 의해 발생하는 오차, 큰 노이즈 까지 모델링에 포함시켜   과적합 문제 야기.     분산과 편향은 트레이드오프(trade-off)관계이다.  예를 들어, 고분산 모델은 트레이닝 셋의 특성을 잘 담는다고도 할 수 있지만, 일반화에 실패했다고도 할 수 있다.  고편향은 지나친 일반화를 하여 과소적합이 되는 것.  즉 적절한 일반화는 편향과 분산의 적절한 분배를 의미하기도 한다.      위 표에서 가로축은 모델의 복잡도, 세로축은 score 성능.  즉, 가로축은 고편향에서 고분산으로 향한다.   복잡도 up! -&gt; 트레이닝셋 score up!  너무 복잡해지면 고분산, 즉 과적합 ( 트레이닝셋에만 과하게 학습됨.)  best model은 트레이닝셋과 독립적인 검증데이터에서 의 score로 찾아야 할 것.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/regression/",
        "teaser": null
      },{
        "title": "가설검정",
        "excerpt":"가설 검정?  ‘서울시는 사람이 많이 산다.’ 는 사실확인이 가능할까? 안된다. ‘많이’는 너무 주관적이기 때문이다.  그렇지만 ‘사람의 머리카락은 평균 11만 가닥이다.’ 는 사실확인을 할 수 있을까?  데이터 직군에서는 이를 통계학을 기반으로하여 p value 값을 통해 ‘통계적으로 유의하다.’ or ‘통계적으로 유의하지않다’ 를 결정한다. (고 약속했다.)   귀무 가설  귀무 가설은 하나의 가설 검정이 하나만 갖는다. 이 가설이 기각되는지, 기각되지 않는지에 따라 통계적으로 어떤 의미를 갖는지 명확해지므로  처음 검정을 할 때, 귀무가설이 무엇인지 정확히 짚고 실행해야 할 것이다.   p-value  pvalue &lt; 0.01 : 귀무가설이 옳을 확률이 1%이하 -&gt; 틀렸다 (깐깐한 기준)  pvalue &lt; 0.05 (5%) : 귀무가설이 옳을 확률이 5%이하 -&gt; 틀렸다 (일반적인 기준)  0.05 ~ pvalue ~ 0.1 사이인 경우: (애매함)  pvalue &gt; 0.1 (10%) : 귀무가설이 옳을 확률이 10%이상인데 -&gt; 귀무가설이 맞다 ~ 틀리지 않았을것이다.  p-value : 0.85 –&gt; 귀무가설은 틀리지 않았다. (귀무가설이 옳다와 톤이 약간 다름)   독립표본 T-Test     one_sample_t_test    from scipy import stats pv1=stats.ttest_1samp(df['AGE'],35).pvalue      귀무가설 : df데이터의 사람들의 나이는 평균이 35라고 할 수 있다.  예) pv1이 0.05 보다 작다면 ‘사람들의 나이 평균은 35라는 가설은 기각한다.’ 즉 ‘신뢰도 95%에서 평균은 35라고 할 수는 없다.’   예) pv1이 0.85 정도가 나왔다면 ‘신뢰도 95%에서 사람들의 나이평균은 35이다’      two_sample_t_test    pv2=stats.ttest_ind(df1['AGE'],df2['AGE']).pvalue pv3=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='greater').pvalue pv4=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='less').pvalue      pv2 귀무가설 : df1 나이평균과 df2나이평균은 통계적으로 같다. (나이에 있어서 두 샘플은 같은 통계적 분포를 가진다)  pv3 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 크다.  pv4 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 작다.   대응표본(쌍체표본) t-Test  데이터 수가 같은 두 표본 (같은 집단에 대한 약물의 전후 효과 비교 등) 의 평균을 비교한다.   import scipy.stats scipy.stats.ttest_rel(dat_M, dat_F) # dat_M과 dat_F는 쌍이되는 두 표본의 어떤 feature값들,   귀무가설 : dat_M 과 dat_F 의 평균은 통계적으로 같다.   chi-square-Test  카이제곱 검정은 두 다른 feature에 대한 검정을 하는데 사용하거나  두 독립 표본이 통계적으로 차이가 있는지를 검정하는데 사용.      적합도 검정 (일원카이제곱)    두 표본이 통계적으로 같은 결과라고 볼 수 있는지 검정한다.  예 : 주사위를 실제로 던져 나온 숫자 데이터셋 vs 같은 횟수만큼 3.5를 적은 데이터셋  혹은 실제 관측 데이터 vs 기대 데이터  혹은 표본이 모집단을 대표할 수 있는지에 대한 검정도 가능 ( 적합도 라는 단어가 이제야 어울린다.. )   귀무가설 : 두 데이터는 통계적으로 같은 데이터이다.  from scipy.stats import chi2_contingency chi_res  = chi2_contingency(pd.crosstab(df['실제관측'], df['기댓값']))      행과 열의 독립성 검정 (이원카이제)       df가 위와같은 데이터셋일 때,  귀무가설 : cut(품질)과 color(색상)은 독립적이다.   from scipy.stats import chi2_contingency  chi2 = chi2_contingency(df) chi2  chi2의 첫번째 값 : chi suare, 두번째 값 : p-value  만약 p-value가 0.05아래라면 신뢰도 95%에서 귀무가설은 기각, 품질과 색상은 관련이 있다.  만약 큰 값을 가진다면 품질과 색상은 독립적이다.      동질성 검정 (이원카이제곱)    두 표본이 같은 모집단에서 나온 것인지 아닌지 판단할 수 있는지 검정  이는 위 독립성 검정의 방법을 그대로 따라하되  그 데이터가 어떤 구성인지의 차이 + 해석의 차이를 두면 된다.   좋은 예시가 있어서 가져와봤다. (출처 : https://hsm-edu.tistory.com/1213)      위와 아래의 차이는 아무런 코멘트가 없다면 ‘모른다’가 정답이다.  그러나 위 데이터는, 한번에 200명의 표본을 추출한것이고.  아래 데이터는 모집단에서 A 데이터 100명과 B 데이터 100명을 추출했다고 하면 차이가 느껴질 것이다.   위 데이터에서 이원카이제곱을 한다면  귀무가설 : 성별과 흡연유무는 독립적이다.   아래 데이터에서 이원카이제곱을 한다면  귀무가설 : A데이터와 B데이터는 다른 모집단에서 왔다. (독립적이다.)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/hypoo/",
        "teaser": null
      },{
        "title": "데이터 다루기",
        "excerpt":"pandas document   pandas 의 모든것이 들어있는 공식문서  https://pandas.pydata.org/docs/reference/index.html   DataFrame ?   데이터프레임을 먼저 알아야 하는데, 열(특성)과 행(단일데이터) 들로 이루어진 2차원 표라고 생각하면 된다.   데이터 불러오기     csv 불러오기    정확히는 csv 파일을 dataFrame의 객체타입으로 변환하는 것  import pandas as pd path = #csv의 경로 df = pd.read_csv(path) #tsv 파일이라면 comma(',')로 구분되는 것이 아닌 tab('\\t')으로 구분되기때문에 sep을 설정해줘야함. df = pd.read_csv(path, sep='\\t')  #csv 파일의 첫번째 줄은 feature의 이름인 경우가 많다. 그땐 index_col=0 으로 하면 0번째 행을 설정하는것 df = pd.read_csv(path, index_col=0)  # 내가 딱 필요한 feature를 정해서 불러온다. 아주 애용한다. df = pd.read_csv(path, usecols=['A', 'B'])    # skiprows를 이용하면 1,2번째 행은 제외할 수 있다. df = pd.read_csv(path, skiprows = [1, 2])  # nrows를 5로 하면 위에서 5개 데이터만 불러온다. df = pd.read_csv(path, nrows = 5)  #na_values를 이용해서 결측값을 제외하고 부를 수 있지만 dataframe의 내장함수를 사용하는 걸 개인적으로 선호한다. df = pd.read_csv(path, na_values = [0, '?', 'N/A', 'NA', 'nan', 'NaN', 'null'])   데이터 프레임 객체의 기본 내장함수  import pandas as pd df = pd.dataFrame(data)  # 데이터프레임 모양확인 df.shape  # 데이터 타입 확인 df.dtypes  # 인덱스 객체 반환 (인덱스는 각 행들의 이름) df.index  # features 의 인덱스 객체 반환 df.columns  # 전치형태의 데이터프레임객체 반환 (전치는 행열 반전) df.T  # 결측치 처리 df.fillna(0, inplace=True)  # 이 예시에서 결측치는 0으로 처리 및, inplace (default=false) 가 True이면 df원본을 손실하고 바로 변경된다. (false이면 결측치 처리된 df 수정본을 return할 뿐.)  # 컬럼 별 결측치 확인 df.isnull().sum()  # 요약통계량 확인 # count : 데이터의 수, mean/std : 평균/표준편차,  min/max : 최소/최대, 25%등 : 백분위수 df.describe()  # 각 feature들이 어떻게 분포되어있는지 한번에 확인가능!! ***** from pandas_profiling import ProfileReport df.profile_report()  데이터 프레임의 데이터 슬라이싱   # 열접근 df.NAME df['NAME'] # 둘은 같은 결과를 도출한다.  # loc 사용 # feature의 '이름' 으로 추출 df.loc[:,['NAME','GENDER']]  # iloc 사용 # 행과 열의 번호(순서)로 추출 df.iloc[2:4, 0:3]     # 이 예시에서 2이상 4미만 행, 0이상 3미만의 열을 추출  # 조건부 슬라이싱 df[df.GENDER == 'M']  # 이 예시에서 'GENDER' 라는 features 가 'M'인 행들을 추출   feature engineering  데이터들을 변형시키거나 특성들간 함수처리를 통해 내가 원하는 특성을 만들어내보자   # apply (중요!) df.apply(함수)              # 여기서 함수는 파이썬 내장함수일 수도, 사용자 정의함수일 수도 있다. lambda도 사용 가능 df.apply(lambda x: x+1)                     #lambda사용 예시 모든 데이터에 1을 더하겠다. df['AGE'] = df['AGE'].apply(lambda x:x+1)   # 모든 AGE에 1을더한걸 원본df에도 적용하겠다.  # 특성간 계산 df['GENDER_AGE'] = df['GENDER'] + df['AGE'].apply(str)  # GENDER가 'M'이고 나이가 23 이면 GENDER_AGE라는 feature는 'M23'을 가짐.      🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/Data/",
        "teaser": null
      },{
        "title": "정규화, 표준화 그리고 PCA",
        "excerpt":"정규화와 표준화     정규화 (normalization)    정규화는 모든 값들을 0과 1사이의 값으로 단순하게 축소한다.  예를 들어 0~100까지의 값중 35 는 0.35가 될 뿐이다.   식 : x = (원래값 - 최댓값) / (최댓값 - 최솟값)   from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler()  df_scaled = scaler.fit_transform(df)      표준화 (standardization)    표준화는 데이터를 0을 중심으로 양쪽으로 데이터를 분포시킨다.  정확히는 0의 평균을 갖고, 1의 표준편차를 갖도록 변환하는 것.   식 : x = (원래값 - 평균) / 표준편차   from sklearn.preprocessing import StandardScaler sclaer = StandardScaler()  df_scaled = scaler.fit_transform(df)      fit 과 fit transform 의 차이    fit은 해당 데이터에 맞춤으로 모델(객체)을 설정해주는 것.  fit_transform 은 fit함과 동시에 해당 데이터를 모델을 사용해서 변형해주어 return함.   분산과 공분산      분산 (variance)    하나의 feature가 갖는 ‘평균으로 부터 퍼져있는 정도’ 이다.  식 : Var(x) = E [(X-X평균)^2]   분산의 양의 제곱근이 표준편차(Standard Deviation)      공분산 (Covariance)    두 feature가 갖는 공동 변화량이다. 직관적으로는 이해하기 힘들더라.. 그 의미를 파악하자면  0보다크면 X가 증가할때 Y도 증가한다.(양의 상관관계)  0보 작으면 서로 음의 상관관계  식 : Cov(X,Y) = E[(X-X평균)(Y-Y평균)]      상관계수 (Correlation coefficient)    공분산과 자연스럽게 이어지는데, 상관계수는 얼만큼 상관관계를 갖는지도 알려준다.   식 : Corr(X,Y) = Cov(X,Y) / (sd(X)*sd(Y))  (sd는 표준편차)   PCA  표준화or정규화 필수!!!!  PCA는 고 차원(feature 종류가 많을 때)의 데이터셋을 차원축소 하고자 할 때 사용한다.  여기서 중요한건 PCA는 특정한 feature를 선택(selection)하는 것이 아니라  모든 feature의 특징을 담아내는 feature로 추출(Extraction) 한다는 것이다.   어떻게 Extraction할 것이냐 !! 바로 축을 고르는 것 이다.   어떠한 축에 모든 feature들을 projection시켰을 때, ‘가장 그 정보들을 많이 담는다’면 그 축은 모든 feature의 특징을  잘 담고 있는 축이 될 것이다.  ‘정보를 많이 담는다’는 것은 공분산이 가장 큰 것이다라고 이해했으며,  그 축에 projection시킨 값들의 집합하나가 하나의 차원이 될 것이다.  그리고 그 다음 축은 첫번째 축과 직교인 축으로 고르게 될 것이다.     features = df.loc[:,'bill_length_mm':'body_mass_g'] species = df['species']  ↪️ df에서 ‘bill_length_mm’부터 ‘body_mass_g’ 까지의 feature의 데이터만 가져오고, ‘species’만 가져온다.  import pandas as pd from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() features = pd.DataFrame(scaler.fit_transform(features), columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])  ↪️ 표준화를 진행한다. (필수!!!!!)  import numpy as np from sklearn.decomposition import PCA  pca = PCA(n_components=2) extracted_df = pd.DataFrame(pca.fit_transform(features), columns=['PC1', 'PC2'])  ↪️ pca를 실행한 후 ‘PC1’, ‘PC2’로 저장된(자동으로 이름 이렇게 지음) 두 차원만 불러온다.  두 차원으로 차원축소 성공 !     🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/pca/",
        "teaser": null
      },{
        "title": "신용카드 데이터로 k-means-cluster, pca 연습 ",
        "excerpt":"데이터 선정 :  kaggle credit customer dataset  (https://www.kaggle.com/arjunbhasin2013/ccdata)   from google.colab import drive drive.mount('/content/drive')   Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).   import pandas as pd import numpy as np df = pd.read_csv('/content/drive/MyDrive/dataset/CC GENERAL.csv.xls') print(df.shape) df.head()   (8950, 18)                                       CUST_ID       BALANCE       BALANCE_FREQUENCY       PURCHASES       ONEOFF_PURCHASES       INSTALLMENTS_PURCHASES       CASH_ADVANCE       PURCHASES_FREQUENCY       ONEOFF_PURCHASES_FREQUENCY       PURCHASES_INSTALLMENTS_FREQUENCY       CASH_ADVANCE_FREQUENCY       CASH_ADVANCE_TRX       PURCHASES_TRX       CREDIT_LIMIT       PAYMENTS       MINIMUM_PAYMENTS       PRC_FULL_PAYMENT       TENURE                       0       C10001       40.900749       0.818182       95.40       0.00       95.4       0.000000       0.166667       0.000000       0.083333       0.000000       0       2       1000.0       201.802084       139.509787       0.000000       12                 1       C10002       3202.467416       0.909091       0.00       0.00       0.0       6442.945483       0.000000       0.000000       0.000000       0.250000       4       0       7000.0       4103.032597       1072.340217       0.222222       12                 2       C10003       2495.148862       1.000000       773.17       773.17       0.0       0.000000       1.000000       1.000000       0.000000       0.000000       0       12       7500.0       622.066742       627.284787       0.000000       12                 3       C10004       1666.670542       0.636364       1499.00       1499.00       0.0       205.788017       0.083333       0.083333       0.000000       0.083333       1       1       7500.0       0.000000       NaN       0.000000       12                 4       C10005       817.714335       1.000000       16.00       16.00       0.0       0.000000       0.083333       0.083333       0.000000       0.000000       0       1       1200.0       678.334763       244.791237       0.000000       12                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             데이터 확인   # 결측치 확인 df.isnull().sum()   CUST_ID                               0 BALANCE                               0 BALANCE_FREQUENCY                     0 PURCHASES                             0 ONEOFF_PURCHASES                      0 INSTALLMENTS_PURCHASES                0 CASH_ADVANCE                          0 PURCHASES_FREQUENCY                   0 ONEOFF_PURCHASES_FREQUENCY            0 PURCHASES_INSTALLMENTS_FREQUENCY      0 CASH_ADVANCE_FREQUENCY                0 CASH_ADVANCE_TRX                      0 PURCHASES_TRX                         0 CREDIT_LIMIT                          1 PAYMENTS                              0 MINIMUM_PAYMENTS                    313 PRC_FULL_PAYMENT                      0 TENURE                                0 dtype: int64   # 데이터 타입 확인 df.dtypes   CUST_ID                              object BALANCE                             float64 BALANCE_FREQUENCY                   float64 PURCHASES                           float64 ONEOFF_PURCHASES                    float64 INSTALLMENTS_PURCHASES              float64 CASH_ADVANCE                        float64 PURCHASES_FREQUENCY                 float64 ONEOFF_PURCHASES_FREQUENCY          float64 PURCHASES_INSTALLMENTS_FREQUENCY    float64 CASH_ADVANCE_FREQUENCY              float64 CASH_ADVANCE_TRX                      int64 PURCHASES_TRX                         int64 CREDIT_LIMIT                        float64 PAYMENTS                            float64 MINIMUM_PAYMENTS                    float64 PRC_FULL_PAYMENT                    float64 TENURE                                int64 dtype: object   # 결측치 처리 (5%이하의 갯수이므로 드랍하겠다.) df.dropna(inplace=True) df.shape   (8636, 18)   import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings('ignore') #경고문을 무시한다.  i=1 plt.figure(figsize= (20,40)) for col in df.drop('CUST_ID', axis=1).columns:     plt.subplot(9,2,i)      sns.distplot(df[col])      i=i+1 plt.show()      # 상관 계수 (correlation coefficient) 확인  plt.figure(figsize=(10,10)) sns.heatmap(df.corr(), annot=True, cmap='coolwarm')   &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff838ba2890&gt;      표준화 및 PCA  +축소할 차원 수 결정.   # 상관관계가 어느정도 있는 feature가 보이므로 PCA 사용이 유의미할 것으로 판단. df.drop('CUST_ID', axis=1, inplace = True) from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler  # PCA전 표준화 ss= StandardScaler() df= ss.fit_transform(df)  #PCA 진행 pca= PCA() pca.fit(df)   PCA()   # PCA 의 축소 차원갯수에 대한 정확도 차이 (기존 차원수를 유지하는게 당연히 100%일것임..) plt.plot(pca.explained_variance_ratio_.cumsum())   [&lt;matplotlib.lines.Line2D at 0x7ff8393998d0&gt;]      # 차원 축소 7개로 하겠다! pca= PCA(n_components=7) df_pca= pca.fit_transform(df)   df_pca.shape   (8636, 7)   K-means Clustering  +K-means 와 실루엣 스코어(silhouette_score)      k-means : 각 데이터들과 해당 centroid까지 거리 합   sil_score : 1에 가까울수록 군집과 군집이 잘 분리되어있다는 뜻    기본적으로 0이상이고 만약 음수라면 군집끼리 겹쳤다는 의미   from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score plt.figure(figsize=(15,10)) distortions=[] sil_scores=[] for i in range(2,30):     # n_cluster : 군집 갯수, n_iter : 중심점 업데이트의 최소 횟수     kmeans= KMeans(n_clusters=i, n_init=10, init= 'k-means++', algorithm='full', max_iter=300)     kmeans.fit(df_pca)     # inertia_ : k-means 구하는 중에 centroid로부터 데이터들의 거리 데이터 (클 수록 중심점으로부터 멀다는 거겟쥐)     distortions.append(kmeans.inertia_)     label= kmeans.labels_     sil_scores.append(silhouette_score(df_pca, label)) plt.plot(np.arange(2,30,1), distortions, alpha=0.5) plt.plot(np.arange(2,30,1), distortions,'o' ,alpha=0.5) plt.show()      plt.figure(figsize=(15,10)) # sil_scores 확인 plt.plot(np.arange(2,30,1), sil_scores) plt.plot(np.arange(2,30,1), sil_scores,'o' ,alpha=1) plt.show()      sil_scores # k-means 는 군집을 늘릴수록 감소 (이상적) # 실루엣 계수는 3에서 0.28정도로 그나마 크나, 전체적으로는 0.25근처로 별로 크게 나오진 않음.   [0.24692450845604266,  0.28065750721461125,  0.2507007863496503,  0.2408710781955889,  0.25789671860000646,  0.25543152274952236,  0.26596171359898996,  0.25980372706530475,  0.2606025124276636,  0.24195602184083095,  0.25144013095228146,  0.23962807370519182,  0.23713138793264468,  0.23981648640294187,  0.2199707245532794,  0.2259075081873923,  0.23446181289456078,  0.21832255569020825,  0.24002990422966186,  0.23109689399081063,  0.22381807371604162,  0.21453215289991348,  0.21537478705286658,  0.21174420647359052,  0.2161573025700017,  0.21208142071199876,  0.2196422007478946,  0.20723949847328374]   # k-means 는 군집 3개로(실루엣계수 따라), PCA는 2개로하여 시각화.  kmeans= KMeans(n_clusters=3, n_init=10, init= 'k-means++', algorithm='full', max_iter=300) kmeans.fit(df_pca) labels= kmeans.labels_  pca= PCA(n_components=2) temp = pca.fit_transform(df) df_pca2 = pd.DataFrame(data=temp, columns=['pca1','pca2']) df_pca2['labels']= labels df_pca2.head()                                       pca1       pca2       labels                       0       -1.696397       -1.122594       2                 1       -1.215688       2.435597       1                 2       0.935860       -0.385170       2                 3       -1.614640       -0.724592       2                 4       0.223706       -0.783584       2                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             plt.figure(figsize=(10,10)) ax = sns.scatterplot(x='pca1', y='pca2', hue='labels', data=df_pca2, palette='bright')            🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["cluster","pca"],
        "url": "http://localhost:4000/pj/Credit_Card_Cluster/",
        "teaser": null
      },{
        "title": "Ridge 회귀와 로지스틱 회귀",
        "excerpt":"CV (교차 검증, cross validation)  먼저 데이터라고 하면 용도에 따라 3가지로 분류할 수 있다.  학습데이터(training_data),  검증데이터(validation_data),  테스트데이터(test_data)   세가지 데이터는 서로 누출되어서는 안되고,  학습데이터로 학습시킨 ‘모델’ 의  점수는 ‘검증데이터’로 체크를 하되,  마지막으로 타겟 데이터로써 테스트 데이터를 사용한다.   단, CV는 ‘검증데이터’를 따로 두지 않고,  학습데이터를 일부 추출하여 검증데이터로 사용한다는 방식이다.   만약 그저 데이터를 처음부터 ‘학습데이터’와 ‘검증데이터’로 나눈다면 아래와 같을 것이다.     그러나 CV를 이용하게 되면 아래와 같이 학습데이터와 검증데이터가 수시로 바뀌게 되며. 더 ‘일반화’ 된 모델을 얻을 수 있다.     릿지 회귀 (Ridge regression)  간단하게 Ridge 회귀는 단순회귀보다 더 일반화된 모델을 만든다고 이해해도 좋을 듯 하다.  즉, 단순선형회귀의 과적합 방지 모델.   from sklearn.linear_model import RidgeCV  # 수행해볼 알파 값들을 정의한다. alphas = [0.01, 0.05, 0.1, 0.2, 1.0, 10.0, 100.0]  # RidgeCV 모델 객체를 정의한다. ridge = RidgeCV(alphas=alphas, normalize=True, cv=3) ridge.fit(ans[['x']], ans['y'])  #결정된 알파와 베스트 스코어를 출력. print(\"alpha :\", ridge.alpha_) print(\"best score :\", ridge.best_score_)  # 예측해보고 싶은 X_test에 대해 y y_pred = ridge.predict(X_test)   로지스틱 회귀 (logistic Regression)  로지스틱 회귀는 sigmoid라는 ‘비선형’을 사용한 ‘2진 분류(classification)’ 모델이다.   예시 ) 환자들의 생체 데이터 + 암에 걸리지 않았다면 0, 걸렸다면 1을 갖는 feature (target)  from sklearn.linear_model import LogisticRegression  model = LogisticRegressionCV(penalty=\"l1\", Cs=[1.0], solver='liblinear', cv=3) model.fit(X, y) # 여기서 X는 여러 feature를 갖고 있을 수 있고, # y는 0이나 1의 값을 갖는 feature일 것이다!  # 스코어 프린트 print('best score: ', model.scores_)  # 테스트 데이터 확인 y_pred = model.predict(X_test) y_pred_proba = model.predict_proba(X_test) # proba는 y_pred를 결정했던 근거가 된 각 확률을 보여준다.  # 계수 확인 coefficients = pd.Series(model.coef_[0], X.columns) coefficients    # 계수가 -1부터 1사이의 값을 갖는데 -1에 가까운 값일 수록 0이 나오게 하는 feature임을 의미      sigmoid    x값이 어떤값이든지, y값은 0부터 1사이의 값을 비선형으로 갖도록 하는 함수       임계값 (Classification Threshold)    임계값이 로지스틱 회귀에서 등장했는데,  어떤 데이터 하나가 A에 속할 확률이 0.41이 나왔다고 했을때,  임계값이 0.5(default)라면 당연히 ‘A가 아니다’ 라고 하겠지만,  임계값을 0.4으로 조정한다면 ‘A 이다’ 라고 판단을 내리게 된다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/ridge_logistic/",
        "teaser": null
      },{
        "title": "특성 선택과 특성 추출",
        "excerpt":"특성을 줄이는데에는 어떤 방법이 있을까.  첫번째로 영향도 높은 특성을 ‘고르는 것’ 과  두번째는 특성 모두 특정 축에 ‘투영시키는 것’ 이 있다.   특성 선택 (feature selection)  특성을 줄이는데,   영향도가 큰 특성을 골라야  모델의 정확도를 유지하면서 복잡도를 줄이거나 일반화를 할 수 있을 것이다.   특성 추출 (feature extraction)  대표적으로 pca와 같이 특성들을   특정 축( 특성들의 특징을 잘 나타내야 한다는 이유로 보통 분산을 최대로하는 축을 선택한다. )에  투영(projection)시켜 차원을 축소하는 방식이 있다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/feature_Selection_Extraction/",
        "teaser": null
      },{
        "title": "파이프라인(PipeLine)과 결정트리모델(Decision Tree)",
        "excerpt":"사이킷런(sklearn) 파이프라인 (PipeLine)  사이킷런에서 제공하는 파이프라인은 각 기능을 하는 모델들을 한번에 묶는 기능과  하이퍼 파라미터를 연결시키는 기능이 있다.   from sklearn.pipeline import make_pipeline from category_encoders import OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.tree import DecisionTreeClassifier  pipe = make_pipeline(     OneHotEncoder(use_cat_names=True),       SimpleImputer(),     StandardScaler(),     DecisionTreeClassifier(random_state=1, criterion='entropy', min_samples_leaf=10, max_depth=6)      # min_samples_leaf는 말단 노드에 최소 존재해야할 데이터 수.     # max_depth 는 최대 깊이를 제한하여 복잡도를 개선. )  # pipe fit pipe.fit(x_train, y_train) print('검증세트 정확도', pipe.score(X_val, y_val))  # 테스트 셋 y_pred = pipe.predict(X_test)  # feature_importance 띄우기 ## 먼저 파이프 내의 학습된 모델들 떼어서 가져온다. model_dt = pipe.named_steps['decisiontreeclassifier'] enc = pipe.named_steps['onehotencoder']  encoded_columns = enc.transform(X_val).columns  importances = pd.Series(model_dt.feature_importances_, encoded_columns)   결정트리 (Decision_Tree)  결정트리는 ‘분류’에 있어서 마치 ‘회귀’의 선형회귀 와 같은 느낌이다.  데이터들을 계속해서 두가지씩 분류하여 결과적으로 모든 데이터들을 정해진 갯수의 class들로 분류하게 된다.     결정트리를 발전시킨  ‘랜덤포레스트 (Random_Forest)’,  ‘그래디언트 부스트 트리 (Gradient Boosted Tree)\u001c’  같은 모델들을 더 많이 사용할 것이다.  그러나 그 기초는 결정트리에 있다.      트리학습에서의 비용함수       지니지수 (Gini Impurity   엔트로피 (Entropy)   두가지 모두 불순도를 나타내는 척도 이며  클수록 골고루 섞여 있다는 뜻.(10개의 공들 중에 5개씩 빨간공, 파란공이라면 0.5) 즉, 0에 가까울수록 치우쳐져 있다는 뜻. (전부 특정공만 10개 있다면 0)   즉 지니지수, 엔트로피가 작아지는 방향으로 트리 노드를 생성한다.       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/decision_tree/",
        "teaser": null
      },{
        "title": "랜덤포레스트(Random Forest)와 ...",
        "excerpt":"랜덤 포레스트 모델  단순 선형회귀와 릿지(Ridge) 회귀가 있었다면,  결정트리와 랜덤포레스트가 있다.   릿지 회귀에서 과적합을 방지하는 장치가 있었다.  랜덤포레스트 또한 결정트리에서 더 일반화시켜주는 장치가 있다.   결정트리를 여러개 만들어 모두의 의견을 합산하여 판단을 내린다.      숲(포레스트)이 만들어지는 과정       많은 feature중에서 n개 (pram: max_features) 를 ‘랜덤’으로 고른다.   n개의 feature중 가장 영향도가 큰 feature를 골라 첫번째 node를 생성하고, 나머지 feature 중 랜덤하게 골라 트리를 완성한다.   위와같은 트리를 m개(pram: n_estimators)만든다.   트리들의 분류 결과로 투표를 해서 최종 결정을 한다.   from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score  # bootstrap을 False로 하고 max_features 수정가능. classifier = RandomForestClassifier(n_estimators = 50, bootstrap = False, max_features = 5)  # 모델 fit classifier.fit(X_train, y_train)  # 결과값 예측 y_pred = classifier.predict(X_test)  # 같은지 다른지 확인. print(\"정확도 : {}\".format(accuracy_score(y_test, y_pred))      CV(cross validation)은 GridsearchCV, RandomizedSearchCV 등을 이용.    # CV(cross validation)은 gridsearch 등을 이용. from sklearn.model_selection import GridSearchCV  grid = {     'n_estimators' : [100,200],     'max_depth' : [6,8,10,12],     'min_samples_leaf' : [3,5,7,10],     'min_samples_split' : [2,3,5,10] }  classifier_grid = GridSearchCV(classifier, param_grid = grid, scoring=\"accuracy\", n_jobs=-1, verbose =1)  classifier_grid.fit(X_train, y_train)  print(\"최고 평균 정확도 : {}\".format(classifier_grid.best_score_)) print(\"최고의 파라미터 :\", classifier_grid.best_params_)     feature_importances 내부변수로 확인가능    import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  feature_importances = model.feature_importances_  ft_importances = pd.Series(feature_importances, index = X_train.columns) ft_importances = ft_importances.sort_values(ascending=False)  plt.figure(fig.size(12,10)) sns.barplot(x=ft_importances, y= X_train.columns) plt.show()       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/random_forest/",
        "teaser": null
      },{
        "title": "이진분류의 평가지표",
        "excerpt":"이진분류 모델의 종류  로지스틱 회귀,  Decision_Tree(base model),  RandomForest,  Gradient_boost,  KNN(K-Nearest-Neighbors),  그외 딥러닝 모델도 될 수 있겠다.   평가지표의 종류  위와 같은 이진분류 모델 (군집x, 지도학습의 분류o) 의 평가지표는 어떤 게 있을까.  (회귀에서는 MSE, MAE, RMS, R-score 등이 있었쥬!)   가장 단순하게 볼 수 있는 것은 validation 데이터에 대해서 얼마나 많이 맞춘 비율(0~1)이 있을 것이다.  바로 정확도(Accuracy)!    그러나 모델을 하나의 값으로 평가하면 안될 것이다.      정확도 (Accuracy)  먼저 정확도를 체크할 때에도 베이스 모델은 반드시 필요하다.  그 이유를 예로 들어보자면,  암을 예측하는 모델의 학습데이터에서 암에 걸린 데이터(1)가 5%, 나머지 95%가 건강(0)하다면,  입력에 상관없이 항상 출력을 ‘0’으로만 한다면 그 모델은  CV나 hold-out 으로 비슷한 비율로 생성된 validation 데이터에서도 정확도를 대략 0.95로 가질 것이다..!  이러한 모델은 정확도 검증에 있어서 0.95는 최소한 넘는 것으로 고려해야 할 것이다.       TP, TN, FP, FN  (T,F는 True, False,   P,N은 Positive, Negative,)  True는 맞춘거, False는 못맞춘거.  P와 N은 예측한 거 기준. (FP는 실제 Negative인데 모델이 Positive로 예측한거야)       정밀도(precision)와 재현율(recall, sensitivity), specificity(TN-rate), Fall-out  precision = TP/(TP+FP) = Positive로 예측한 것들중 맞춘비율  recall = TP/(TP+FN) = 실제 Positive 중 맞춘비율  (TPRate) specificity = TN/(TN+FP) = 실제 Negative 중 맞춘비율 (TNR) Fall-out = FP/(FP+TN) = 실제 Negative 중 틀린비율 (FPR)       confusion Metrics  TP, TN, FP, FN 을 한번에 나타낸 행렬  아래 사진을 참고하면 한번에 이해될것.ㅎㅎ    from sklearn.metrics import plot_confusion_matrix import matplotlib.pyplot as plt  # pipe는 파이프라인 fig, ax = plt.subplots() matrix = plot_confusion_matrix(pipe, X_val_cleaned, y_val,                             cmap = plt.cm.Blues,                             ax = ax);        AUC, ROC 와 임계값(Threshold)  ROC 커브는 임계값에 따른 FPR과 TPR 값이 그리는 커브인데. 모두 일대일 관계이다.   AOC수치는 ROC curve의 아래 면적을 뜻한다.  AOC는 클수록 좋은 모델인 것은 사실이다.  이유 : FPR (틀린비율, 낮을수록 좋음) 이 낮음에도 TPR(맞춘비율, 높을수록 좋음)은 높아야 AOC가 높은 것이기 때문.  결과적으로 tpr-fpr이 최대가 되는 점의 임계점을 고르는게 최선이라고 할 수 있다.             🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["Something_else"],
        "tags": [],
        "url": "http://localhost:4000/something_else/_classfication_metrics/",
        "teaser": null
      },{
        "title": "크롤링(Crawling)",
        "excerpt":"크롤링이란 ?  web에 있는 데이터들을 긁어모으는 것을 말한다.  크게 두 종류로 먼저 나눌 수 있다.     정적크롤링 : 항상 같은 값을 주는 HTML로 부터 파싱을 해서 크롤링.   동적크롤링 : 같은 HTML이라도 동작, 명령을 통해 변화된 상태에서 데이터들을 크롤링.   정적크롤링은 멈춰있는 페이지에서 정보를 찾아 긁어모은다면,  동적크롤링은 검색, 스크롤, 페이지 클릭 등을 해서 나오는 정보를 긁어모을 수 있다.   대표적으로 정적크롤링 관련 라이브러리로 beautifulsoup(bs4)가 있다.  그리고 동적크롤링의 방법에도 여러 종류가 있는데 그 중 2가지를 적어보자면 다음과 같다.     openAPI를 이용하여 명령 후, response된 정보로부터 크롤링.   selenium을 통해 webdriver (크롬, 사파리 등)를 제어한 후 나온 페이지(HTML)로부터 크롤링.   크롤링 예시  먼저 아래 함수 두개를 지정하겠다.     createDirectory : 입력값으로 받은 문자열(경로)에 해당하는 폴더를 생성한다.   crawling_img : 입력값으로 받은 문자열을 크롬에서 검색해서 함수내에 지정되어 있는(직접변경) 경로로 이미지를 저장. (이름은 번호순으로 증가)   from selenium import webdriver from selenium.webdriver.common.keys import Keys import time import urllib.request import os   def createDirectory(directory):     try:         if not os.path.exists(directory):             os.makedirs(directory)     except OSError:         print(\"Error: Failed to create the directory.\")  def crawling_img(name):     ## 입력값 name 문자열을 검색하여 나오는 이미지를 저장하는 함수.     ## 저장하는 경로는 함수 내에서 별도로 지정해야함.      # 크롬을 드라이버로 채택. 버전오류가 날 수 있다.     driver = webdriver.Chrome()     driver.get(\"https://www.google.co.kr/imghp?hl=ko&amp;tab=wi&amp;authuser=0&amp;ogbl\")      # q로 태그되어 있는 곳이 구글홈페이지의 검색창이다.     elem = driver.find_element_by_name(\"q\")     elem.send_keys(name)     elem.send_keys(Keys.RETURN)       SCROLL_PAUSE_TIME = 1   # 1초씩 기다렸다가 내렸다를 반복할거임.       # Get scroll height     last_height = driver.execute_script(\"return document.body.scrollHeight\")  # 브라우저의 높이를 자바스크립트로 찾음     while True:         # Scroll down to bottom         driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # 브라우저 끝까지 스크롤을 내림         # Wait to load page         time.sleep(SCROLL_PAUSE_TIME)         # Calculate new scroll height and compare with last scroll height         new_height = driver.execute_script(\"return document.body.scrollHeight\")         if new_height == last_height:             try:                 # 더보기 버튼을 클릭할 거임.                 driver.find_element_by_css_selector(\".mye4qd\").click()             except:                 # 더보기 버튼이 없어서 클릭을 못하면 끝.                 break         last_height = new_height      imgs = driver.find_elements_by_css_selector(\".rg_i.Q4LuWd\")      # 경로와 폴더 명 지정.     dir = \".\\tree_flower_dog_cat\" + \"\\\\\" + name      createDirectory(dir) #폴더 생성     count = 1     for img in imgs:         try:             img.click()             time.sleep(3)             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img').get_attribute(                 \"src\")             path = \".\\idols\\\\\" + name + \"\\\\\"             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \".jpg\")              # 이 아래는 관련이미지 저장             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[1]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_1\" + \".jpg\")             imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[2]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_2\" + \".jpg\")              imgUrl = driver.find_element_by_xpath(                 '//*[@id=\"Sva75c\"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[3]/a[1]/div[1]/img').get_attribute(                 \"src\")             urllib.request.urlretrieve(imgUrl, path + name + \"_\"+ str(count) + \"_3\" + \".jpg\")              count = count + 1             if count &gt;= 500:                 break         except:             pass     driver.close()   이제 위 함수를 사용하여 for문을통해 검색 및 저장을 동시에 해주면 된다.   searching_keyword = [\"나무\", \"꽃\", \"강아지얼굴\", \"고양이얼굴\"]  for i in range(len(searching_keyword)) :     searching_keyword[i] += '_사진' # 사진을 뒤에 붙이면 검색이 잘될 것 같아!  for keyword in searching_keyword:     crawling_img(keyword)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["de"],
        "tags": [],
        "url": "http://localhost:4000/de/crawling/",
        "teaser": null
      },{
        "title": "데이터 인코딩(encoding)",
        "excerpt":"인코딩은 왜 하는거?  데이터에는 연속적인 숫자만 있는게 아니다.  문자열의 카테고리형 feature일 수도,  숫자라 하더라도 비연속적인 카테고리형 feature도 있다.  (예: 사는지역(“수유동”, “인수동” …), 평점(“매우별로”,”별로”, … , “매우좋음”))   인코딩은 이러한 카테고리형 feature들에 대하여 ‘데이터’로써 유의미하도록 숫자로 바꿔주는 역할을 한다.   인코딩의 종류와 간단 정리  본래 데이터        One Hot Encoding  하나의 feature가 갖는 범주 전체에 대하여 ‘이다’, ‘아니다’로 분류하여 0, 1을 갖는 feature를 생성.  (사는지역의 종류가 “수유동”, “쌍문동” 등 16개의 동이 있다면, 16개의 feature (사는지역_수유동, 사는지역_쌍문동… 등)     pd.get_dummies(df, prefix=[\"지  \"], columns=[\"사는지역\"]) #또는 아래처럼 sklern.preprocessing의 함수 사용 from sklearn.preprocessing import OneHotEncoder                 Ordinal Encoding categorical_feature의 값들이 어떤 ‘순서’를 갖고있을 때 사용한다.  (‘매우 그렇다’, ‘그렇다’, ‘보통’, ‘아니다’, ‘매우 아니다’) 같은거!     from sklearn.preprocessing import OrdinalEncoder enc = OrdinalEncoder(categories = [['불만족', '보통', '만족']]) df['선호도_enc']=enc.fit_transform(df[['선호도']])                 Binary Encoding OneHotEncoding의 이진수 버전이라고 이해했다.  사는지역의 종류가 16개의 동이 있다면. 4자리 이진수로 표현이 가능하므로 4개의 feature를 생성 한다.(cardinality가 너무 큰 특성에 대해서 사용하면 장점일 듯!)  ‘수유동은’ 4개의 feature에 0이나 1이 채워져 0000부터 1111중 하나의 형태를 가질 것!     import category_encoders as ce encoder = ce.BinaryEncoder(cols=['사는지역']) dfbin = encoder.fit_transform(df['사는지역']) df = pd.concat([df, dfbin], axis=1) df.drop(['선호도','성별(1남, 0여)'], axis=1)                 Frequency Encoding 빈도로써 표현하는 방법  ‘수유동’이 10개 데이터중 2번있다면 0.2로 매핑됨.     # Frequency Encoding fe = df.groupby(\"사는지역\").size()/len(df) df.loc[:, \"사는지역_freq_encode\"] = df[\"사는지역\"].map(fe) df                      Mean Encoding  여기부터는 지도학습에만 해당하는 내용이라고 생각한다. ‘target’ 이 존재할 때만 가능하기 때문!  어떻게 인코딩을 시킬 지를 ‘타겟의 평균값’에 따라 결정한다.       먼저, 단순하게 target의 평균자체로 매핑을 하는 방법이 있는데,  (target을 gender라 하면, 수유동에사는 사람들의 target평균을 수유동의 인코딩 매핑값으로 삼는다.)  과적합이 되기 쉽다.  ————————————————————————————–       두번째로, smoothing mean target encoding은 과적합을 좀더 방지한다.  (수유동 사람들의 target평균 / weight) + (전체 target평균 / 수유동의 갯수)   의미 : weight로 개별평균을 분산시켜주고, 수유동의 갯수가 클수록 수유동평균에 힘을, 수유동 갯수가 적다면 전체평균의 힘을 실어준다.  weight가 클수록 편차가 작아진다!       # smoothing target encoding  # 1. 평균을 계산  mean = df['성별(1남, 0여)'].mean()  # 2. 각 그룹에 대한 값들의 빈도와 평균을 계산  Agg = df.groupby('사는지역')['성별(1남, 0여)'].agg(['count', 'mean'])  counts = Agg['count']  means = Agg['mean']  weight = 10  # 3. “smooth”한 평균을 계산  smooth = (counts * means + weight * mean) / (counts + weight)  # smooth한 평균에 따라 각 값을 대체하는 것  print(smooth)  df.loc[:, '사는지역_smean_enc'] = df['사는지역'].map(smooth)  df                 Probability Ratio Encoding (확률비율 인코딩)  (target이 1인 확률 / target이 0인 확률) 의 비율로 매핑을 하는 방법이다.  주의할 점은 0으로나뉘는걸 꼭 방지하자!    (예 : ‘수유동’들 중에 target이 1인갯수는 3개, 0인갯수는 1개라면, ‘수유동’은 3으로 매핑됨.)   Weight of Evidence Encoding  위 PRE의 비율에 log_2를 취하고 weight을 곱해준 것으로 인코딩하는 방법.  (6번 인코딩 예시에서 ‘수유동’은 w*ln(3)의 값으로 매핑 될 것.)       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["ds"],
        "tags": [],
        "url": "http://localhost:4000/ds/encoding/",
        "teaser": null
      },{
        "title": "사람얼굴에 스타일 바꿔보기",
        "excerpt":"목표  임의의 초상권 없는 얼굴을 특정 스타일을 입혀 만들기.   방법  임시로 생성한 사람얼굴(실존x) 이미지에 원하는 특정 스타일을 가진 사람(실존o)의 이미지를 적용하여 새롭게 생성.      styleGAN2-ada 모델의 ffhq preTrained 가중치를 사용하여 얼굴생성.   원하는 스타일의 사람사진의 스타일 벡터들을 추출 (PSP 모델의 일부 사용)   1번에서 생성한 이미지를 inversion하여 다시 이미지를 생성하는 과정에서 2번의 스타일 벡터들을 inject하여 최종 생성.      StyleGAN2 - ada 간단 요약  GAN (latent vector로 부터 이미지 생성) 모델 중에서  styleGAN (latent vector를 style 별로 생성된 여러 w-vector로 만들어 이미지 생성) 이 있다.   ada는 데이터 증강기법(적은 데이터로 다양한 데이터생성, 일반화 효과 + 데이터 수 늘리는 효과)   PSP(pixel2style2pixel) 간단 요약  구조를 두 부분으로 나눌 수 있는데,     psp encoder : 이미지를 매핑하여 w-vector를 생성함.   styleGAN generator  : w-vector를 사용하여 이미지를 생성(styleGAN 방식과 같이 해상도를 올리면서 이미지 생성.)   이 구조를 이용해서 여러 기능으로 사용가능 (ffhq_encode, celeb_seg_to_face, toonify 등)   test 이미지  styleGAN2-ada, ffhq-pretrained 로 생성한 이미지 들          결과 이미지     스타일이미지         결과 이미지  순서대로 “스타일 적용전” ::::::: “w벡터 중 랜덤으로 1 개만 적용” ::::::: “모든 w벡터를 적용한 경우”                   결론   결과를 해석하자면     스타일이미지의 얼굴형, 머리스타일 등에 원본이미지의 눈코입, 피부 등을 적용시켜 이미지를 생성한다.   랜덤한 w벡터를 적용했을 때, 스타일 이미지의 정확히 어떤 특징을 담은 w벡터가 적용된 것인지 알기 힘들다..  (물론 스타일이미지와 무언가 닮아지긴 한다.)   성공적인가의 여부는 스타일 이라는 애매한 단어에 어떤 걸 포함시키느냐에 따라 해석이 다를 것 같다.  얼굴형, 머리스타일을 ‘스타일’이라고 한다면, 성공에 가깝다고 볼 수 있겠다!  다만 원하는 스타일(머리스타일, 얼굴형 중 하나를 택하고 싶은 경우)을 적용하는데에는 실험적인 시도가 필요해 보인다.   레포지토리 링크  https://github.com/chan9480/Style_image_GAN       🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄   맨 위로 이동하기  ","categories": ["pj"],
        "tags": ["styleGAN2","PSP"],
        "url": "http://localhost:4000/pj/stylegan/",
        "teaser": null
      }]
