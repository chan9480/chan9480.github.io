<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-02-28T17:52:24+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">배우자 그리고 써먹자</title><subtitle>포트폴리오</subtitle><author><name>옹달샘👱🏼‍♂️</name></author><entry><title type="html">랜덤포레스트(Random Forest)와 …</title><link href="http://localhost:4000/ds/random_forest/" rel="alternate" type="text/html" title="랜덤포레스트(Random Forest)와 …" /><published>2022-02-28T00:00:00+09:00</published><updated>2022-02-28T00:00:00+09:00</updated><id>http://localhost:4000/ds/random_forest</id><content type="html" xml:base="http://localhost:4000/ds/random_forest/"><![CDATA[<h2 id="랜덤-포레스트-모델">랜덤 포레스트 모델</h2>
<p>단순 선형회귀와 릿지(Ridge) 회귀가 있었다면,<br />
결정트리와 랜덤포레스트가 있다.</p>

<p>릿지 회귀에서 과적합을 방지하는 장치가 있었다.<br />
랜덤포레스트 또한 결정트리에서 더 일반화시켜주는 장치가 있다.</p>

<p>결정트리를 여러개 만들어 모두의 의견을 합산하여 판단을 내린다.</p>

<blockquote>
  <p>숲(포레스트)이 만들어지는 과정</p>
</blockquote>

<ol>
  <li>많은 feature중에서 n개 (pram: max_features) 를 ‘랜덤’으로 고른다.</li>
  <li>n개의 feature중 가장 영향도가 큰 feature를 골라 첫번째 node를 생성하고, 나머지 feature 중 랜덤하게 골라 트리를 완성한다.</li>
  <li>위와같은 트리를 m개(pram: n_estimators)만든다.</li>
  <li>트리들의 분류 결과로 투표를 해서 최종 결정을 한다.</li>
</ol>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# bootstrap을 False로 하고 max_features 수정가능.
classifier = RandomForestClassifier(n_estimators = 50, bootstrap = False, max_features = 5)

# 모델 fit
classifier.fit(X_train, y_train)

# 결과값 예측
y_pred = classifier.predict(X_test)

# 같은지 다른지 확인.
print("정확도 : {}".format(accuracy_score(y_test, y_pred))
</code></pre></div></div>

<blockquote>
  <p>CV(cross validation)은 gridsearch 등을 이용.</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># CV(cross validation)은 gridsearch 등을 이용.
from sklearn.model_selection import GridSearchCV

grid = {
    'n_estimators' : [100,200],
    'max_depth' : [6,8,10,12],
    'min_samples_leaf' : [3,5,7,10],
    'min_samples_split' : [2,3,5,10]
}

classifier_grid = GridSearchCV(classifier, param_grid = grid, scoring="accuracy", n_jobs=-1, verbose =1)

classifier_grid.fit(X_train, y_train)

print("최고 평균 정확도 : {}".format(classifier_grid.best_score_))
print("최고의 파라미터 :", classifier_grid.best_params_)
</code></pre></div></div>
<blockquote>
  <p>feature_importances 내부변수로 확인가능</p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

feature_importances = model.feature_importances_

ft_importances = pd.Series(feature_importances, index = X_train.columns)
ft_importances = ft_importances.sort_values(ascending=False)

plt.figure(fig.size(12,10))
sns.barplot(x=ft_importances, y= X_train.columns)
plt.show()
</code></pre></div></div>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="ds" /><summary type="html"><![CDATA[랜덤 포레스트 모델 단순 선형회귀와 릿지(Ridge) 회귀가 있었다면, 결정트리와 랜덤포레스트가 있다.]]></summary></entry><entry><title type="html">파이프라인(PipeLine)과 결정트리모델(Decision Tree)</title><link href="http://localhost:4000/ds/decision_tree/" rel="alternate" type="text/html" title="파이프라인(PipeLine)과 결정트리모델(Decision Tree)" /><published>2022-02-26T00:00:00+09:00</published><updated>2022-02-26T00:00:00+09:00</updated><id>http://localhost:4000/ds/decision_tree</id><content type="html" xml:base="http://localhost:4000/ds/decision_tree/"><![CDATA[<h2 id="사이킷런sklearn-파이프라인-pipeline">사이킷런(sklearn) 파이프라인 (PipeLine)</h2>
<p>사이킷런에서 제공하는 파이프라인은 각 기능을 하는 모델들을 한번에 묶는 기능과<br />
하이퍼 파라미터를 연결시키는 기능이 있다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.pipeline import make_pipeline
from category_encoders import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier

pipe = make_pipeline(
    OneHotEncoder(use_cat_names=True),  
    SimpleImputer(),
    StandardScaler(),
    DecisionTreeClassifier(random_state=1, criterion='entropy', min_samples_leaf=10, max_depth=6)

    # min_samples_leaf는 말단 노드에 최소 존재해야할 데이터 수.
    # max_depth 는 최대 깊이를 제한하여 복잡도를 개선.
)

# pipe fit
pipe.fit(x_train, y_train)
print('검증세트 정확도', pipe.score(X_val, y_val))

# 테스트 셋
y_pred = pipe.predict(X_test)

# feature_importance 띄우기
## 먼저 파이프 내의 학습된 모델들 떼어서 가져온다.
model_dt = pipe.named_steps['decisiontreeclassifier']
enc = pipe.named_steps['onehotencoder']

encoded_columns = enc.transform(X_val).columns

importances = pd.Series(model_dt.feature_importances_, encoded_columns)
</code></pre></div></div>

<h2 id="결정트리-decision_tree">결정트리 (Decision_Tree)</h2>
<p>결정트리는 ‘분류’에 있어서 마치 ‘회귀’의 선형회귀 와 같은 느낌이다.<br />
데이터들을 계속해서 두가지씩 분류하여 결과적으로 모든 데이터들을 정해진 갯수의 class들로 분류하게 된다.</p>

<p><img src="/assets/images/source_22.png" width="60%" height="60%" title="제목" alt="아무거나" /><br />
결정트리를 발전시킨<br />
‘랜덤포레스트 (Random_Forest)’,<br />
‘그래디언트 부스트 트리 (Gradient Boosted Tree)’<br />
같은 모델들을 더 많이 사용할 것이다.<br />
그러나 그 기초는 결정트리에 있다.</p>

<blockquote>
  <p>트리학습에서의 비용함수</p>
</blockquote>

<ol>
  <li>지니지수 (Gini Impurity</li>
  <li>엔트로피 (Entropy)</li>
</ol>

<p>두가지 모두 불순도를 나타내는 척도 이며<br />
클수록 골고루 섞여 있다는 뜻.(10개의 공들 중에 5개씩 빨간공, 파란공이라면 0.5)
즉, 0에 가까울수록 치우쳐져 있다는 뜻. (전부 특정공만 10개 있다면 0)</p>

<p>즉 지니지수, 엔트로피가 작아지는 방향으로 트리 노드를 생성한다.</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="ds" /><summary type="html"><![CDATA[사이킷런(sklearn) 파이프라인 (PipeLine) 사이킷런에서 제공하는 파이프라인은 각 기능을 하는 모델들을 한번에 묶는 기능과 하이퍼 파라미터를 연결시키는 기능이 있다.]]></summary></entry><entry><title type="html">Ridge 회귀와 로지스틱 회귀</title><link href="http://localhost:4000/ds/ridge_logistic/" rel="alternate" type="text/html" title="Ridge 회귀와 로지스틱 회귀" /><published>2022-02-25T00:00:00+09:00</published><updated>2022-02-25T00:00:00+09:00</updated><id>http://localhost:4000/ds/ridge_logistic</id><content type="html" xml:base="http://localhost:4000/ds/ridge_logistic/"><![CDATA[<h2 id="cv-교차-검증-cross-validation">CV (교차 검증, cross validation)</h2>
<p>먼저 데이터라고 하면 용도에 따라 3가지로 분류할 수 있다.<br />
학습데이터(training_data),<br />
검증데이터(validation_data),<br />
테스트데이터(test_data)</p>

<p>세가지 데이터는 서로 누출되어서는 안되고,<br />
학습데이터로 학습시킨 ‘모델’ 의<br />
점수는 ‘검증데이터’로 체크를 하되,<br />
마지막으로 타겟 데이터로써 테스트 데이터를 사용한다.</p>

<p>단, CV는 ‘검증데이터’를 따로 두지 않고,<br />
학습데이터를 일부 추출하여 검증데이터로 사용한다는 방식이다.</p>

<p>만약 그저 데이터를 처음부터 ‘학습데이터’와 ‘검증데이터’로 나눈다면 아래와 같을 것이다.<br />
<img src="/assets/images/source_19.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<p>그러나 CV를 이용하게 되면 아래와 같이 학습데이터와 검증데이터가 수시로 바뀌게 되며. 더 <strong>‘일반화’</strong> 된 모델을 얻을 수 있다.<br />
<img src="/assets/images/source_20.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<h2 id="릿지-회귀-ridge-regression">릿지 회귀 (Ridge regression)</h2>
<p>간단하게 Ridge 회귀는 단순회귀보다 더 일반화된 모델을 만든다고 이해해도 좋을 듯 하다.<br />
즉, 단순선형회귀의 과적합 방지 모델.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.linear_model import RidgeCV

# 수행해볼 알파 값들을 정의한다.
alphas = [0.01, 0.05, 0.1, 0.2, 1.0, 10.0, 100.0]

# RidgeCV 모델 객체를 정의한다.
ridge = RidgeCV(alphas=alphas, normalize=True, cv=3)
ridge.fit(ans[['x']], ans['y'])

#결정된 알파와 베스트 스코어를 출력.
print("alpha :", ridge.alpha_)
print("best score :", ridge.best_score_)

# 예측해보고 싶은 X_test에 대해 y
y_pred = ridge.predict(X_test)
</code></pre></div></div>

<h2 id="로지스틱-회귀-logistic-regression">로지스틱 회귀 (logistic Regression)</h2>
<p>로지스틱 회귀는 sigmoid라는 ‘비선형’을 사용한 ‘2진 분류(classification)’ 모델이다.</p>

<p>예시 ) 환자들의 생체 데이터 + 암에 걸리지 않았다면 0, 걸렸다면 1을 갖는 feature (target)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.linear_model import LogisticRegression

model = LogisticRegressionCV(penalty="l1", Cs=[1.0], solver='liblinear', cv=3)
model.fit(X, y)
# 여기서 X는 여러 feature를 갖고 있을 수 있고,
# y는 0이나 1의 값을 갖는 feature일 것이다!

# 스코어 프린트
print('best score: ', model.scores_)

# 테스트 데이터 확인
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test) # proba는 y_pred를 결정했던 근거가 된 각 확률을 보여준다.

# 계수 확인
coefficients = pd.Series(model.coef_[0], X.columns)
coefficients    # 계수가 -1부터 1사이의 값을 갖는데 -1에 가까운 값일 수록 0이 나오게 하는 feature임을 의미
</code></pre></div></div>

<blockquote>
  <p>sigmoid</p>
</blockquote>

<p>x값이 어떤값이든지, y값은 0부터 1사이의 값을 비선형으로 갖도록 하는 함수
<img src="/assets/images/source_21.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<blockquote>
  <p>임계값 (Classification Threshold)</p>
</blockquote>

<p>임계값이 로지스틱 회귀에서 등장했는데,<br />
어떤 데이터 하나가 A에 속할 확률이 0.41이 나왔다고 했을때,<br />
임계값이 0.5(default)라면 당연히 ‘A가 아니다’ 라고 하겠지만,<br />
임계값을 0.4으로 조정한다면 ‘A 이다’ 라고 판단을 내리게 된다.</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="ds" /><summary type="html"><![CDATA[CV (교차 검증, cross validation) 먼저 데이터라고 하면 용도에 따라 3가지로 분류할 수 있다. 학습데이터(training_data), 검증데이터(validation_data), 테스트데이터(test_data)]]></summary></entry><entry><title type="html">특성 선택과 특성 추출</title><link href="http://localhost:4000/something_else/feature_Selection_Extraction/" rel="alternate" type="text/html" title="특성 선택과 특성 추출" /><published>2022-02-25T00:00:00+09:00</published><updated>2022-02-25T00:00:00+09:00</updated><id>http://localhost:4000/something_else/feature_Selection_Extraction</id><content type="html" xml:base="http://localhost:4000/something_else/feature_Selection_Extraction/"><![CDATA[<p>특성을 줄이는데에는 어떤 방법이 있을까.<br />
첫번째로 영향도 높은 특성을 <strong>‘고르는 것’</strong> 과<br />
두번째는 특성 모두 특정 축에 <strong>‘투영시키는 것’</strong> 이 있다.</p>

<h2 id="특성-선택-feature-selection">특성 선택 (feature selection)</h2>
<p>특성을 줄이는데, <br />
영향도가 큰 특성을 골라야<br />
모델의 정확도를 유지하면서 복잡도를 줄이거나 일반화를 할 수 있을 것이다.</p>

<h2 id="특성-추출-feature-extraction">특성 추출 (feature extraction)</h2>
<p>대표적으로 pca와 같이 특성들을 <br />
특정 축( 특성들의 특징을 잘 나타내야 한다는 이유로 보통 분산을 최대로하는 축을 선택한다. )에<br />
투영(projection)시켜 차원을 축소하는 방식이 있다.</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="Something_else" /><summary type="html"><![CDATA[특성을 줄이는데에는 어떤 방법이 있을까. 첫번째로 영향도 높은 특성을 ‘고르는 것’ 과 두번째는 특성 모두 특정 축에 ‘투영시키는 것’ 이 있다.]]></summary></entry><entry><title type="html">신용카드 데이터로 k-means-cluster, pca 연습</title><link href="http://localhost:4000/pj/Credit_Card_Cluster/" rel="alternate" type="text/html" title="신용카드 데이터로 k-means-cluster, pca 연습" /><published>2022-02-21T00:00:00+09:00</published><updated>2022-02-23T00:00:00+09:00</updated><id>http://localhost:4000/pj/Credit_Card_Cluster</id><content type="html" xml:base="http://localhost:4000/pj/Credit_Card_Cluster/"><![CDATA[<h1 id="데이터-선정-">데이터 선정 :</h1>
<p>kaggle credit customer dataset  (https://www.kaggle.com/arjunbhasin2013/ccdata)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="p">.</span><span class="n">mount</span><span class="p">(</span><span class="s">'/content/drive'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/content/drive/MyDrive/dataset/CC GENERAL.csv.xls'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8950, 18)
</code></pre></div></div>

<div id="df-0239163e-fb5c-444a-8b2b-314b7f2a8a42">
    <div class="colab-df-container">
      <div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CUST_ID</th>
      <th>BALANCE</th>
      <th>BALANCE_FREQUENCY</th>
      <th>PURCHASES</th>
      <th>ONEOFF_PURCHASES</th>
      <th>INSTALLMENTS_PURCHASES</th>
      <th>CASH_ADVANCE</th>
      <th>PURCHASES_FREQUENCY</th>
      <th>ONEOFF_PURCHASES_FREQUENCY</th>
      <th>PURCHASES_INSTALLMENTS_FREQUENCY</th>
      <th>CASH_ADVANCE_FREQUENCY</th>
      <th>CASH_ADVANCE_TRX</th>
      <th>PURCHASES_TRX</th>
      <th>CREDIT_LIMIT</th>
      <th>PAYMENTS</th>
      <th>MINIMUM_PAYMENTS</th>
      <th>PRC_FULL_PAYMENT</th>
      <th>TENURE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>C10001</td>
      <td>40.900749</td>
      <td>0.818182</td>
      <td>95.40</td>
      <td>0.00</td>
      <td>95.4</td>
      <td>0.000000</td>
      <td>0.166667</td>
      <td>0.000000</td>
      <td>0.083333</td>
      <td>0.000000</td>
      <td>0</td>
      <td>2</td>
      <td>1000.0</td>
      <td>201.802084</td>
      <td>139.509787</td>
      <td>0.000000</td>
      <td>12</td>
    </tr>
    <tr>
      <th>1</th>
      <td>C10002</td>
      <td>3202.467416</td>
      <td>0.909091</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>6442.945483</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.250000</td>
      <td>4</td>
      <td>0</td>
      <td>7000.0</td>
      <td>4103.032597</td>
      <td>1072.340217</td>
      <td>0.222222</td>
      <td>12</td>
    </tr>
    <tr>
      <th>2</th>
      <td>C10003</td>
      <td>2495.148862</td>
      <td>1.000000</td>
      <td>773.17</td>
      <td>773.17</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0</td>
      <td>12</td>
      <td>7500.0</td>
      <td>622.066742</td>
      <td>627.284787</td>
      <td>0.000000</td>
      <td>12</td>
    </tr>
    <tr>
      <th>3</th>
      <td>C10004</td>
      <td>1666.670542</td>
      <td>0.636364</td>
      <td>1499.00</td>
      <td>1499.00</td>
      <td>0.0</td>
      <td>205.788017</td>
      <td>0.083333</td>
      <td>0.083333</td>
      <td>0.000000</td>
      <td>0.083333</td>
      <td>1</td>
      <td>1</td>
      <td>7500.0</td>
      <td>0.000000</td>
      <td>NaN</td>
      <td>0.000000</td>
      <td>12</td>
    </tr>
    <tr>
      <th>4</th>
      <td>C10005</td>
      <td>817.714335</td>
      <td>1.000000</td>
      <td>16.00</td>
      <td>16.00</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.083333</td>
      <td>0.083333</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0</td>
      <td>1</td>
      <td>1200.0</td>
      <td>678.334763</td>
      <td>244.791237</td>
      <td>0.000000</td>
      <td>12</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-0239163e-fb5c-444a-8b2b-314b7f2a8a42')" title="Convert this dataframe to an interactive table." style="display:none;">

  &lt;svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px"&gt;
    <path d="M0 0h24v24H0V0z" fill="none" />
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z" /><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z" />
  &lt;/svg&gt;
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-0239163e-fb5c-444a-8b2b-314b7f2a8a42 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-0239163e-fb5c-444a-8b2b-314b7f2a8a42');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>

<h1 id="데이터-확인">데이터 확인</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 결측치 확인
</span><span class="n">df</span><span class="p">.</span><span class="n">isnull</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CUST_ID                               0
BALANCE                               0
BALANCE_FREQUENCY                     0
PURCHASES                             0
ONEOFF_PURCHASES                      0
INSTALLMENTS_PURCHASES                0
CASH_ADVANCE                          0
PURCHASES_FREQUENCY                   0
ONEOFF_PURCHASES_FREQUENCY            0
PURCHASES_INSTALLMENTS_FREQUENCY      0
CASH_ADVANCE_FREQUENCY                0
CASH_ADVANCE_TRX                      0
PURCHASES_TRX                         0
CREDIT_LIMIT                          1
PAYMENTS                              0
MINIMUM_PAYMENTS                    313
PRC_FULL_PAYMENT                      0
TENURE                                0
dtype: int64
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 데이터 타입 확인
</span><span class="n">df</span><span class="p">.</span><span class="n">dtypes</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CUST_ID                              object
BALANCE                             float64
BALANCE_FREQUENCY                   float64
PURCHASES                           float64
ONEOFF_PURCHASES                    float64
INSTALLMENTS_PURCHASES              float64
CASH_ADVANCE                        float64
PURCHASES_FREQUENCY                 float64
ONEOFF_PURCHASES_FREQUENCY          float64
PURCHASES_INSTALLMENTS_FREQUENCY    float64
CASH_ADVANCE_FREQUENCY              float64
CASH_ADVANCE_TRX                      int64
PURCHASES_TRX                         int64
CREDIT_LIMIT                        float64
PAYMENTS                            float64
MINIMUM_PAYMENTS                    float64
PRC_FULL_PAYMENT                    float64
TENURE                                int64
dtype: object
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 결측치 처리 (5%이하의 갯수이므로 드랍하겠다.)
</span><span class="n">df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8636, 18)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span> <span class="c1">#경고문을 무시한다.
</span>
<span class="n">i</span><span class="o">=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">))</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'CUST_ID'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

    <span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>

    <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/source_11.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 상관 계수 (correlation coefficient) 확인
</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'coolwarm'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff838ba2890&gt;
</code></pre></div></div>

<p><img src="/assets/images/source_12.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<h1 id="표준화-및-pca">표준화 및 PCA</h1>
<p>+축소할 차원 수 결정.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 상관관계가 어느정도 있는 feature가 보이므로 PCA 사용이 유의미할 것으로 판단.
</span><span class="n">df</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'CUST_ID'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># PCA전 표준화
</span><span class="n">ss</span><span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">df</span><span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1">#PCA 진행
</span><span class="n">pca</span><span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PCA()
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PCA 의 축소 차원갯수에 대한 정확도 차이 (기존 차원수를 유지하는게 당연히 100%일것임..)
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">.</span><span class="n">cumsum</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x7ff8393998d0&gt;]
</code></pre></div></div>

<p><img src="/assets/images/source_13.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 차원 축소 7개로 하겠다!
</span><span class="n">pca</span><span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">df_pca</span><span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_pca</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(8636, 7)
</code></pre></div></div>

<h1 id="k-means-clustering">K-means Clustering</h1>
<p>+K-means 와 실루엣 스코어(silhouette_score)</p>

<ul>
  <li>k-means : 각 데이터들과 해당 centroid까지 거리 합</li>
  <li>sil_score : 1에 가까울수록 군집과 군집이 잘 분리되어있다는 뜻<br />
  기본적으로 0이상이고 만약 음수라면 군집끼리 겹쳤다는 의미</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">distortions</span><span class="o">=</span><span class="p">[]</span>
<span class="n">sil_scores</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">30</span><span class="p">):</span>
    <span class="c1"># n_cluster : 군집 갯수, n_iter : 중심점 업데이트의 최소 횟수
</span>    <span class="n">kmeans</span><span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span> <span class="s">'k-means++'</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_pca</span><span class="p">)</span>
    <span class="c1"># inertia_ : k-means 구하는 중에 centroid로부터 데이터들의 거리 데이터 (클 수록 중심점으로부터 멀다는 거겟쥐)
</span>    <span class="n">distortions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
    <span class="n">label</span><span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>
    <span class="n">sil_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">df_pca</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">distortions</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">distortions</span><span class="p">,</span><span class="s">'o'</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/source_14.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="c1"># sil_scores 확인
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">sil_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">sil_scores</span><span class="p">,</span><span class="s">'o'</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/source_15.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sil_scores</span>
<span class="c1"># k-means 는 군집을 늘릴수록 감소 (이상적)
# 실루엣 계수는 3에서 0.28정도로 그나마 크나, 전체적으로는 0.25근처로 별로 크게 나오진 않음.
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.24692450845604266,
 0.28065750721461125,
 0.2507007863496503,
 0.2408710781955889,
 0.25789671860000646,
 0.25543152274952236,
 0.26596171359898996,
 0.25980372706530475,
 0.2606025124276636,
 0.24195602184083095,
 0.25144013095228146,
 0.23962807370519182,
 0.23713138793264468,
 0.23981648640294187,
 0.2199707245532794,
 0.2259075081873923,
 0.23446181289456078,
 0.21832255569020825,
 0.24002990422966186,
 0.23109689399081063,
 0.22381807371604162,
 0.21453215289991348,
 0.21537478705286658,
 0.21174420647359052,
 0.2161573025700017,
 0.21208142071199876,
 0.2196422007478946,
 0.20723949847328374]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># k-means 는 군집 3개로(실루엣계수 따라), PCA는 2개로하여 시각화.
</span>
<span class="n">kmeans</span><span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span> <span class="s">'k-means++'</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s">'full'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">kmeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_pca</span><span class="p">)</span>
<span class="n">labels</span><span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>

<span class="n">pca</span><span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">df_pca2</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">temp</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'pca1'</span><span class="p">,</span><span class="s">'pca2'</span><span class="p">])</span>
<span class="n">df_pca2</span><span class="p">[</span><span class="s">'labels'</span><span class="p">]</span><span class="o">=</span> <span class="n">labels</span>
<span class="n">df_pca2</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div id="df-ed39b30d-40d0-4339-9588-39e85728f5f0">
    <div class="colab-df-container">
      <div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pca1</th>
      <th>pca2</th>
      <th>labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.696397</td>
      <td>-1.122594</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-1.215688</td>
      <td>2.435597</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.935860</td>
      <td>-0.385170</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.614640</td>
      <td>-0.724592</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.223706</td>
      <td>-0.783584</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ed39b30d-40d0-4339-9588-39e85728f5f0')" title="Convert this dataframe to an interactive table." style="display:none;">

  &lt;svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px"&gt;
    <path d="M0 0h24v24H0V0z" fill="none" />
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z" /><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z" />
  &lt;/svg&gt;
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ed39b30d-40d0-4339-9588-39e85728f5f0 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ed39b30d-40d0-4339-9588-39e85728f5f0');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'pca1'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'pca2'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'labels'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_pca2</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s">'bright'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/assets/images/source_16.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="pj" /><category term="cluster" /><category term="pca" /><summary type="html"><![CDATA[데이터 선정 : kaggle credit customer dataset (https://www.kaggle.com/arjunbhasin2013/ccdata)]]></summary></entry><entry><title type="html">정규화, 표준화 그리고 PCA</title><link href="http://localhost:4000/ds/pca/" rel="alternate" type="text/html" title="정규화, 표준화 그리고 PCA" /><published>2022-02-20T00:00:00+09:00</published><updated>2022-02-21T00:00:00+09:00</updated><id>http://localhost:4000/ds/pca</id><content type="html" xml:base="http://localhost:4000/ds/pca/"><![CDATA[<h2 id="정규화와-표준화">정규화와 표준화</h2>
<blockquote>
  <p>정규화 (normalization)</p>
</blockquote>

<p>정규화는 모든 값들을 0과 1사이의 값으로 단순하게 축소한다.<br />
예를 들어 0~100까지의 값중 35 는 0.35가 될 뿐이다. <br />
식 : x = (원래값 - 최댓값) / (최댓값 - 최솟값)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

df_scaled = scaler.fit_transform(df)
</code></pre></div></div>

<blockquote>
  <p>표준화 (standardization)</p>
</blockquote>

<p>표준화는 데이터를 0을 중심으로 양쪽으로 데이터를 분포시킨다.<br />
정확히는 0의 평균을 갖고, 1의 표준편차를 갖도록 변환하는 것.</p>

<p>식 : x = (원래값 - 평균) / 표준편차</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import StandardScaler
sclaer = StandardScaler()

df_scaled = scaler.fit_transform(df)
</code></pre></div></div>

<blockquote>
  <p>fit 과 fit transform 의 차이</p>
</blockquote>

<p>fit은 해당 데이터에 맞춤으로 모델(객체)을 설정해주는 것.<br />
fit_transform 은 fit함과 동시에 해당 데이터를 모델을 사용해서 변형해주어 return함.</p>

<h2 id="분산과-공분산">분산과 공분산</h2>

<blockquote>
  <p>분산 (variance)</p>
</blockquote>

<p>하나의 feature가 갖는 ‘평균으로 부터 퍼져있는 정도’ 이다.<br />
식 : Var(x) = E [(X-X평균)^2]</p>

<p>분산의 양의 제곱근이 표준편차(Standard Deviation)</p>

<blockquote>
  <p>공분산 (Covariance)</p>
</blockquote>

<p>두 feature가 갖는 공동 변화량이다. 직관적으로는 이해하기 힘들더라.. 그 의미를 파악하자면<br />
<strong>0보다크면 X가 증가할때 Y도 증가한다.(양의 상관관계)</strong><br />
<strong>0보 작으면 서로 음의 상관관계</strong><br />
식 : Cov(X,Y) = E[(X-X평균)(Y-Y평균)]</p>

<blockquote>
  <p>상관계수 (Correlation coefficient)</p>
</blockquote>

<p>공분산과 자연스럽게 이어지는데, 상관계수는 <strong>얼만큼</strong> 상관관계를 갖는지도 알려준다.</p>

<p>식 : Corr(X,Y) = Cov(X,Y) / (sd(X)*sd(Y))  (sd는 표준편차)</p>

<h2 id="pca">PCA</h2>
<p><strong>표준화or정규화 필수!!!!</strong><br />
PCA는 고 차원(feature 종류가 많을 때)의 데이터셋을 차원축소 하고자 할 때 사용한다.<br />
여기서 중요한건 PCA는 특정한 feature를 선택(selection)하는 것이 아니라<br />
모든 feature의 특징을 담아내는 feature로 추출(Extraction) 한다는 것이다.</p>

<p>어떻게 Extraction할 것이냐 !!
바로 <strong>축을 고르는 것</strong> 이다.</p>

<p>어떠한 축에 모든 feature들을 projection시켰을 때, ‘가장 그 정보들을 많이 담는다’면 그 축은 모든 feature의 특징을<br />
잘 담고 있는 축이 될 것이다.<br />
‘정보를 많이 담는다’는 것은 공분산이 가장 큰 것이다라고 이해했으며,<br />
그 축에 projection시킨 값들의 집합하나가 하나의 차원이 될 것이다.<br />
그리고 그 다음 축은 첫번째 축과 직교인 축으로 고르게 될 것이다.<br />
<img src="/assets/images/source_10.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>features = df.loc[:,'bill_length_mm':'body_mass_g']
species = df['species']
</code></pre></div></div>
<p>↪️ df에서 ‘bill_length_mm’부터 ‘body_mass_g’ 까지의 feature의 데이터만 가져오고, ‘species’만 가져온다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features = pd.DataFrame(scaler.fit_transform(features), columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])
</code></pre></div></div>
<p>↪️ 표준화를 진행한다. (<strong>필수!!!!!</strong>)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
extracted_df = pd.DataFrame(pca.fit_transform(features), columns=['PC1', 'PC2'])
</code></pre></div></div>
<p>↪️ pca를 실행한 후 ‘PC1’, ‘PC2’로 저장된(자동으로 이름 이렇게 지음) 두 차원만 불러온다.<br />
두 차원으로 차원축소 성공 !
<br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="ds" /><summary type="html"><![CDATA[정규화와 표준화 정규화 (normalization)]]></summary></entry><entry><title type="html">데이터 다루기</title><link href="http://localhost:4000/ds/Data/" rel="alternate" type="text/html" title="데이터 다루기" /><published>2022-02-12T00:00:00+09:00</published><updated>2022-02-25T00:00:00+09:00</updated><id>http://localhost:4000/ds/Data</id><content type="html" xml:base="http://localhost:4000/ds/Data/"><![CDATA[<h2 id="pandas-document">pandas document</h2>

<p>pandas 의 모든것이 들어있는 공식문서<br />
https://pandas.pydata.org/docs/reference/index.html</p>

<h2 id="dataframe-">DataFrame ?</h2>

<p>데이터프레임을 먼저 알아야 하는데, 열(특성)과 행(단일데이터) 들로 이루어진 2차원 표라고 생각하면 된다.</p>

<h2 id="데이터-불러오기">데이터 불러오기</h2>
<blockquote>
  <p>csv 불러오기</p>
</blockquote>

<p>정확히는 csv 파일을 dataFrame의 객체타입으로 변환하는 것</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
path = #csv의 경로
df = pd.read_csv(path)
#tsv 파일이라면 comma(',')로 구분되는 것이 아닌 tab('\t')으로 구분되기때문에 sep을 설정해줘야함.
df = pd.read_csv(path, sep='\t')

#csv 파일의 첫번째 줄은 feature의 이름인 경우가 많다. 그땐 index_col=0 으로 하면 0번째 행을 설정하는것
df = pd.read_csv(path, index_col=0)

# 내가 딱 필요한 feature를 정해서 불러온다. 아주 애용한다.
df = pd.read_csv(path, usecols=['A', 'B'])  

# skiprows를 이용하면 1,2번째 행은 제외할 수 있다.
df = pd.read_csv(path, skiprows = [1, 2])

# nrows를 5로 하면 위에서 5개 데이터만 불러온다.
df = pd.read_csv(path, nrows = 5)

#na_values를 이용해서 결측값을 제외하고 부를 수 있지만 dataframe의 내장함수를 사용하는 걸 개인적으로 선호한다.
df = pd.read_csv(path, na_values = [0, '?', 'N/A', 'NA', 'nan', 'NaN', 'null'])
</code></pre></div></div>

<h2 id="데이터-프레임-객체의-기본-내장함수">데이터 프레임 객체의 기본 내장함수</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
df = pd.dataFrame(data)

# 데이터프레임 모양확인
df.shape

# 데이터 타입 확인
df.dtypes

# 인덱스 객체 반환 (인덱스는 각 행들의 이름)
df.index

# features 의 인덱스 객체 반환
df.columns

# 전치형태의 데이터프레임객체 반환 (전치는 행열 반전)
df.T

# 결측치 처리
df.fillna(0, inplace=True)  # 이 예시에서 결측치는 0으로 처리 및, inplace (default=false) 가 True이면 df원본을 손실하고 바로 변경된다. (false이면 결측치 처리된 df 수정본을 return할 뿐.)

# 컬럼 별 결측치 확인
df.isnull().sum()

# 요약통계량 확인
# count : 데이터의 수, mean/std : 평균/표준편차,  min/max : 최소/최대, 25%등 : 백분위수
df.describe()

# 각 feature들이 어떻게 분포되어있는지 한번에 확인가능!! *****
from pandas_profiling import ProfileReport
df.profile_report()
</code></pre></div></div>
<h2 id="데이터-프레임의-데이터-슬라이싱">데이터 프레임의 데이터 슬라이싱</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 열접근
df.NAME
df['NAME'] # 둘은 같은 결과를 도출한다.

# loc 사용
# feature의 '이름' 으로 추출
df.loc[:,['NAME','GENDER']]

# iloc 사용
# 행과 열의 번호(순서)로 추출
df.iloc[2:4, 0:3]     # 이 예시에서 2이상 4미만 행, 0이상 3미만의 열을 추출

# 조건부 슬라이싱
df[df.GENDER == 'M']  # 이 예시에서 'GENDER' 라는 features 가 'M'인 행들을 추출
</code></pre></div></div>

<h2 id="feature-engineering">feature engineering</h2>
<p>데이터들을 변형시키거나 특성들간 함수처리를 통해 내가 원하는 특성을 만들어내보자</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># apply (중요!)
df.apply(함수)              # 여기서 함수는 파이썬 내장함수일 수도, 사용자 정의함수일 수도 있다. lambda도 사용 가능
df.apply(lambda x: x+1)                     #lambda사용 예시 모든 데이터에 1을 더하겠다.
df['AGE'] = df['AGE'].apply(lambda x:x+1)   # 모든 AGE에 1을더한걸 원본df에도 적용하겠다.

# 특성간 계산
df['GENDER_AGE'] = df['GENDER'] + df['AGE'].apply(str)  # GENDER가 'M'이고 나이가 23 이면 GENDER_AGE라는 feature는 'M23'을 가짐.
</code></pre></div></div>
<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="ds" /><summary type="html"><![CDATA[pandas document]]></summary></entry><entry><title type="html">가설검정</title><link href="http://localhost:4000/something_else/hypoo/" rel="alternate" type="text/html" title="가설검정" /><published>2022-02-11T00:00:00+09:00</published><updated>2022-02-12T00:00:00+09:00</updated><id>http://localhost:4000/something_else/hypoo</id><content type="html" xml:base="http://localhost:4000/something_else/hypoo/"><![CDATA[<h2 id="가설-검정">가설 검정?</h2>
<p>‘서울시는 사람이 많이 산다.’ 는 사실확인이 가능할까? 안된다. ‘많이’는 너무 주관적이기 때문이다.<br />
그렇지만 ‘사람의 머리카락은 평균 11만 가닥이다.’ 는 사실확인을 할 수 있을까?<br />
데이터 직군에서는 이를 통계학을 기반으로하여 p value 값을 통해 ‘통계적으로 유의하다.’ or ‘통계적으로 유의하지않다’ 를 결정한다. (고 약속했다.)</p>

<h2 id="귀무-가설">귀무 가설</h2>
<p>귀무 가설은 하나의 가설 검정이 하나만 갖는다. 이 가설이 기각되는지, 기각되지 않는지에 따라 통계적으로 어떤 의미를 갖는지 명확해지므로<br />
처음 검정을 할 때, 귀무가설이 무엇인지 정확히 짚고 실행해야 할 것이다.</p>

<h2 id="p-value">p-value</h2>
<p>pvalue &lt; 0.01 : 귀무가설이 옳을 확률이 1%이하 -&gt; 틀렸다 (깐깐한 기준)<br />
pvalue &lt; 0.05 (5%) : 귀무가설이 옳을 확률이 5%이하 -&gt; 틀렸다 (일반적인 기준)<br />
0.05 ~ pvalue ~ 0.1 사이인 경우: (애매함)<br />
pvalue &gt; 0.1 (10%) : 귀무가설이 옳을 확률이 10%이상인데 -&gt; 귀무가설이 맞다 ~ 틀리지 않았을것이다.<br />
p-value : 0.85 –&gt; 귀무가설은 틀리지 않았다. (귀무가설이 옳다와 톤이 약간 다름)</p>

<h2 id="독립표본-t-test">독립표본 T-Test</h2>
<blockquote>
  <p>one_sample_t_test</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from scipy import stats
pv1=stats.ttest_1samp(df['AGE'],35).pvalue
</code></pre></div>  </div>
</blockquote>

<p>귀무가설 : df데이터의 사람들의 나이는 평균이 35라고 할 수 있다.<br />
예) pv1이 0.05 보다 작다면 ‘사람들의 나이 평균은 35라는 가설은 기각한다.’ 즉 ‘신뢰도 95%에서 평균은 35라고 할 수는 없다.’ <br />
예) pv1이 0.85 정도가 나왔다면 ‘신뢰도 95%에서 사람들의 나이평균은 35이다’</p>

<blockquote>
  <p>two_sample_t_test</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pv2=stats.ttest_ind(df1['AGE'],df2['AGE']).pvalue
pv3=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='greater').pvalue
pv4=stats.ttest_ind(df1['AGE'],df2['AGE'], alternative='less').pvalue
</code></pre></div>  </div>
</blockquote>

<p>pv2 귀무가설 : df1 나이평균과 df2나이평균은 통계적으로 같다. (나이에 있어서 두 샘플은 같은 통계적 분포를 가진다)<br />
pv3 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 크다.<br />
pv4 귀무가설 : df2 나이평균은 df1나이평균보다 통계적으로 작다.</p>

<h2 id="대응표본쌍체표본-t-test">대응표본(쌍체표본) t-Test</h2>
<p>데이터 수가 같은 두 표본 (같은 집단에 대한 약물의 전후 효과 비교 등) 의 평균을 비교한다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import scipy.stats
scipy.stats.ttest_rel(dat_M, dat_F)
# dat_M과 dat_F는 쌍이되는 두 표본의 어떤 feature값들,
</code></pre></div></div>

<p>귀무가설 : dat_M 과 dat_F 의 평균은 통계적으로 같다.</p>

<h2 id="chi-square-test">chi-square-Test</h2>
<p>카이제곱 검정은 두 다른 feature에 대한 검정을 하는데 사용하거나<br />
두 독립 표본이 통계적으로 차이가 있는지를 검정하는데 사용.</p>

<blockquote>
  <p>적합도 검정 (일원카이제곱)</p>
</blockquote>

<p>두 표본이 통계적으로 같은 결과라고 볼 수 있는지 검정한다.<br />
예 : 주사위를 실제로 던져 나온 숫자 데이터셋 vs 같은 횟수만큼 3.5를 적은 데이터셋<br />
혹은 실제 관측 데이터 vs 기대 데이터<br />
혹은 표본이 모집단을 대표할 수 있는지에 대한 검정도 가능 ( 적합도 라는 단어가 이제야 어울린다.. )</p>

<p>귀무가설 : 두 데이터는 통계적으로 같은 데이터이다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from scipy.stats import chi2_contingency
chi_res  = chi2_contingency(pd.crosstab(df['실제관측'], df['기댓값']))
</code></pre></div></div>

<blockquote>
  <p>행과 열의 독립성 검정 (이원카이제)</p>
</blockquote>

<p><img src="/assets/images/source_7.png" width="50%" height="50%" title="제목" alt="아무거나" /></p>

<p>df가 위와같은 데이터셋일 때,<br />
귀무가설 : cut(품질)과 color(색상)은 독립적이다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from scipy.stats import chi2_contingency

chi2 = chi2_contingency(df)
chi2
</code></pre></div></div>
<p>chi2의 첫번째 값 : chi suare, 두번째 값 : p-value<br />
만약 p-value가 0.05아래라면 신뢰도 95%에서 귀무가설은 기각, 품질과 색상은 관련이 있다.<br />
만약 큰 값을 가진다면 품질과 색상은 독립적이다.</p>

<blockquote>
  <p>동질성 검정 (이원카이제곱)</p>
</blockquote>

<p>두 표본이 같은 모집단에서 나온 것인지 아닌지 판단할 수 있는지 검정<br />
이는 위 독립성 검정의 방법을 그대로 따라하되<br />
그 데이터가 어떤 구성인지의 차이 + 해석의 차이를 두면 된다.</p>

<p>좋은 예시가 있어서 가져와봤다. (출처 : https://hsm-edu.tistory.com/1213)<br />
<img src="/assets/images/source_8.png" width="50%" height="50%" title="제목" alt="아무거나" />
<img src="/assets/images/source_9.png" width="50%" height="50%" title="제목" alt="아무거나" /></p>

<p>위와 아래의 차이는 아무런 코멘트가 없다면 ‘모른다’가 정답이다.<br />
그러나 위 데이터는, 한번에 200명의 표본을 추출한것이고.<br />
아래 데이터는 모집단에서 A 데이터 100명과 B 데이터 100명을 추출했다고 하면 차이가 느껴질 것이다.</p>

<p>위 데이터에서 이원카이제곱을 한다면<br />
귀무가설 : 성별과 흡연유무는 독립적이다.</p>

<p>아래 데이터에서 이원카이제곱을 한다면<br />
귀무가설 : A데이터와 B데이터는 다른 모집단에서 왔다. (독립적이다.)</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="Something_else" /><summary type="html"><![CDATA[가설 검정? ‘서울시는 사람이 많이 산다.’ 는 사실확인이 가능할까? 안된다. ‘많이’는 너무 주관적이기 때문이다. 그렇지만 ‘사람의 머리카락은 평균 11만 가닥이다.’ 는 사실확인을 할 수 있을까? 데이터 직군에서는 이를 통계학을 기반으로하여 p value 값을 통해 ‘통계적으로 유의하다.’ or ‘통계적으로 유의하지않다’ 를 결정한다. (고 약속했다.)]]></summary></entry><entry><title type="html">단순선형회귀와 모델의 적합</title><link href="http://localhost:4000/something_else/regression/" rel="alternate" type="text/html" title="단순선형회귀와 모델의 적합" /><published>2022-01-21T00:00:00+09:00</published><updated>2022-02-12T00:00:00+09:00</updated><id>http://localhost:4000/something_else/regression</id><content type="html" xml:base="http://localhost:4000/something_else/regression/"><![CDATA[<h2 id="선형회귀">선형회귀</h2>
<blockquote>
  <p>엑셀에서 점들의 추세선을 그어본적이 있다. 거기서 R값을 표시할 수도 있었는데, 이게 단순 선형회귀의 예시가 아닌가 싶다.<br />
이 또한, 머신러닝이다. 특정 평가지표가 최소가 되도록 모델에 학습데이터를 fit을 시켜주는 데, 평가지표의 종류는 아래와 같다.<br />
y는 실제값(관측값, 측정값)     y^는 예측값 즉 ax+b, y평균은 y전체의 평균.<br />
<img width="677" alt="image" src="https://user-images.githubusercontent.com/84547813/150494749-1be8815a-3e36-416c-9ff7-62370f5c540f.png" /></p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 라이브러리 import
from sklearn.linear_model import LinearRegression

# 데이터 정의
df = '타겟을 포함한 데이터 프레임'
df_test = '타겟 미포함 "테스트" 데이터 프레임'

# 모델 클래스 정의
model = LinearRegression()

# feature, target 정의
feature = ['feature_name']
target = ['target_name']
X_train = df[feature]
y_train = df[target]

# 모델 학습
model.fit(X_train, y_train)

# 예측값.
X_test = [[x] for x in df_test['feature_name']]
y_pred = model.predict(X_test)

# 계수확인
print('절편', model.intercept_)
print('계수(여러개일 수 있기때문에 array)', model.coef_)

# 시각화.
import matplotlib.pyplot as plt
## train 데이터에 대한 그래프를 검정색 점으로.
plt.scatter(X_train, y_train, color='black', linewidth=1)

## test 데이터에 대한 예측을 파란색 점으로.
plt.scatter(X_test, y_pred, color='blue', linewidth=1);
</code></pre></div></div>

<h2 id="다중선형회귀">다중선형회귀</h2>
<blockquote>
  <p>x에 대해서 y의 추세선을 그으면 단순선형회귀, x와 y에 대해서 z의 추세선을 그으면 다중선형회귀가 될 것이다. <br />
그렇다면 4개이상 n개의 특성(feature)에 대해서 target의 추세선을 그릴 수 있을까?<br />
위 평가지표(MSE, MAE, R square)에서 예측값 y^ 는 ax+b일수도 있지만<br />
ax+bw+c의 형태로도 될 수있고 일반화하면 아래와 같다.<br />
<img width="102" alt="image" src="https://user-images.githubusercontent.com/84547813/150503177-644087c1-f1a0-4bb3-bc41-203cb552d7ec.png" /></p>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 위 단순 선형회귀에서 X만 여러개 넣어주면 된다! (model도 동일)

# feature, target 정의
feature = ['feature_name_1', 'feature_name_2 ]
target = ['target_name']
X_train = df[feature]
y_train = df[target]

# 나머지는 동일!!
</code></pre></div></div>

<h2 id="평가지표">평가지표</h2>
<blockquote>
  <p>MSE (Mean Square Error),<br />
MAE(Mean Absolute Error),<br />
RMSE (Root Mean Square Error)
R-score</p>
</blockquote>

<h2 id="과적합--과소적합">과적합 &amp; 과소적합</h2>
<p>사진이랑 편향 분산 얘기 넣어놓자.</p>

<p><strong>편향</strong>은 잘못된 가정을 했을 때 발생하는 오차,<br />
<strong>과소적합</strong> 문제를 야기.<br />
<strong>분산</strong>은 트레이닝 셋의 복잡도에 의해 발생하는 오차, 큰 노이즈 까지 모델링에 포함시켜 <br />
<strong>과적합</strong> 문제 야기.<br />
<img src="/assets/images/source_17.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<p>분산과 편향은 트레이드오프(trade-off)관계이다.<br />
예를 들어, 고분산 모델은 트레이닝 셋의 특성을 잘 담는다고도 할 수 있지만, 일반화에 실패했다고도 할 수 있다.<br />
고편향은 지나친 일반화를 하여 과소적합이 되는 것.<br />
즉 적절한 일반화는 편향과 분산의 적절한 분배를 의미하기도 한다. <br />
<img src="/assets/images/source_18.png" width="40%" height="40%" title="제목" alt="아무거나" /></p>

<p>위 표에서 가로축은 모델의 복잡도, 세로축은 score 성능.<br />
즉, 가로축은 고편향에서 고분산으로 향한다.</p>

<p>복잡도 up! -&gt; 트레이닝셋 score up!<br />
너무 복잡해지면 고분산, 즉 과적합 ( 트레이닝셋에만 과하게 학습됨.)<br />
best model은 트레이닝셋과 독립적인 검증데이터에서 의 score로 찾아야 할 것.</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="Something_else" /><summary type="html"><![CDATA[선형회귀 엑셀에서 점들의 추세선을 그어본적이 있다. 거기서 R값을 표시할 수도 있었는데, 이게 단순 선형회귀의 예시가 아닌가 싶다. 이 또한, 머신러닝이다. 특정 평가지표가 최소가 되도록 모델에 학습데이터를 fit을 시켜주는 데, 평가지표의 종류는 아래와 같다. y는 실제값(관측값, 측정값) y^는 예측값 즉 ax+b, y평균은 y전체의 평균. # 라이브러리 import from sklearn.linear_model import LinearRegression # 데이터 정의 df = '타겟을 포함한 데이터 프레임' df_test = '타겟 미포함 "테스트" 데이터 프레임' # 모델 클래스 정의 model = LinearRegression() # feature, target 정의 feature = ['feature_name'] target = ['target_name'] X_train = df[feature] y_train = df[target] # 모델 학습 model.fit(X_train, y_train) # 예측값. X_test = [[x] for x in df_test['feature_name']] y_pred = model.predict(X_test) # 계수확인 print('절편', model.intercept_) print('계수(여러개일 수 있기때문에 array)', model.coef_) # 시각화. import matplotlib.pyplot as plt ## train 데이터에 대한 그래프를 검정색 점으로. plt.scatter(X_train, y_train, color='black', linewidth=1) ## test 데이터에 대한 예측을 파란색 점으로. plt.scatter(X_test, y_pred, color='blue', linewidth=1); 다중선형회귀 x에 대해서 y의 추세선을 그으면 단순선형회귀, x와 y에 대해서 z의 추세선을 그으면 다중선형회귀가 될 것이다. 그렇다면 4개이상 n개의 특성(feature)에 대해서 target의 추세선을 그릴 수 있을까? 위 평가지표(MSE, MAE, R square)에서 예측값 y^ 는 ax+b일수도 있지만 ax+bw+c의 형태로도 될 수있고 일반화하면 아래와 같다. # 위 단순 선형회귀에서 X만 여러개 넣어주면 된다! (model도 동일) # feature, target 정의 feature = ['feature_name_1', 'feature_name_2 ] target = ['target_name'] X_train = df[feature] y_train = df[target] # 나머지는 동일!! 평가지표 MSE (Mean Square Error), MAE(Mean Absolute Error), RMSE (Root Mean Square Error) R-score 과적합 &amp; 과소적합 사진이랑 편향 분산 얘기 넣어놓자. 편향은 잘못된 가정을 했을 때 발생하는 오차, 과소적합 문제를 야기. 분산은 트레이닝 셋의 복잡도에 의해 발생하는 오차, 큰 노이즈 까지 모델링에 포함시켜 과적합 문제 야기. 분산과 편향은 트레이드오프(trade-off)관계이다. 예를 들어, 고분산 모델은 트레이닝 셋의 특성을 잘 담는다고도 할 수 있지만, 일반화에 실패했다고도 할 수 있다. 고편향은 지나친 일반화를 하여 과소적합이 되는 것. 즉 적절한 일반화는 편향과 분산의 적절한 분배를 의미하기도 한다. 위 표에서 가로축은 모델의 복잡도, 세로축은 score 성능. 즉, 가로축은 고편향에서 고분산으로 향한다. 복잡도 up! -&gt; 트레이닝셋 score up! 너무 복잡해지면 고분산, 즉 과적합 ( 트레이닝셋에만 과하게 학습됨.) best model은 트레이닝셋과 독립적인 검증데이터에서 의 score로 찾아야 할 것. 🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우 언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄 맨 위로 이동하기]]></summary></entry><entry><title type="html">정렬</title><link href="http://localhost:4000/something_else/5th/" rel="alternate" type="text/html" title="정렬" /><published>2021-11-27T00:00:00+09:00</published><updated>2022-02-12T00:00:00+09:00</updated><id>http://localhost:4000/something_else/5th</id><content type="html" xml:base="http://localhost:4000/something_else/5th/"><![CDATA[<h2 id="선택정렬">선택정렬</h2>
<blockquote>
  <blockquote>
    <p>제일 작은걸 차례대로 찾아서, 맨앞부터 계속 교체해주는 정렬.<br />
n개에 대해서 교환을 하며, 그 과정에서 최솟값을 찾기위해 n번 비교<br />
(비교대상이 1씩 줄긴하지만 n이 최고차항이긴함.)를 하므로 O(n^2)<br />
<img src="https://user-images.githubusercontent.com/84547813/145223514-1b92d47c-4204-4890-8ab3-07af49e0894b.png" alt="image" /></p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def selection_sort(li):
    n = len(li)
    for i in range(n):      # i번쨰가 주체
        for j in range(i,n):# i번째 뒤로 쭉
            if li[i] &gt; li[j]:
                li[i], li[j] = li[j], li[i]     # 교환
            else :
                pass
    return li
</code></pre></div>    </div>
    <h2 id="삽입정렬">삽입정렬</h2>
    <p>순서대로 자리를 찾아서 끼워 넣어주는 정렬<br />
n개에 대해서 하며, 최악의 경우 탐색을 계속 n개에 대해 길게하게 되기때문에 O(n^2)<br />
아래 사진은 31의 자리를 찾아 넣어주고있다.<br />
<img src="https://user-images.githubusercontent.com/84547813/145224534-3f5b9130-8641-48b6-9151-53a91695617f.png" alt="image" /></p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def insertion_sort(li):
    n=len(li)
    if n ==0:
        return None
    for i in range(1,n): # 두번째부터 끝까지 수행하면 댐.
        temp = li[i]
        for j in range(i-1, -1 , -1): # i-1부터 0까지 비교를 해야댐.
            if li[j] &gt; temp:
                li[j+1] = li[j]       # j에서 temp보다 크다면 옆으로복사
                if j==0:              # 그와중에 0에 도착햇다면 그냥 0(맨앞)에 temp대입
                    li[0] = temp
            else:
                li[j+1] = temp        # j보다 temp가 크다면 그 오른쪽에 temp넣고 break
                break
    return li
</code></pre></div>    </div>
    <h2 id="버블정렬">버블정렬</h2>
    <p>계속해서 옆과 비교,교환을 하는 방식.<br />
(오름차순에서) 만약 맨앞에 제일큰숫자가 있다면, 끊임없이 교환을 거듭하여 끝까지 갈 것이다.(n번)<br />
그 과정을 (1번부터 n까지), (1번부터 n-1까지) … (1과 2) 반복하면 (n번) O(n^2)이다.<br />
<img src="https://user-images.githubusercontent.com/84547813/145225965-b9c07b7f-44b5-438e-9d6b-487b99eb5863.png" alt="image" /></p>
  </blockquote>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def bubble_sort(li):
    n = len(li)
    for i in range(n):      # 0부터 n-1까지
        for j in range(0,n-i):# j는 항상 0부터이되, n-i까지만반복
            if j==(n-1):      # indexError 방지.
                pass
            elif li[j] &gt; li[j+1]:
                li[j], li[j+1] = li[j+1], li[j]     # 교환
            else :
                pass
    return li   
</code></pre></div></div>
<h2 id="퀵정렬">퀵정렬</h2>
<blockquote>
  <blockquote>
    <p>특정값을 잡아 그보다 큰값과 작은값으로 계속해서 반으로 쪼갠다 1개짜리 리스트가 될때까지.<br />
이상적으로 쪼개어진다면 아래그림에서 한층에 대해 모든수를 비교하므로 n , 그 층은 2^x의 해 이므로 log n 이므로 O(nlogn) <br />
<img src="https://user-images.githubusercontent.com/84547813/145229539-66ac03f9-ff21-451a-b60d-262e8b4d09ca.png" alt="image" /><br />
그러나 아래처럼 계속 최악의 경우로 된다면 층은 n층이 되기때문에 최악의 경우 O(n^2)
<img src="https://user-images.githubusercontent.com/84547813/145229599-9731477a-81d1-4bda-9e28-3ca1a4e62241.png" alt="image" /></p>
  </blockquote>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def quick_sort(li):
    if len(li)&lt;=1:
        return li
    else:
        L1, L2 = [], []
        for x in li[1:]:
            if x&lt;=li[0]:
                L1.append(x)
            else :
                L2.append(x)
        return quick_sort( L1 ) + quick_sort([ li[0] ]) + quick_sort( L2 )
</code></pre></div></div>
<h2 id="병합정렬">병합정렬</h2>]]></content><author><name>옹달샘👱🏼‍♂️</name></author><category term="Something_else" /><summary type="html"><![CDATA[선택정렬 제일 작은걸 차례대로 찾아서, 맨앞부터 계속 교체해주는 정렬. n개에 대해서 교환을 하며, 그 과정에서 최솟값을 찾기위해 n번 비교 (비교대상이 1씩 줄긴하지만 n이 최고차항이긴함.)를 하므로 O(n^2) def selection_sort(li): n = len(li) for i in range(n): # i번쨰가 주체 for j in range(i,n):# i번째 뒤로 쭉 if li[i] &gt; li[j]: li[i], li[j] = li[j], li[i] # 교환 else : pass return li 삽입정렬 순서대로 자리를 찾아서 끼워 넣어주는 정렬 n개에 대해서 하며, 최악의 경우 탐색을 계속 n개에 대해 길게하게 되기때문에 O(n^2) 아래 사진은 31의 자리를 찾아 넣어주고있다. def insertion_sort(li): n=len(li) if n ==0: return None for i in range(1,n): # 두번째부터 끝까지 수행하면 댐. temp = li[i] for j in range(i-1, -1 , -1): # i-1부터 0까지 비교를 해야댐. if li[j] &gt; temp: li[j+1] = li[j] # j에서 temp보다 크다면 옆으로복사 if j==0: # 그와중에 0에 도착햇다면 그냥 0(맨앞)에 temp대입 li[0] = temp else: li[j+1] = temp # j보다 temp가 크다면 그 오른쪽에 temp넣고 break break return li 버블정렬 계속해서 옆과 비교,교환을 하는 방식. (오름차순에서) 만약 맨앞에 제일큰숫자가 있다면, 끊임없이 교환을 거듭하여 끝까지 갈 것이다.(n번) 그 과정을 (1번부터 n까지), (1번부터 n-1까지) … (1과 2) 반복하면 (n번) O(n^2)이다. def bubble_sort(li): n = len(li) for i in range(n): # 0부터 n-1까지 for j in range(0,n-i):# j는 항상 0부터이되, n-i까지만반복 if j==(n-1): # indexError 방지. pass elif li[j] &gt; li[j+1]: li[j], li[j+1] = li[j+1], li[j] # 교환 else : pass return li 퀵정렬 특정값을 잡아 그보다 큰값과 작은값으로 계속해서 반으로 쪼갠다 1개짜리 리스트가 될때까지. 이상적으로 쪼개어진다면 아래그림에서 한층에 대해 모든수를 비교하므로 n , 그 층은 2^x의 해 이므로 log n 이므로 O(nlogn) 그러나 아래처럼 계속 최악의 경우로 된다면 층은 n층이 되기때문에 최악의 경우 O(n^2) def quick_sort(li): if len(li)&lt;=1: return li else: L1, L2 = [], [] for x in li[1:]: if x&lt;=li[0]: L1.append(x) else : L2.append(x) return quick_sort( L1 ) + quick_sort([ li[0] ]) + quick_sort( L2 ) 병합정렬]]></summary></entry></feed>