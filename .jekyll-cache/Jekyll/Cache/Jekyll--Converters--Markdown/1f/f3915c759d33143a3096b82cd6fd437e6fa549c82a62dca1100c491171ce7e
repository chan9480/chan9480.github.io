I"Y<h2 id="í¬ë¡¤ë§ì´ë€-">í¬ë¡¤ë§ì´ë€ ?</h2>
<p>webì— ìˆëŠ” ë°ì´í„°ë“¤ì„ ê¸ì–´ëª¨ìœ¼ëŠ” ê²ƒì„ ë§í•œë‹¤.<br />
í¬ê²Œ ë‘ ì¢…ë¥˜ë¡œ ë¨¼ì € ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.</p>
<ol>
  <li>ì •ì í¬ë¡¤ë§ : í•­ìƒ ê°™ì€ ê°’ì„ ì£¼ëŠ” HTMLë¡œ ë¶€í„° íŒŒì‹±ì„ í•´ì„œ í¬ë¡¤ë§.</li>
  <li>ë™ì í¬ë¡¤ë§ : ê°™ì€ HTMLì´ë¼ë„ ë™ì‘, ëª…ë ¹ì„ í†µí•´ ë³€í™”ëœ ê°’ë“¤ì„ í¬ë¡¤ë§.</li>
</ol>

<p>ì •ì í¬ë¡¤ë§ì€ ë©ˆì¶°ìˆëŠ” í˜ì´ì§€ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ ê¸ì–´ëª¨ì€ë‹¤ë©´,<br />
ë™ì í¬ë¡¤ë§ì€ ê²€ìƒ‰, ìŠ¤í¬ë¡¤, í˜ì´ì§€ í´ë¦­ ë“±ì„ í•´ì„œ ë‚˜ì˜¤ëŠ” ì •ë³´ë¥¼ ê¸ì–´ëª¨ì„ ìˆ˜ ìˆë‹¤.</p>

<p>ëŒ€í‘œì ìœ¼ë¡œ ì •ì í¬ë¡¤ë§ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ beautifulsoup(bs4)ê°€ ìˆë‹¤.<br />
ê·¸ë¦¬ê³  ë™ì í¬ë¡¤ë§ì˜ ë°©ë²•ì—ë„ ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ìˆëŠ”ë° ê·¸ ì¤‘ 2ê°€ì§€ë¥¼ ì ì–´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ol>
  <li>openAPIë¥¼ ì´ìš©í•˜ì—¬ ëª…ë ¹ í›„, responseëœ ì •ë³´ë¡œë¶€í„° í¬ë¡¤ë§.</li>
  <li>seleniumì„ í†µí•´ webdriver (í¬ë¡¬, ì‚¬íŒŒë¦¬ ë“±)ë¥¼ ì œì–´í•œ í›„ ë‚˜ì˜¨ í˜ì´ì§€(HTML)ë¡œë¶€í„° í¬ë¡¤ë§.</li>
</ol>

<h2 id="í¬ë¡¤ë§-ì˜ˆì‹œ">í¬ë¡¤ë§ ì˜ˆì‹œ</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import time
import urllib.request
import os


def createDirectory(directory):
    try:
        if not os.path.exists(directory):
            os.makedirs(directory)
    except OSError:
        print("Error: Failed to create the directory.")

def crawling_img(name):
    ## ì…ë ¥ê°’ name ë¬¸ìì—´ì„ ê²€ìƒ‰í•˜ì—¬ ë‚˜ì˜¤ëŠ” ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    ## ì €ì¥í•˜ëŠ” ê²½ë¡œëŠ” í•¨ìˆ˜ ë‚´ì—ì„œ ë³„ë„ë¡œ ì§€ì •í•´ì•¼í•¨.

    # í¬ë¡¬ì„ ë“œë¼ì´ë²„ë¡œ ì±„íƒ. ë²„ì „ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆë‹¤.
    driver = webdriver.Chrome()
    driver.get("https://www.google.co.kr/imghp?hl=ko&amp;tab=wi&amp;authuser=0&amp;ogbl")

    # që¡œ íƒœê·¸ë˜ì–´ ìˆëŠ” ê³³ì´ êµ¬ê¸€í™ˆí˜ì´ì§€ì˜ ê²€ìƒ‰ì°½ì´ë‹¤.
    elem = driver.find_element_by_name("q")
    elem.send_keys(name)
    elem.send_keys(Keys.RETURN)

    #
    SCROLL_PAUSE_TIME = 1
    # Get scroll height
    last_height = driver.execute_script("return document.body.scrollHeight")  # ë¸Œë¼ìš°ì €ì˜ ë†’ì´ë¥¼ ìë°”ìŠ¤í¬ë¦½íŠ¸ë¡œ ì°¾ìŒ
    while True:
        # Scroll down to bottom
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  # ë¸Œë¼ìš°ì € ëê¹Œì§€ ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¼
        # Wait to load page
        time.sleep(SCROLL_PAUSE_TIME)
        # Calculate new scroll height and compare with last scroll height
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            try:
                driver.find_element_by_css_selector(".mye4qd").click()
            except:
                break
        last_height = new_height

    imgs = driver.find_elements_by_css_selector(".rg_i.Q4LuWd")

    # ê²½ë¡œì™€ í´ë” ëª… ì§€ì •.
    dir = ".\idols" + "\\" + name

    createDirectory(dir) #í´ë” ìƒì„±
    count = 1
    for img in imgs:
        try:
            img.click()
            time.sleep(3)
            imgUrl = driver.find_element_by_xpath(
                '//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[1]/div[2]/div/a/img').get_attribute(
                "src")
            path = ".\idols\\" + name + "\\"
            urllib.request.urlretrieve(imgUrl, path + name + "_"+ str(count) + ".jpg")

            # ì´ ì•„ë˜ëŠ” ê´€ë ¨ì´ë¯¸ì§€ ì €ì¥
            imgUrl = driver.find_element_by_xpath(
                '//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[1]/a[1]/div[1]/img').get_attribute(
                "src")
            urllib.request.urlretrieve(imgUrl, path + name + "_"+ str(count) + "_1" + ".jpg")
            imgUrl = driver.find_element_by_xpath(
                '//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[2]/a[1]/div[1]/img').get_attribute(
                "src")
            urllib.request.urlretrieve(imgUrl, path + name + "_"+ str(count) + "_2" + ".jpg")

            imgUrl = driver.find_element_by_xpath(
                '//*[@id="Sva75c"]/div/div/div[3]/div[2]/c-wiz/div/div[1]/div[3]/div[3]/c-wiz/div/div/div/div[3]/div[1]/div[3]/a[1]/div[1]/img').get_attribute(
                "src")
            urllib.request.urlretrieve(imgUrl, path + name + "_"+ str(count) + "_3" + ".jpg")

            count = count + 1
            if count &gt;= 500:
                break
        except:
            pass
    driver.close()
searching_keyword = ["dinadenoire", "melvnin", "Adut Akech", "lola chuil", "leomie anderson",
"khoudia diop", "zoe saldana", "tyra banks", "karrueche tran", "duckie thot"]

for i in range(len(searching_keyword)) :
    searching_keyword[i] += ' face'

for keyword in searching_keyword:
    crawling_img(keyword)
</code></pre></div></div>

<h2 id="test_ì œëª©-2">test_ì œëª© 2</h2>
<p>test2 ëª©ì°¨ í…ŒìŠ¤íŠ¸ ì…ë‹ˆë‹¤.</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ğŸŒœ ê°œì¸ ê³µë¶€ ê¸°ë¡ìš© ë¸”ë¡œê·¸ì…ë‹ˆë‹¤. ì˜¤ë¥˜ë‚˜ í‹€ë¦° ë¶€ë¶„ì´ ìˆì„ ê²½ìš°
ì–¸ì œë“ ì§€ ëŒ“ê¸€ í˜¹ì€ ë©”ì¼ë¡œ ì§€ì í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤! ğŸ˜„
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">ë§¨ ìœ„ë¡œ ì´ë™í•˜ê¸°</a></p>
:ET