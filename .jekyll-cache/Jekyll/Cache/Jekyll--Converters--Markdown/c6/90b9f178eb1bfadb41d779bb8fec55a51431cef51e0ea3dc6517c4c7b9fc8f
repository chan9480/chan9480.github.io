I"<h2 id="cv-교차-검증-cross-validation">CV (교차 검증, cross validation)</h2>
<p>먼저 데이터라고 하면 용도에 따라 3가지로 분류할 수 있다.<br />
학습데이터(training_data),<br />
검증데이터(validation_data),
테스트데이터(test_data)</p>

<p>세가지 데이터는 서로 누출되어서는 안되고,<br />
학습데이터로 학습시킨 ‘모델’ 의<br />
점수는 ‘검증데이터’로 체크를 하되,<br />
마지막으로 타겟 데이터로써 테스트 데이터를 사용한다.</p>

<p>단, CV는 ‘검증데이터’를 따로 두지 않고,<br />
학습데이터를 일부 추출하여 검증데이터로 사용한다는 방식이다.</p>

<p>만약 그저 데이터를 처음부터 ‘학습데이터’와 ‘검증데이터’로 나눈다면 아래와 같을 것이다.<br />
<img src="/assets/images/source_19.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<p>그러나 CV를 이용하게 되면 아래와 같이 학습데이터와 검증데이터가 수시로 바뀌게 되며. 더 <strong>‘일반화’</strong> 된 모델을 얻을 수 있다.<br />
<img src="/assets/images/source_20.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<h2 id="릿지-회귀-ridge-regression">릿지 회귀 (Ridge regression)</h2>
<p>간단하게 Ridge 회귀는 단순회귀보다 더 일반화된 모델을 만든다고 이해해도 좋을 듯 하다.<br />
즉, 단순선형회귀의 과적합 방지 모델.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.linear_model import RidgeCV

# 수행해볼 알파 값들을 정의한다.
alphas = [0.01, 0.05, 0.1, 0.2, 1.0, 10.0, 100.0]

# RidgeCV 모델 객체를 정의한다.
ridge = RidgeCV(alphas=alphas, normalize=True, cv=3)
ridge.fit(ans[['x']], ans['y'])

#결정된 알파와 베스트 스코어를 출력.
print("alpha :", ridge.alpha_)
print("best score :", ridge.best_score_)

# 예측해보고 싶은 X_test에 대해 y
y_pred = ridge.predict(X_test)
</code></pre></div></div>

<h2 id="로지스틱-회귀-logistic-regression">로지스틱 회귀 (logistic Regression)</h2>
<p>로지스틱 회귀는 sigmoid라는 ‘비선형’을 사용한 ‘2진 분류(classification)’ 모델이다.</p>

<blockquote>
  <p>sigmoid
0부터 1사이의 값을 갖도록 하는</p>
</blockquote>

<blockquote>
  <p>임계값 (Classification Threshold)</p>
</blockquote>

<p>임계값이 로지스틱 회귀에서 등장했는데,</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>
:ET