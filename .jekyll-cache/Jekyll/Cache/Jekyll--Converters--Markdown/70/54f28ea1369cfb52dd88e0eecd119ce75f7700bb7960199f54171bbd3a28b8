I"'<h2 id="정규화와-표준화">정규화와 표준화</h2>
<blockquote>
  <p>정규화 (normalization)</p>
</blockquote>

<p>정규화는 모든 값들을 0과 1사이의 값으로 단순하게 축소한다.<br />
예를 들어 0~100까지의 값중 35 는 0.35가 될 뿐이다. <br />
식 : x = (원래값 - 최댓값) / (최댓값 - 최솟값)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

df_scaled = scaler.fit_transform(df)
</code></pre></div></div>

<blockquote>
  <p>표준화 (standardization)</p>
</blockquote>

<p>표준화는 데이터를 0을 중심으로 양쪽으로 데이터를 분포시킨다.<br />
정확히는 0의 평균을 갖고, 1의 표준편차를 갖도록 변환하는 것.</p>

<p>식 : x = (원래값 - 평균) / 표준편차</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.preprocessing import StandardScaler
sclaer = StandardScaler()

df_scaled = scaler.fit_transform(df)
</code></pre></div></div>

<blockquote>
  <p>fit 과 fit transform 의 차이</p>
</blockquote>

<p>fit은 해당 데이터에 맞춤으로 모델(객체)을 설정해주는 것.<br />
fit_transform 은 fit함과 동시에 해당 데이터를 모델을 사용해서 변형해주어 return함.</p>

<h2 id="분산과-공분산">분산과 공분산</h2>

<blockquote>
  <p>분산 (variance)</p>
</blockquote>

<p>하나의 feature가 갖는 ‘평균으로 부터 퍼져있는 정도’ 이다.<br />
식 : Var(x) = E [(X-X평균)^2]</p>

<p>분산의 양의 제곱근이 표준편차(Standard Deviation)</p>

<blockquote>
  <p>공분산 (Covariance)</p>
</blockquote>

<p>두 feature가 갖는 공동 변화량이다. 직관적으로는 이해하기 힘들더라.. 그 의미를 파악하자면<br />
<strong>0보다크면 X가 증가할때 Y도 증가한다.(양의 상관관계)</strong><br />
<strong>0보 작으면 서로 음의 상관관계</strong><br />
식 : Cov(X,Y) = E[(X-X평균)(Y-Y평균)]</p>

<blockquote>
  <p>상관계수 (Correlation coefficient)</p>
</blockquote>

<p>공분산과 자연스럽게 이어지는데, 상관계수는 <strong>얼만큼</strong> 상관관계를 갖는지도 알려준다.</p>

<p>식 : Corr(X,Y) = Cov(X,Y) / (sd(X)*sd(Y))  (sd는 표준편차)</p>

<h2 id="pca">PCA</h2>
<p><strong>표준화or정규화 필수!!!!</strong><br />
PCA는 고 차원(feature 종류가 많을 때)의 데이터셋을 차원축소 하고자 할 때 사용한다.<br />
여기서 중요한건 PCA는 특정한 feature를 선택(selection)하는 것이 아니라<br />
모든 feature의 특징을 담아내는 feature로 추출(Extraction) 한다는 것이다.</p>

<p>어떻게 Extraction할 것이냐 !!
바로 <strong>축을 고르는 것</strong> 이다.</p>

<p>어떠한 축에 모든 feature들을 projection시켰을 때, ‘가장 그 정보들을 많이 담는다’면 그 축은 모든 feature의 특징을<br />
잘 담고 있는 축이 될 것이다.<br />
‘정보를 많이 담는다’는 것은 공분산이 가장 큰 것이다라고 이해했으며,<br />
그 축에 projection시킨 값들의 집합하나가 하나의 차원이 될 것이다.<br />
그리고 그 다음 축은 첫번째 축과 직교인 축으로 고르게 될 것이다.<br />
<img src="/assets/images/source_10.png" width="60%" height="60%" title="제목" alt="아무거나" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>features = df.loc[:,'bill_length_mm':'body_mass_g']
species = df['species']
</code></pre></div></div>
<p>↪️ df에서 ‘bill_length_mm’부터 ‘body_mass_g’ 까지의 feature의 데이터만 가져오고, ‘species’만 가져온다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features = pd.DataFrame(scaler.fit_transform(features), columns=['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g'])
</code></pre></div></div>
<p>↪️ 표준화를 진행한다. (<strong>필수!!!!!</strong>)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
extracted_df = pd.DataFrame(pca.fit_transform(features), columns=['PC1', 'PC2'])
</code></pre></div></div>
<p>↪️ pca를 실행한 후 ‘PC1’, ‘PC2’로 저장된(자동으로 이름 이렇게 지음) 두 차원만 불러온다.<br />
두 차원으로 차원축소 성공 !
<br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>
:ET