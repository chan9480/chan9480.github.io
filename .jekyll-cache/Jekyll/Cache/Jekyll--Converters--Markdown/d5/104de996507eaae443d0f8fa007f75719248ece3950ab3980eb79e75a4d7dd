I"P	<p>차원을 축소하는 데에는 어떤 방법이 있을까.<br />
첫번째로 영향도 높은 특성을 <strong>‘고르는 것’</strong> 과<br />
두번째는 특성 모두를 특정 축에 <strong>‘투영시키는 것’</strong> 이 있다.</p>

<h2 id="특성-선택-feature-selection">특성 선택 (feature selection)</h2>
<p>특성을 줄이는데, <br />
영향도가 큰 특성을 골라야<br />
모델의 정확도를 유지하면서 복잡도를 줄이거나 일반화를 할 수 있을 것이다.</p>
<ol>
  <li>filter method (전처리 과정에서 통계값으로 선택.)<br />
통계값 종류 : 카이제곱, ANOVA_f_score, 상관계수 등.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_selection import chi2, SelectKBest
selector1 = SelectKBest(chi2, k=14330)
X_train1 = selector1.fit_transform(X_train, y_train)
X_test1 = selector1.transform(X_test)
</code></pre></div>    </div>
    <p>↪️ 카이제곱으로 선택한 예시</p>
  </li>
  <li>wrapper method (모델학습과 검정(validation)을 반복하면서 특성을 선택)<br />
‘성능을 위한 정답’ 이지만 비합리적인 방법.(greedy 나 grid 단어가 떠오른다.. 왜 발음도 비슷하지ㅎ)<br />
시간과 비용이 많이 소모된다.</li>
</ol>

<p>3.</p>

<h2 id="특성-추출-feature-extraction">특성 추출 (feature extraction)</h2>
<p>대표적으로 pca와 같이 특성들을 <br />
특정 축( 특성들의 특징을 잘 나타내야 한다는 이유로 보통 분산을 최대로하는 축을 선택한다. )에<br />
투영(projection)시켜 차원을 축소하는 방식이 있다.<br />
방법에 대해서는 <a href="https://chan9480.github.io/ds/pca/">https://chan9480.github.io/ds/pca/</a> 에서 조금 다루어놓았다.</p>

<h2 id="그래서-차원축소-선택할거야-추출할거야">그래서 차원축소… 선택할거야, 추출할거야?</h2>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>
:ET