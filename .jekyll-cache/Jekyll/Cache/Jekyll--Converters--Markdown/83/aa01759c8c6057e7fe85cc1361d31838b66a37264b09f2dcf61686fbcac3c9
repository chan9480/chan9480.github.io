I".<p>차원을 축소하는 데에는 어떤 방법이 있을까.<br />
첫번째로 영향도 높은 특성을 <strong>‘고르는 것’</strong> 과<br />
두번째는 특성 모두를 특정 축에 <strong>‘투영시키는 것’</strong> 이 있다.</p>

<h2 id="특성-선택-feature-selection">특성 선택 (feature selection)</h2>
<p>특성을 줄이는데, <br />
영향도가 큰 특성을 골라야<br />
모델의 정확도를 유지하면서 복잡도를 줄이거나 일반화를 할 수 있을 것이다.</p>
<ol>
  <li>filter method (전처리 과정에서 통계값으로 선택.)<br />
통계값 종류 : 카이제곱, ANOVA_f_score, 상관계수 등.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_selection import chi2, SelectKBest
selector1 = SelectKBest(chi2, k=14330)
X_train1 = selector1.fit_transform(X_train, y_train)
X_test1 = selector1.transform(X_test)
</code></pre></div>    </div>
    <p>↪️ 카이제곱으로 선택한 예시</p>
  </li>
  <li>wrapper method (모델학습과 검정(validation)을 반복하면서 특성을 선택)<br />
‘성능을 위한 정답’ 이지만 비합리적인 방법.(greedy 나 grid 단어가 떠오른다.. 왜 발음도 비슷하지ㅎ)<br />
시간과 비용이 많이 소모된다.<br />
forward step, backward step, stepwise 방식이 있는데,<br />
각각 feature를 하나씩 ‘추가’, ‘소거’, ‘추가소거 병합’ 방법이다.<br />
~~~
from mlxtend.feature_selection import SequentialFeatureSelector</li>
</ol>

<p>feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),
           k_features=15,
           forward=True,
           verbose=2,
           scoring=’roc_auc’,
           cv=4)
features = feature_selector.fit(np.array(X, y)
filtered_features= train_features.columns[list(features.k_feature_idx_)]
#이후 filtered_features로 모델에 fit</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>↪️ roc_auc 를 평가지표로, RandomForestClassifier 모델을 이용하여 forward step 기법을 사용한 예시
forward 변수만 False로 바꾸면 backward로 사용  

</code></pre></div></div>
<p>from mlxtend.feature_selection import ExhaustiveFeatureSelector
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import roc_auc_score</p>

<p>feature_selector = ExhaustiveFeatureSelector(RandomForestClassifier(n_jobs=-1),
           min_features=2,
           max_features=4,
           scoring=’roc_auc’,
           print_progress=True,
           cv=2)
features = feature_selector.fit(X, y)
filtered_features= train_features.columns[list(features.k_feature_idx_)]
#이후 filtered_features로 모델에 fit
~~~
↪️ 추가소거를 동시에 하는 기법, 최종feature의 최소갯수와 최대갯수를 정한다.</p>
<ol>
  <li>Embedded method (학습과 동시에 )
l1 norm l2 norm ridge lasso elastic_net
selectfrommodel</li>
</ol>

<h2 id="특성-추출-feature-extraction">특성 추출 (feature extraction)</h2>
<p>대표적으로 pca와 같이 특성들을 <br />
특정 축( 특성들의 특징을 잘 나타내야 한다는 이유로 보통 분산을 최대로하는 축을 선택한다. )에<br />
투영(projection)시켜 차원을 축소하는 방식이 있다.<br />
방법에 대해서는 <a href="https://chan9480.github.io/ds/pca/">https://chan9480.github.io/ds/pca/</a> 에서 조금 다루어놓았다.</p>

<p>한계점을 말하자면
무조건 직선에 투영시킨다는 것.
분산이 큰 특성을 무조건 중요 특성으로 판단한다는 것.</p>
<h2 id="그래서-차원축소-선택할거야-추출할거야">그래서 차원축소… 선택할거야, 추출할거야?</h2>
<p>답은 없겠지만 개인적으로 pca는 시각화나 비지도 군집화에 유용하게 사용되며,<br />
그 외 분류, 회귀 등에는</p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>🌜 개인 공부 기록용 블로그입니다. 오류나 틀린 부분이 있을 경우
언제든지 댓글 혹은 메일로 지적해주시면 감사하겠습니다! 😄
</code></pre></div></div>

<p><a href="#" class="btn btn--primary align-right">맨 위로 이동하기</a></p>
:ET